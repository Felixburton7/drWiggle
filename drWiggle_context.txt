### drWiggle Project Context ###

**Project:** drWiggle - Protein Flexibility Classification Framework

**Description:**
A Python framework designed to classify protein residue flexibility based on Root Mean Square Fluctuation (RMSF) data and structural features. It transforms continuous RMSF values into discrete flexibility categories using methods like K-means or quantiles. The framework includes trainable classification models (Random Forest, Neural Network) with hyperparameter optimization capabilities. It integrates PDB feature extraction (B-factor, SS, ACC, dihedrals - requires DSSP for some) and offers visualization outputs (PyMOL scripts, plots). A key feature is the ability to analyze and compare flexibility across different temperatures. The entire workflow is managed via a command-line interface (CLI) and configurable through YAML files.

**Key Components:**
- **Configuration:** YAML-based (`default_config.yaml`, `config.py`)
- **Data Handling:** Loading (`loader.py`), Processing (`processor.py`), Binning (`binning.py`)
- **Models:** Base class (`base.py`), RandomForest (`random_forest.py`), NeuralNetwork (`neural_network.py`) with HPO (Optuna/RandomizedSearch)
- **Pipeline:** Orchestrator (`pipeline.py`)
- **CLI:** User interface (`cli.py` using Click)
- **Utilities:** Metrics (`metrics.py`), PDB Tools (`pdb_tools.py` using BioPython), Visualization (`visualization.py` using Matplotlib/Seaborn), Helpers (`helpers.py`)
- **Temperature Analysis:** Comparison logic (`comparison.py`)
- **Packaging:** `setup.py` for installation.

**Purpose of this File:**
This file concatenates the core source code and configuration files to provide context for understanding the project's implementation.

**Input Data shape from temperature_320_train.csv**
domain_id,resid,resname,rmsf_320,protein_size,normalized_resid,core_exterior,relative_accessibility,dssp,phi,psi,resname_encoded,core_exterior_encoded,secondary_structure_encoded,phi_norm,psi_norm,esm_rmsf,voxel_rmsf
1aabA00,1,GLY,1.0563009,83,0.0,exterior,1.0,C,360.0,61.8,8,1,2,2.0,0.3433333333333333,0.6741838,0.818848
1aabA00,2,LYS,0.950946,83,0.0121951219512195,exterior,0.9219512195121952,C,-148.1,55.8,14,1,2,-0.8227777777777777,0.31,0.84813493,0.97168
1aabA00,3,GLY,0.83723533,83,0.024390243902439,exterior,0.8214285714285714,C,-160.8,81.8,8,1,2,-0.8933333333333334,0.4544444444444444,0.84601504,0.545898
1aabA00,4,ASP,0.675135,83,0.0365853658536585,exterior,0.6012269938650306,C,-70.3,157.5,4,1,2,-0.3905555555555555,0.875,0.81490785,0.505371

=========================================

--- Core Project Files ---

--- File: setup.py ---
---------------------------------------
from setuptools import setup, find_packages
import os

# Read the contents of README file
this_directory = os.path.abspath(os.path.dirname(__file__))
readme_path = os.path.join(this_directory, 'README.md')
long_description = ""
if os.path.exists(readme_path):
    with open(readme_path, encoding='utf-8') as f:
        long_description = f.read()
else:
    print(f"Warning: README.md not found at {readme_path}")


setup(
    name="drwiggle",
    version="1.0.0", # Updated version
    author="AI Assistant (via Prompt)", # Please Change
    author_email="your.email@example.com", # Please Change
    description="Protein Flexibility Classification Framework",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/drwiggle", # Please Change
    # find_packages() will find the 'drwiggle' source directory inside the current dir
    packages=find_packages(where='.'),
    package_dir={'': '.'}, # The 'drwiggle' package is in the current dir
    include_package_data=True, # To include non-code files like default_config.yaml
    install_requires=[
        "numpy>=1.20",
        "pandas>=1.3",
        "scikit-learn>=1.1", # Ensure version supports weighted kappa etc.
        "pyyaml>=6.0",
        "click>=8.0",
        "matplotlib>=3.5",
        "seaborn>=0.11",
        "joblib>=1.0",
        "tqdm>=4.60",
        "torch>=1.10", # Specify version compatible with your CUDA/MPS if needed
        "optuna>=2.10",
        "biopython>=1.79",
        "xgboost>=1.5",
        "lightgbm>=3.0",
    ],
    entry_points={
        "console_scripts": [
            "drwiggle=drwiggle.cli:cli",
        ],
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "License :: OSI Approved :: MIT License", # Choose your license
        "Operating System :: OS Independent",
        "Intended Audience :: Science/Research",
        "Topic :: Scientific/Engineering :: Bio-Informatics",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires='>=3.8',
    package_data={
        # Tells setuptools to include default_config.yaml when the package is installed
        'drwiggle': ['default_config.yaml'],
    },
)

--- End File: setup.py ---

--- File: README.md ---
---------------------------------------
# drWiggle 🧬🔬🤖 <0xF0><0x9F><0xAA><0xB6>

**Protein Flexibility Classification Framework**

[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen)]() <!-- Placeholder: Replace with actual status badge if CI/CD is set up -->
[![Code Coverage](https://img.shields.io/badge/Coverage-N/A-lightgrey)]() <!-- Placeholder: Replace with actual coverage badge -->
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE) <!-- Make sure you have a LICENSE file -->
[![PyPI version](https://img.shields.io/badge/PyPI-v1.0.0-orange)]() <!-- Update version as needed -->

**drWiggle** is a Python framework designed to classify protein residue flexibility based on structural features and Root Mean Square Fluctuation (RMSF) data derived from molecular dynamics simulations or other sources. It transforms continuous RMSF values into discrete flexibility categories and provides an end-to-end pipeline for data processing, model training (Random Forest, Neural Networks with HPO), evaluation, PDB feature integration, visualization (including structural mapping), and temperature-dependent analysis.

## Key Features ✨

*   **RMSF Binning 📊:** Converts continuous RMSF into a configurable number of discrete classes (default: 5, Very Rigid to Very Flexible) using K-means or Quantile-based strategies.
*   **Feature Engineering 🛠️:** Extracts relevant features including sequence context (windowing), amino acid properties, PDB structural properties (B-factor, SS, ACC, Dihedrals), and normalized values.
*   **Machine Learning Models 🧠:** Includes Random Forest and Feed-Forward Neural Network classifiers implemented using Scikit-learn and PyTorch.
*   **Hyperparameter Optimization (HPO) 📈:** Integrated support via Scikit-learn's `RandomizedSearchCV` (for RF) and `Optuna` (for NN) to find optimal model parameters.
*   **PDB Integration <0xF0><0x9F><0xAA><0xBD>:** Fetches PDB files, extracts features directly using BioPython, and requires external DSSP installation for secondary structure and accessibility calculations.
*   **Temperature Analysis 🔥❄️:** Facilitates comparison of classifications and model performance across different temperatures (requires results from multiple runs). Calculates transition matrices.
*   **Command-Line Interface (CLI) 💻:** User-friendly CLI powered by Click for easy pipeline execution (`train`, `evaluate`, `predict`, `process-pdb`, `compare-temperatures`, `analyze-distribution`).
*   **Configuration ⚙️:** Pipeline behavior controlled via YAML configuration files (`default_config.yaml`) with overrides via environment variables (`DRWIGGLE_*`) or CLI parameters.
*   **Visualization 🎨:** Generates plots for RMSF distributions, confusion matrices, feature importance, class distributions, and metric comparisons. Creates PyMOL scripts (`.pml`) and colored PDB files (via B-factor) to map flexibility onto protein structures.
*   **Modularity & Readability ✅:** Code structured into logical modules (data, models, utils, etc.) following Python best practices with type hinting and logging.

## Project Workflow Diagram 🐍 → 📊

This diagram illustrates the data flow and key processes within the `drWiggle` framework:

```mermaid
graph TD
    subgraph Inputs
        A[Input Data (.csv)];
        B[PDB ID/File];
        C[Configuration (YAML/CLI/ENV)];
    end

    subgraph Processing & Training
        D(Load & Process Data);
        E(Parse PDB);
        F(Extract PDB Features);
        G(Calculate Bin Boundaries);
        H(Apply Binning);
        I(Split Data);
        J(Train Models + HPO);
    end

    subgraph Evaluation & Prediction
        K(Evaluate Models);
        L(Predict on New Data);
        M(Compare Temperatures);
        N(Process Single PDB);
    end

    subgraph Outputs
        O[Trained Models (.joblib)];
        P[Binner Object (.joblib)];
        Q[Predictions (.csv)];
        R[Evaluation Reports (.csv/.json)];
        S[Plots (.png)];
        T[Colored PDB / PyMOL Script (.pdb/.pml)];
        U[Temperature Comparison Results];
    end

    %% Connections
    C --> D;
    C --> E;
    C --> F;
    C --> G;
    C --> H;
    C --> I;
    C --> J;
    C --> K;
    C --> L;
    C --> M;
    C --> N;

    A --> D;
    B --> E;
    E --> F;
    F --> D;

    D -- RMSF Values --> G;
    G --> P;
    P --> H;
    D --> H;

    H --> I;
    I -- Train Set --> J;
    I -- Validation Set --> J;
    I -- Test Set --> K;

    J --> O;

    O --> K;
    O --> L;
    O --> N;
    P --> K;  % Need binner for evaluation consistency
    P --> N;  % Need binner for PDB processing if based on bins

    K -- Metrics --> R;
    K -- Confusion Matrix Data --> S;

    L -- Input Features --> O;
    L --> Q;

    N -- PDB Structure & Features --> L;
    N -- Predictions --> T;
    N -- Model --> O;

    M -- Needs Multiple Run Outputs --> U;
    U -- Metrics & Transitions --> R;
    U -- Plots --> S;
```

## Inputs & Outputs 📥📤

This table summarizes the main inputs required and outputs generated by the framework:

| Category      | Item                              | Format / Type          | Description                                                                 | Related Command(s)                     |
| :------------ | :-------------------------------- | :--------------------- | :-------------------------------------------------------------------------- | :------------------------------------- |
| **Inputs**    | Configuration File                | YAML                   | Controls all pipeline parameters (paths, binning, features, models, etc.).  | *All*                                  |
|               | Training/Input Feature Data       | CSV                    | Contains residue features and target RMSF values (for training/binning).    | `train`, `predict`, `analyze-dist`     |
|               | Prediction Input Data             | CSV                    | Contains required features for model prediction.                            | `predict`                              |
|               | PDB ID / File                     | String / Path          | Protein structure for feature extraction and visualization.                 | `process-pdb`                          |
|               | CLI Options / ENV Vars            | String / Variables     | Overrides settings from configuration files.                                | *All*                                  |
|               | Saved Model (for prediction)      | `.joblib` / `.pt`      | Pre-trained model file.                                                     | `predict`, `evaluate`, `process-pdb`   |
|               | Saved Binner (for prediction)     | `.joblib`              | Pre-calculated binner file (needed if prediction involves non-binned RMSF). | `predict` (implicitly), `evaluate`     |
| **Outputs**   | Trained Model                     | `.joblib` / `.pt`      | Serialized model object (RF, NN state dict + scaler).                       | `train`                                |
|               | Binner Object                     | `.joblib`              | Serialized binner object containing calculated boundaries.                  | `train`                                |
|               | Predictions                       | CSV                    | Predicted flexibility classes (and optionally probabilities) per residue.   | `predict`, `evaluate`                  |
|               | Evaluation Summary                | CSV                    | Table comparing metrics (accuracy, F1, etc.) across evaluated models.       | `evaluate`                             |
|               | Detailed Reports                  | JSON / CSV             | Classification report (per-class metrics), Confusion Matrix (counts).     | `evaluate`                             |
|               | Plots                             | PNG                    | Visualizations (RMSF dist, CM, feat importance, metric trends, etc.).     | `train`, `evaluate`, `analyze-dist`... |
|               | Colored PDB / PyMOL Script        | PDB / PML              | Structure files colored by predicted flexibility for visualization software.  | `process-pdb`                          |
|               | Temperature Comparison Results    | CSV / PNG              | Combined metrics, transition matrices, comparison plots.                   | `compare-temperatures`                 |
|               | Extracted PDB Features            | CSV                    | Intermediate file containing features extracted by `process-pdb`.           | `process-pdb`                          |
|               | Log File                          | `.log` (Optional)      | Detailed execution logs (if file handler is configured).                    | *All*                                  |

## Project Structure 🌳

```
drwiggle_project/      # Top-level directory for your project instance
├── data/              # Input data files (e.g., CSVs with RMSF)
├── models/            # Saved trained models (.joblib) and binners (.joblib)
├── output/            # Generated outputs (plots, reports, predictions)
├── pdb_cache/         # Cached downloaded PDB files
└── drwiggle/          # The installable package code lives here
    ├── drwiggle/      # Source code package
    │   ├── __init__.py
    │   ├── cli.py     # Command Line Interface logic (Click)
    │   ├── config.py  # Configuration loading and helpers
    │   ├── pipeline.py # Main workflow orchestration
    │   ├── default_config.yaml # Default settings
    │   ├── data/      # Data loading, processing, binning
    │   │   ├── __init__.py
    │   │   ├── binning.py
    │   │   ├── loader.py
    │   │   └── processor.py
    │   ├── models/    # ML model implementations
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── neural_network.py
    │   │   └── random_forest.py
    │   ├── utils/     # Utility functions
    │   │   ├── __init__.py
    │   │   ├── helpers.py
    │   │   ├── metrics.py
    │   │   ├── pdb_tools.py
    │   │   └── visualization.py
    │   └── temperature/ # Temperature comparison logic
    │       ├── __init__.py
    │       └── comparison.py
    ├── setup.py       # Installation script
    ├── README.md      # This file
    └── tests/         # Unit and integration tests (placeholders)
        └── ...
```

## Installation 🚀

1.  **Clone the repository or create the structure:** Ensure you have the `drwiggle_project` directory containing the inner `drwiggle` package directory as shown above.

2.  **Navigate to the `drwiggle` package directory:**
    ```bash
    cd path/to/drwiggle_project/drwiggle
    ```

3.  **Create and activate a virtual environment (Recommended):**
    ```bash
    python -m venv venv
    # On Linux/macOS
    source venv/bin/activate
    # On Windows
    .\venv\Scripts\activate
    ```

4.  **Upgrade pip and Install the package:**
    ```bash
    python -m pip install --upgrade pip setuptools wheel
    pip install -e .
    ```
    *(The `-e .` installs the package in "editable" mode, linking directly to your source code, which is great for development.)*

5.  **(Optional but Recommended) Install DSSP:** For calculating secondary structure and solvent accessibility from PDB files (`process-pdb` command), install the `dssp` executable (e.g., via `apt`, `yum`, `conda`, `brew`, or from source) and ensure it's in your system's PATH. Alternatively, specify the full path in your configuration file (`pdb.dssp_path`).

## Configuration ⚙️

The pipeline's behavior is controlled primarily by `drwiggle/drwiggle/default_config.yaml`. You can customize runs by overriding these defaults using the following methods (highest precedence first):

1.  **Command-line parameters:** Specific options like `--temperature VALUE` or generic overrides like `--param dataset.split.test_size=0.25`.
2.  **Environment variables:** Prefix keys with `DRWIGGLE_`, use `__` for nesting (e.g., `export DRWIGGLE_BINNING__METHOD=quantile`).
3.  **Custom YAML file:** Pass a user-defined YAML file using `drwiggle --config my_config.yaml ...`.
4.  **Default config:** The settings in `drwiggle/drwiggle/default_config.yaml`.

**Important Note on Paths:** Paths defined in the configuration (`paths` section) are typically *relative to the directory where you execute the `drwiggle` command*. The recommended practice is to run `drwiggle` commands from the main `drwiggle_project` directory.

## Usage Examples 🧑‍💻

Ensure your data (e.g., `temperature_320_train.csv`) is placed in the `drwiggle_project/data` directory. Run commands from the `drwiggle_project` directory.

*   **Train models:**
    ```bash
    # Train default models (all enabled in config) for temperature 320K
    # Uses data matching pattern in config (e.g., data/temperature_320_*.csv)
    # Outputs go to output/, models saved to models/
    drwiggle train --temperature 320

    # Train only Random Forest using a specific config, overriding binning method
    drwiggle train --model random_forest --config drwiggle/my_config.yaml --binning quantile --temperature 348
    ```

*   **Evaluate models:**
    ```bash
    # Evaluate models previously trained for 320K on the test split
    drwiggle evaluate --temperature 320

    # Evaluate specific models on an external test dataset
    drwiggle evaluate --model neural_network --input /path/to/external_test_features.csv --temperature 320
    ```

*   **Predict on new data:**
    ```bash
    # Predict using the saved random_forest model for 379K
    # Input file needs necessary features
    drwiggle predict --input data/my_new_protein_features.csv --temperature 379 --model random_forest --output output/my_new_protein_predictions.csv --probabilities
    ```

*   **Process a PDB file:**
    ```bash
    # Download 1AKE, extract features, predict flexibility at 320K, generate pymol script
    # Assumes a model for 320K exists in models/ and DSSP is installed
    drwiggle process-pdb --pdb 1AKE --temperature 320 --output-prefix output/pdb_vis/1AKE_flex --model random_forest

    # Process a local PDB file
    drwiggle process-pdb --pdb /path/to/my_protein.pdb --temperature 320 --output-prefix output/pdb_vis/my_protein_flex
    ```

*   **Compare temperatures:**
    ```bash
    # Analyze how classifications change across temperatures defined in config
    # Assumes train/evaluate has been run for multiple temperatures storing results in output/
    drwiggle compare-temperatures --output-dir output/temp_comparison --model random_forest
    ```

*   **Analyze RMSF distribution:**
    ```bash
    # Plot the RMSF distribution from a data file and show bin boundaries (requires saved binner)
    drwiggle analyze-distribution --input data/temperature_320_train.csv --temperature 320
    ```

➡️ Use `drwiggle <command> --help` for details on specific commands and options.

## Contributing 🤝

Contributions, bug reports, and feature requests are welcome! Please open an issue or submit a pull request on the project repository. Adhering to code style (e.g., Black, Flake8) and adding tests for new features is appreciated.

## License 📜

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details (assuming you add one).
```

**Key additions and changes:**

1.  **Title/Subtitle:** Clearly defined.
2.  **Emojis:** Added relevant emojis to headings and features.
3.  **Workflow Diagram:** Included a `mermaid` graph illustrating the process flow. This diagram is rendered automatically on platforms like GitHub.
4.  **Inputs & Outputs Table:** Created a detailed table clarifying expected inputs and generated outputs, along with the relevant CLI commands.
5.  **Project Structure:** Included the `tree`-like structure for clarity.
6.  **Installation:** Added step to upgrade pip/setuptools/wheel. Emphasized editable install. Added detail about DSSP.
7.  **Configuration:** Clarified the override precedence and the important note about relative paths.
8.  **Usage:** Provided more varied and descriptive examples.
9.  **Contributing/License:** Standard sections included. Added placeholder badges at the top (update these if you set up CI/CD, code coverage, etc.).

--- End File: README.md ---

--- File: drwiggle/default_config.yaml ---
---------------------------------------
# drWiggle Default Configuration

# --- Paths ---
# IMPORTANT: These paths are relative to the directory where the 'drwiggle' command is run.
# The recommended setup is to run 'drwiggle' from the top 'drwiggle_project' directory.
paths:
  data_dir: ../data                # Input data directory
  output_dir: ../output            # General output directory (plots, reports, predictions)
  models_dir: ../models            # Directory to save/load trained models and binners
  pdb_cache_dir: ../pdb_cache      # Directory to cache downloaded PDB files

# --- Binning Configuration ---
binning:
  method: "kmeans"                # "kmeans" or "quantile"
  num_classes:  5                 # Number of flexibility classes (bins)
  # num_classes:  5                 # Number of flexibility classes (bins)
  kmeans:                         # Settings for K-means binning
    random_state: 42              # Random seed for reproducibility
    max_iter: 300                 # Max iterations for a single run
    n_init: 30                    # Number of times k-means runs with different centroids
  quantile:                       # Settings for Quantile binning
    # Percentiles defining bin edges. Must have num_classes + 1 elements.
    percentiles: [0, 33.3, 66.7, 100] # Roughly equal thirds
    # percentiles: [0, 20, 40, 60, 80, 100] # Equal population
  store_boundaries: true          # Save the calculated bin boundaries along with models

# --- Dataset Configuration ---
dataset:
  # Pattern to find data files if --input is not specified. {temperature} is replaced.
  file_pattern: "temperature_{temperature}_train.csv" # ADJUST based on your filenames
  features:
    # Select features to use as input for the models
    use_features:
      # Basic Features (typically present in input CSV or calculable)
      protein_size: true               # Total number of residues in the domain/protein
      normalized_resid: true           # Residue index normalized to [0, 1] within the domain
      resname_encoded: true            # Amino acid type encoded numerically

      # PDB-derived Features (require --pdb or pre-calculation)
      b_factor: false                   # B-factor from PDB (if pdb.enabled and pdb.features.b_factor)
      relative_accessibility: true    # Relative solvent accessibility (requires DSSP)
      core_exterior_encoded: true     # Encoded residue location (requires calculation/annotation)
      secondary_structure_encoded: true # Encoded secondary structure (requires DSSP)
      phi_norm: true                   # Normalized Phi dihedral angle (requires PDB processing)
      psi_norm: true                   # Normalized Psi dihedral angle (requires PDB processing)
      esm_rmsf: true
      voxel_rmsf: true

    # Window Features (Sequence Context)
    window:
      enabled: true               # Use window-based features around the central residue
      size: 3                     # Number of residues on EACH side (e.g., 3 -> window of 7)
      # Features included in the window are those enabled above (e.g., resname_encoded, ss_encoded)

  # Target Column (Continuous RMSF)
  # {temperature} is replaced based on the run context.
  target: rmsf_{temperature}      # Column name containing the continuous RMSF values for binning

  # Data Splitting (Train/Validation/Test)
  split:
    test_size: 0.1               # Proportion of data for the final test set
    validation_size: 0.1         # Proportion of ORIGINAL data for the validation set
    stratify_by_domain: true      # Keep all residues of a domain in the same split (requires 'domain_id' column)
    random_state: 42              # Seed for reproducible splits

# --- Model Configurations ---
models:
  # Shared settings applicable to all models unless overridden
  common:
    # Cross-validation settings (currently primarily used by HPO)
    cross_validation:
      enabled: false              # Whether to use CV during main training (pipeline.train doesn't use this yet)
      folds: 5                    # Number of CV folds
    save_best: true               # Save the model that performs best on the validation set

  lightgbm:
    enabled: true                 # Train and evaluate this model
    # Core LightGBM parameters
    objective: 'multiclass'
    metric: 'multi_logloss'       # Metric for internal evaluation and early stopping
    n_estimators: 100
    learning_rate: 0.1
    num_leaves: 31                # Main parameter to control complexity (usually < 2^max_depth)
    max_depth: -1                 # -1 means no limit, complexity controlled by num_leaves
    subsample: 0.8                # Fraction of samples per tree (alias: bagging_fraction)
    colsample_bytree: 0.8         # Fraction of features per tree (alias: feature_fraction)
    reg_alpha: 0.0                # L1 regularization
    reg_lambda: 0.0               # L2 regularization
    class_weight: 'balanced'      # Handles class imbalance directly

    # Training specific settings
    training:
      early_stopping_rounds: 10   # Stop if eval metric doesn't improve (requires validation set)
      verbose: false              # Set LightGBM's internal verbosity during fit callbacks

    # HPO using RandomizedSearchCV
    randomized_search:
      enabled: false               # Enable hyperparameter optimization
      n_iter: 2                  # Number of parameter combinations to try
      cv: 1                       # Number of cross-validation folds within HPO
      scoring: "balanced_accuracy" # Metric to optimize during HPO
      param_distributions:        # Parameter search space for LightGBM
        n_estimators: [50, 100, 200, 300, 500, 700]
        learning_rate: [0.01, 0.05, 0.1, 0.2]
        num_leaves: [15, 31, 50, 70, 100] # Key tuning parameter
        # max_depth: [5, 7, 10, -1] # Often less critical if num_leaves is tuned
        subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
        colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
        reg_alpha: [0, 0.01, 0.1, 1]
        reg_lambda: [0, 0.01, 0.1, 1]

  xgboost:
    enabled: true                 # Train and evaluate this model
    # Core XGBoost parameters (used if HPO is disabled)
    objective: 'multi:softprob'   # Output probabilities for each class
    eval_metric: 'mlogloss'       # Multiclass logloss for evaluation/early stopping
    n_estimators: 100             # Number of boosting rounds
    learning_rate: 0.1            # Step size shrinkage
    max_depth: 6                  # Maximum tree depth
    subsample: 0.8                # Fraction of samples used per tree
    colsample_bytree: 0.8         # Fraction of features used per tree
    gamma: 0                      # Minimum loss reduction required to make a further partition
    reg_alpha: 0                  # L1 regularization term on weights
    reg_lambda: 1                 # L2 regularization term on weights (default in XGBoost)
    use_sample_weights: true      # Calculate and use sample weights for class imbalance

    # Training specific settings (like early stopping)
    training:
      early_stopping_rounds: 10   # Stop if eval metric doesn't improve for this many rounds (requires validation set)
      verbose: false              # Print eval metrics during training if True

    # HPO using RandomizedSearchCV
    randomized_search:
      enabled: false               # Enable hyperparameter optimization
      n_iter: 30                  # Number of parameter combinations to try (increase for better search)
      cv: 3                       # Number of cross-validation folds within HPO
      scoring: "balanced_accuracy" # Metric to optimize during HPO
      param_distributions:        # Parameter search space for XGBoost
        n_estimators: [50, 100, 200, 300, 500]
        learning_rate: [0.01, 0.05, 0.1, 0.2] # Could use scipy.stats.uniform(0.01, 0.2) for continuous search
        max_depth: [3, 5, 7, 9, 11]
        subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
        colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
        gamma: [0, 0.1, 0.5, 1]
        reg_alpha: [0, 0.01, 0.1, 1]
        reg_lambda: [0.5, 1, 1.5, 2]

  # Random Forest Model
  random_forest:
    enabled: true                 # Train and evaluate this model
    # Core RF parameters (used if HPO is disabled)
    n_estimators: 100
    max_depth: null               # null means nodes expanded until all leaves pure or min_samples_split
    min_samples_split: 2
    min_samples_leaf: 1
    class_weight: "balanced"      # Adjusts weights inversely proportional to class frequencies
    # HPO using RandomizedSearchCV
    randomized_search:
      enabled: false               # Enable hyperparameter optimization
      n_iter: 2                  # Number of parameter combinations to try
      cv: 2                       # Number of cross-validation folds within HPO
      scoring: "balanced_accuracy" # Metric to optimize during HPO
      param_distributions:        # Parameter search space
        n_estimators: [50, 100, 200, 300, 400]
        max_depth: [null, 10, 20, 30, 40, 50]
        min_samples_split: [2, 5, 10, 15]
        min_samples_leaf: [1, 2, 4, 6]
        class_weight: ["balanced", "balanced_subsample", null]
        # max_features: ['sqrt', 'log2', 0.5, 0.7] # Consider adding if many features
  

  # Neural Network Model
  neural_network:
    enabled: true                 # Train and evaluate this model
    architecture:
      # Default architecture if HPO is disabled or parameters aren't tuned
      hidden_layers: [128, 64, 32] # Sizes of hidden layers
      activation: "relu"          # Activation function ('relu' or 'leaky_relu')
      dropout: 0.3                # Dropout rate applied after activation in hidden layers
      ordinal_output: false       # If true, uses a simplified ordinal setup (MSE loss - EXPERIMENTAL). False = standard classification.

    training:
      # Default training parameters if HPO is disabled or parameters aren't tuned
      optimizer: "adam"           # 'adam' or 'sgd'
      learning_rate: 0.001
      batch_size: 64
      epochs: 15                 # Maximum number of training epochs
      early_stopping: true        # Enable early stopping based on validation loss
      patience: 15                # Number of epochs to wait for improvement before stopping
      class_weights: true         # Compute and use class weights for CrossEntropyLoss (ignored if ordinal_output=true)

    # HPO using Optuna
    hyperparameter_optimization:
      enabled: false               # Enable hyperparameter optimization
      method: "random"            # Optuna sampler ('random', 'tpe', etc.) - only random is simple default
      trials: 30                  # Number of HPO trials to run
      objective_metric: "val_loss" # Metric to optimize ('val_loss' or 'val_accuracy') - Optuna minimizes
      parameters:                 # Parameter search space
        hidden_layers: # Suggest choices for layer structures
          - [64, 32]
          - [128, 64]
          - [256, 128]
          - [128, 64, 32]
          - [256, 128, 64]
        learning_rate: [0.0001, 0.01] # Suggest Float (log uniform)
        batch_size: [32, 64, 128]     # Suggest Categorical
        dropout: [0.1, 0.5]          # Suggest Float (uniform)
        activation: ["relu", "leaky_relu"] # Suggest Categorical
        # Note: optimizer is not included in default search space but could be added

# --- Evaluation Settings ---
evaluation:
  metrics: # Which metrics to calculate and report
    accuracy: true
    balanced_accuracy: true
    precision: true               # Reports macro and weighted averages
    recall: true                  # Reports macro and weighted averages
    f1: true                      # Reports macro and weighted averages
    confusion_matrix: true        # Requires saving/plotting separately
    classification_report: true   # Requires saving separately
    cohen_kappa: true
    weighted_kappa: true          # Quadratic weighted kappa for ordinal agreement
    ordinal_error: true           # Mean Absolute Error between predicted and true class indices

  # Class names used in reports and visualizations
  # Should have 'num_classes' entries, indexed from 0
  class_names:
    0: "Very Rigid"
    1: "Rigid"
    2: "Moderately Flexible"
    3: "Flexible"
    4: "Very Flexible"

# --- Visualization Settings ---
visualization:
  # Colors used for classes in plots and structure visualizations
  colors:
    0: "#0C2D48"  # Very Rigid (dark blue)
    1: "#145DA0"  # Rigid (blue)
    2: "#B1D4E0"  # Moderately Flexible (light blue)
    3: "#E8AB30"  # Flexible (orange)
    4: "#D1603D"  # Very Flexible (red)
  # Which plots to generate during the pipeline run
  plots:
    bin_distribution: true        # Plot RMSF distribution with bin boundaries (after binning)
    confusion_matrix: true        # Plot confusion matrix (during evaluation)
    feature_importance: true      # Plot feature importance (after training RF model)
    class_distribution: false     # Plot class distribution (can be generated separately via 'visualize' command)
    protein_visualization: true   # Generate PyMOL script (during process-pdb)
    temperature_comparison: true  # Generate plots for temperature comparison results

# --- Temperature Analysis Configuration ---
temperature:
  # Default temperature if not specified via CLI or env var.
  # Also used for finding default data file if --input is not given.
  current: 320
  # List of temperatures available in your dataset (used by compare-temperatures command)
  # Can include strings like "average" if you have averaged data.
  available: [320, 348, 379, 413, 450] # Example temperatures
  comparison:
    enabled: true                 # Enable the 'compare-temperatures' command functionality
    # How to handle binning when comparing across temperatures
    # "global": Use bins defined by the 'current' temperature run for all comparisons.
    # "specific": (Not fully implemented) Requires retraining/loading bins for each specific temperature.
    binning_strategy: "global"
    # Metrics to focus on in the comparison report/plots
    metrics: ["accuracy", "balanced_accuracy", "ordinal_error", "weighted_kappa"]
    plot_transition_matrix: true  # Plot class transition matrix between temperature pairs (TBD)

# --- PDB Integration Settings ---
pdb:
  enabled: true                   # Master switch to enable PDB fetching and feature extraction
  # Path to the DSSP executable. If null, searches PATH. REQUIRED for SS and ACC features.
  dssp_path: null                 # Example: "/usr/local/bin/dssp" or "mkdssp" if in path
  features:                       # Which features to extract/calculate from PDB
    b_factor: true                # Extract B-factors
    secondary_structure: true     # Calculate secondary structure (needs DSSP)
    solvent_accessibility: true   # Calculate relative solvent accessibility (needs DSSP)
    dihedral_angles: true         # Calculate phi, psi, omega angles

# --- System Settings ---
system:
  n_jobs: -1                      # Number of CPU cores (-1 = use all available) for parallel tasks (e.g., RF training, HPO CV)
  random_state: 42                # Global random seed for reproducibility across the pipeline
  log_level: "INFO"               # Logging level ("DEBUG", "INFO", "WARNING", "ERROR")
  gpu_enabled: "auto"             # "auto": Use GPU if available (CUDA/MPS), true: Require GPU, false: Force CPU

# 5xh3. 
# - RMSF data. 
# - 

# 6eqe. 

--- End File: drwiggle/default_config.yaml ---

=========================================
      Python Source Code Files           
=========================================

--- File: drwiggle/cli.py ---
---------------------------------------
import logging
import click
import os
import sys
import pandas as pd
from typing import Optional, Tuple
import traceback


# --- Configure Logging ---
# Basic config here, gets potentially overridden by config file later in load_config
logging.basicConfig(
    level=logging.INFO, # Default level
    format='%(asctime)s | %(levelname)-8s | %(name)-15s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[logging.StreamHandler(sys.stdout)] # Log to stdout
)
# Silence overly verbose libraries by default
logging.getLogger("matplotlib").setLevel(logging.WARNING)
logging.getLogger("numexpr").setLevel(logging.WARNING)

logger = logging.getLogger("drwiggle.cli") # Logger specific to CLI

# --- Import Core Components ---
# Defer heavy imports until commands are run if possible
# (though pipeline import might trigger others)
from drwiggle.config import load_config
from drwiggle.pipeline import Pipeline


# --- Helper Functions ---
def _setup_pipeline(ctx, config_path: Optional[str], param_overrides: Optional[Tuple[str]], cli_option_overrides: dict) -> Pipeline:
    """Loads config and initializes the pipeline."""
    try:
        # Pass CLI overrides directly to load_config
        # Resolve paths relative to the current working directory (where CLI is run)
        cfg = load_config(
            config_path=config_path,
            cli_overrides=cli_option_overrides,
            param_overrides=param_overrides,
            resolve_paths_base_dir=os.getcwd() # Resolve relative to CWD
        )
        # Store config in context for potential use by other commands if needed
        ctx.obj = cfg
        pipeline = Pipeline(cfg)
        return pipeline
    except FileNotFoundError as e:
        logger.error(f"Configuration Error: {e}")
        sys.exit(1)
    except (ValueError, TypeError, KeyError) as e:
         logger.error(f"Configuration Error: Invalid setting or structure - {e}", exc_info=True)
         sys.exit(1)
    except Exception as e:
        logger.error(f"Failed to initialize pipeline: {e}", exc_info=True)
        sys.exit(1)

# --- Click CLI Definition ---

@click.group(context_settings=dict(help_option_names=['-h', '--help']))
@click.version_option(version="1.0.0", package_name="drwiggle") # Assumes setup.py version
@click.option('--config', '-c', type=click.Path(exists=True, dir_okay=False), help='Path to custom YAML config file.')
@click.option('--param', '-p', multiple=True, help='Override config param (key.subkey=value). Can be used multiple times.')
@click.pass_context # Pass context to store config
def cli(ctx, config, param):
    """
    drWiggle: Protein Flexibility Classification Framework.

    Train models, evaluate performance, predict flexibility, and analyze results
    across different temperatures based on RMSF data and structural features.

    Configuration is loaded from default_config.yaml, overridden by the --config file,
    environment variables (DRWIGGLE_*), and finally --param options.
    """
    # Store base config path and params in context for commands to use
    # The actual config loading happens within each command using _setup_pipeline
    ctx.ensure_object(dict)
    ctx.obj['config_path'] = config
    ctx.obj['param_overrides'] = param
    logger.info("drWiggle CLI started.")


@cli.command()
@click.option("--model", '-m', help="Model(s) to train (comma-separated, e.g., 'random_forest,neural_network'). Default: all enabled in config.")
@click.option("--input", '-i', type=click.Path(resolve_path=True), help="Input data file/pattern. Overrides 'dataset.file_pattern' in config.")
@click.option("--temperature", '-t', type=str, help="Temperature context (e.g., 320). Overrides 'temperature.current'. REQUIRED if data pattern uses {temperature}.")
@click.option("--binning", '-b', type=click.Choice(["kmeans", "quantile"], case_sensitive=False), help="Override binning method.")
@click.option("--output-dir", '-o', type=click.Path(resolve_path=True), help="Override 'paths.output_dir'.")
@click.option("--models-dir", type=click.Path(resolve_path=True), help="Override 'paths.models_dir'.")
@click.pass_context
def train(ctx, model, input, temperature, binning, output_dir, models_dir):
    """Train flexibility classification model(s)."""
    logger.info("=== Train Command Initiated ===")
    # Prepare CLI overrides dictionary for load_config
    cli_overrides = {}
    if temperature: cli_overrides.setdefault('temperature', {})['current'] = temperature
    if binning: cli_overrides.setdefault('binning', {})['method'] = binning
    if output_dir: cli_overrides.setdefault('paths', {})['output_dir'] = output_dir
    if models_dir: cli_overrides.setdefault('paths', {})['models_dir'] = models_dir
    # Input override needs careful handling - pass directly to pipeline method
    # if input: cli_overrides.setdefault('dataset', {})['file_pattern'] = input # This isn't quite right, input can be path

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    # Temperature check: crucial if file_pattern uses {temperature} and --input not given
    if input is None and '{temperature}' in pipeline.config['dataset']['file_pattern']:
         current_temp = pipeline.config.get("temperature", {}).get("current")
         if current_temp is None:
              logger.error("Training data pattern requires {temperature}, but temperature not set via --temperature or config.")
              sys.exit(1)
         logger.info(f"Using temperature {current_temp} for finding training data.")


    model_list = model.split(',') if model else None # Pass None to train all enabled

    try:
        pipeline.train(model_names=model_list, data_path=input)
        logger.info("=== Train Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"Training pipeline failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--model", '-m', help="Model(s) to evaluate (comma-separated). Default: All models found in models_dir.")
@click.option("--input", '-i', type=click.Path(resolve_path=True), help="Evaluate on specific data file/pattern. Default: Use test split from training data source.")
@click.option("--temperature", '-t', type=str, help="Temperature context for loading models/data (e.g., 320). REQUIRED if default data pattern needs it.")
@click.option("--output-dir", '-o', type=click.Path(resolve_path=True), help="Override 'paths.output_dir'.")
@click.option("--models-dir", type=click.Path(resolve_path=True), help="Override 'paths.models_dir'.")
@click.pass_context
def evaluate(ctx, model, input, temperature, output_dir, models_dir):
    """Evaluate trained classification model(s)."""
    logger.info("=== Evaluate Command Initiated ===")
    cli_overrides = {}
    if temperature: cli_overrides.setdefault('temperature', {})['current'] = temperature
    if output_dir: cli_overrides.setdefault('paths', {})['output_dir'] = output_dir
    if models_dir: cli_overrides.setdefault('paths', {})['models_dir'] = models_dir

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    # Temperature check if default test set derivation needs it
    if input is None and '{temperature}' in pipeline.config['dataset']['file_pattern']:
         current_temp = pipeline.config.get("temperature", {}).get("current")
         if current_temp is None:
              logger.error("Deriving test set requires {temperature} in data pattern, but temperature not set via --temperature or config.")
              sys.exit(1)
         logger.info(f"Using temperature {current_temp} for deriving test set.")

    model_list = model.split(',') if model else None

    try:
        pipeline.evaluate(model_names=model_list, data_path=input)
        logger.info("=== Evaluate Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"Evaluation pipeline failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--input", '-i', type=click.Path(exists=True, dir_okay=False, resolve_path=True), required=True, help="Input data file (CSV) for prediction.")
@click.option("--model", '-m', type=str, help="Model name to use. Default: 'random_forest'.")
@click.option("--output", '-o', type=click.Path(resolve_path=True), help="Output file path for predictions (CSV). Default: derive from input filename.")
@click.option("--temperature", '-t', type=str, help="Temperature context for loading model (e.g., 320). Sets 'temperature.current' in config.")
@click.option("--probabilities", is_flag=True, default=False, help="Include class probabilities in output.")
@click.option("--models-dir", type=click.Path(resolve_path=True), help="Override 'paths.models_dir'.")
@click.pass_context
def predict(ctx, input, model, output, temperature, probabilities, models_dir):
    """Predict flexibility classes for new data."""
    logger.info("=== Predict Command Initiated ===")
    cli_overrides = {}
    if temperature: cli_overrides.setdefault('temperature', {})['current'] = temperature
    if models_dir: cli_overrides.setdefault('paths', {})['models_dir'] = models_dir
    # Store probability flag for pipeline to access
    cli_overrides.setdefault('cli_options', {})['predict_probabilities'] = probabilities

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    if not output:
        base, ext = os.path.splitext(input)
        output = f"{base}_predictions.csv"
        logger.info(f"Output path not specified, defaulting to: {output}")

    try:
        predictions_df = pipeline.predict(data=input, model_name=model, output_path=output)
        # Predict method handles saving if output_path is given
        if predictions_df is not None:
             # This happens if output_path wasn't specified or saving failed
             logger.info("Prediction method returned DataFrame (likely because output_path was None or saving failed).")
        logger.info("=== Predict Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"Prediction pipeline failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--pdb", required=True, help="PDB ID (e.g., '1AKE') or path to a local PDB file.")
@click.option("--model", '-m', type=str, help="Model name to use for prediction. Default: 'random_forest'.")
@click.option("--temperature", '-t', type=str, help="Temperature context for prediction model (e.g., 320). Sets 'temperature.current'.")
@click.option("--output-prefix", '-o', type=click.Path(resolve_path=True), help="Output prefix for generated files (e.g., ./output/1ake_flex). Default: '{output_dir}/pdb_vis/{pdb_id}_{model}_flexibility'")
@click.option("--models-dir", type=click.Path(resolve_path=True), help="Override 'paths.models_dir'.")
@click.option("--pdb-cache-dir", type=click.Path(resolve_path=True), help="Override 'paths.pdb_cache_dir'.")
@click.pass_context
def process_pdb(ctx, pdb, model, temperature, output_prefix, models_dir, pdb_cache_dir):
    """Fetch/Parse PDB, Extract Features, Predict Flexibility, and Generate Visualizations."""
    logger.info("=== Process PDB Command Initiated ===")
    cli_overrides = {}
    if temperature: cli_overrides.setdefault('temperature', {})['current'] = temperature
    if models_dir: cli_overrides.setdefault('paths', {})['models_dir'] = models_dir
    if pdb_cache_dir: cli_overrides.setdefault('paths', {})['pdb_cache_dir'] = pdb_cache_dir
    # Ensure PDB processing is enabled in the loaded config
    cli_overrides.setdefault('pdb', {})['enabled'] = True

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    try:
        pipeline.process_pdb(pdb_input=pdb, model_name=model, output_prefix=output_prefix)
        logger.info("=== Process PDB Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"PDB processing pipeline failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--input", '-i', type=click.Path(exists=True, dir_okay=False, resolve_path=True), required=True, help="Input RMSF data file (CSV) for analysis.")
@click.option("--temperature", '-t', type=str, help="Temperature context (e.g., 320).")
@click.option("--output-dir", '-o', type=click.Path(resolve_path=True), help="Directory to save the plot. Overrides 'paths.output_dir'.")
@click.option("--models-dir", type=click.Path(resolve_path=True), help="Directory containing saved binner. Overrides 'paths.models_dir'.")
@click.pass_context
def analyze_distribution(ctx, input, temperature, output_dir, models_dir):
    """Analyze RMSF distribution and visualize binning boundaries."""
    logger.info("=== Analyze Distribution Command Initiated ===")
    cli_overrides = {}
    if temperature: cli_overrides.setdefault('temperature', {})['current'] = temperature
    if output_dir: cli_overrides.setdefault('paths', {})['output_dir'] = output_dir
    if models_dir: cli_overrides.setdefault('paths', {})['models_dir'] = models_dir

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    plot_filename = f"rmsf_distribution_analysis_{temperature or 'default'}.png"
    plot_path = os.path.join(pipeline.config['paths']['output_dir'], plot_filename)

    try:
        pipeline.analyze_rmsf_distribution(input_data_path=input, output_plot_path=plot_path)
        logger.info("=== Analyze Distribution Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"RMSF distribution analysis failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--model", '-m', type=str, help="Model name to focus comparison on (optional).")
@click.option("--output-dir", '-o', type=click.Path(resolve_path=True), help="Override base 'paths.output_dir' for finding results and saving comparison.")
@click.pass_context
def compare_temperatures(ctx, model, output_dir):
    """Compare classification results across different temperatures."""
    logger.info("=== Compare Temperatures Command Initiated ===")
    cli_overrides = {}
    # Output dir override applies to the base dir where temp results are sought
    if output_dir: cli_overrides.setdefault('paths', {})['output_dir'] = output_dir
    # Temperature override doesn't make sense here as we compare multiple temps

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    try:
        pipeline.run_temperature_comparison(model_name=model)
        logger.info("=== Compare Temperatures Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"Temperature comparison failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


@cli.command()
@click.option("--predictions", type=click.Path(exists=True, dir_okay=False, resolve_path=True), required=True, help="Path to the predictions CSV file (must contain 'predicted_class').")
@click.option("--output-dir", '-o', type=click.Path(resolve_path=True), help="Directory to save visualizations. Overrides 'paths.output_dir'.")
@click.pass_context
def visualize(ctx, predictions, output_dir):
    """Generate visualizations from saved prediction files."""
    logger.info("=== Visualize Command Initiated ===")
    cli_overrides = {}
    if output_dir: cli_overrides.setdefault('paths', {})['output_dir'] = output_dir
    # Temperature override might be needed if config depends on it for vis settings
    # Add --temperature option if necessary later.

    pipeline = _setup_pipeline(ctx, ctx.obj['config_path'], ctx.obj['param_overrides'], cli_overrides)

    try:
        pipeline.visualize_results(predictions_path=predictions, output_dir=output_dir) # Pass specified output dir
        logger.info("=== Visualize Command Finished Successfully ===")
    except Exception as e:
        logger.error(f"Visualization generation failed: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


# Main entry point for script execution
if __name__ == '__main__':
    # Set process title if possible (useful for monitoring)
    try:
        import setproctitle
        setproctitle.setproctitle("drwiggle_cli")
    except ImportError:
        pass
    # Execute the Click application
    cli()

--- End File: drwiggle/cli.py ---

--- File: drwiggle/config.py ---
---------------------------------------
import os
import logging
import yaml
from typing import Dict, Any, Optional, List, Union, Tuple
import copy # For deep copying config
import warnings

logger = logging.getLogger(__name__)

DEFAULT_CONFIG_FILENAME = "default_config.yaml"

def deep_merge(base_dict: Dict, overlay_dict: Dict) -> Dict:
    """
    Recursively merge two dictionaries. Values from overlay_dict take precedence.
    Modifies base_dict in place. Handles nested dictionaries.
    """
    for key, value in overlay_dict.items():
        if key in base_dict:
            # If both are dicts, recurse
            if isinstance(base_dict[key], dict) and isinstance(value, dict):
                deep_merge(base_dict[key], value)
            # If overlay value is not None, overwrite (allows explicit nulls)
            elif value is not None:
                 base_dict[key] = value
            # If overlay value is None, keep base value (allows unsetting via None in overlay)
            # This behavior might need adjustment depending on desired merge logic for None.
            # Currently: None in overlay means "don't change the base value".
            # To explicitly set a value to None, the overlay should contain `key: null` in YAML.
            else:
                pass # Keep base_dict[key]
        else:
            # Key not in base, add it
            base_dict[key] = value
    return base_dict


def _get_default_config_path() -> str:
    """Find the default config file path relative to this config.py script."""
    # This assumes default_config.yaml is in the same directory as config.py
    return os.path.join(os.path.dirname(__file__), DEFAULT_CONFIG_FILENAME)

def _parse_value(value: str) -> Any:
    """Try to parse string value into bool, int, float, list, or keep as string."""
    val_lower = value.lower()
    if val_lower == "true": return True
    if val_lower == "false": return False
    if val_lower in ["null", "none", ""]: return None
    try: return int(value)
    except ValueError:
        try: return float(value)
        except ValueError:
            # Try parsing comma-separated lists (e.g., "1,2,3" -> [1, 2, 3])
            if ',' in value:
                try:
                    # Attempt to parse each element as int/float/string
                    return [_parse_value(item.strip()) for item in value.split(',')]
                except Exception:
                    pass # Fallback to string if list parsing fails
            return value # Keep as string

def _set_nested_value(d: Dict, keys: List[str], value: Any):
    """Set a value in a nested dictionary using a list of keys."""
    for key in keys[:-1]:
        # If the key exists but isn't a dict, behavior is undefined.
        # For robustness, maybe raise error or overwrite? Currently overwrites.
        if key not in d or not isinstance(d[key], dict):
             d[key] = {} # Ensure intermediate keys are dicts
        d = d[key]

    # Check if the final key exists and is a dictionary, and the value is also a dictionary
    # This indicates a potential merge scenario instead of overwrite, especially for HPO params
    if keys[-1] in d and isinstance(d[keys[-1]], dict) and isinstance(value, dict):
        deep_merge(d[keys[-1]], value) # Merge if both are dicts
    else:
        d[keys[-1]] = value # Otherwise, overwrite/set

def _get_env_var_config() -> Dict[str, Any]:
    """Load configuration overrides from environment variables (prefixed DRWIGGLE_)."""
    config = {}
    prefix = "DRWIGGLE_"
    for env_var, value_str in os.environ.items():
        if env_var.startswith(prefix):
            key_str = env_var[len(prefix):].lower()
            # Split by double underscore for nesting, single for parts of name
            keys = key_str.split('__') # e.g., DRWIGGLE_MODELS__RANDOM_FOREST__N_ESTIMATORS
            parsed_keys = []
            for k in keys:
                 parsed_keys.extend(k.split('_')) # Split remaining parts by single underscore

            value = _parse_value(value_str)
            try:
                 _set_nested_value(config, parsed_keys, value)
                 logger.debug(f"Loaded from env: {env_var} -> {parsed_keys} = {value}")
            except (TypeError, IndexError) as e:
                 logger.warning(f"Could not set nested value from env var {env_var} (keys: {parsed_keys}): {e}")
    return config

def _parse_param_overrides(params: Optional[Tuple[str]]) -> Dict[str, Any]:
    """Parse --param key.subkey=value overrides from CLI."""
    config = {}
    if not params:
        return config
    for param in params:
        if '=' not in param:
            logger.warning(f"Ignoring invalid param override (no '='): {param}")
            continue
        key_str, value_str = param.split('=', 1)
        keys = key_str.split('.') # Use dot for nesting in CLI params
        value = _parse_value(value_str)
        try:
            _set_nested_value(config, keys, value)
            logger.debug(f"Loaded from CLI param: {param} -> {keys} = {value}")
        except (TypeError, IndexError) as e:
             logger.warning(f"Could not set nested value from CLI param {param} (keys: {keys}): {e}")
    return config

def _resolve_paths(config: Dict[str, Any], base_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    Resolve relative paths in the config, making them absolute.
    Paths under the 'paths' key ending in '_dir' or '_file' are resolved.
    Relative paths are resolved relative to 'base_dir'.
    If 'base_dir' is None, uses the current working directory.
    """
    resolved_config = copy.deepcopy(config)
    if base_dir is None:
        base_dir = os.getcwd()
        logger.debug(f"No base directory provided for path resolution, using CWD: {base_dir}")

    paths_config = resolved_config.get('paths', {})
    if paths_config:
        for key, value in paths_config.items():
            if isinstance(value, str) and (key.endswith('_dir') or key.endswith('_file') or key.endswith('_path')):
                if not os.path.isabs(value):
                    abs_path = os.path.abspath(os.path.join(base_dir, value))
                    paths_config[key] = abs_path
                    logger.debug(f"Resolved relative path '{key}': '{value}' -> '{abs_path}' (relative to {base_dir})")
                else:
                     logger.debug(f"Path '{key}' is already absolute: '{value}'")

    # Resolve dssp_path if present and relative
    pdb_config = resolved_config.get('pdb', {})
    if pdb_config:
         dssp_path = pdb_config.get('dssp_path')
         if dssp_path and isinstance(dssp_path, str) and not os.path.isabs(dssp_path):
             # Assume dssp_path is relative to base_dir if not absolute
             # Although usually it refers to an executable in PATH or an absolute location
             abs_dssp_path = os.path.abspath(os.path.join(base_dir, dssp_path))
             pdb_config['dssp_path'] = abs_dssp_path
             logger.debug(f"Resolved relative dssp_path: '{dssp_path}' -> '{abs_dssp_path}'")

    return resolved_config


def _template_config(config: Dict[str, Any], temperature: Union[int, str]) -> Dict[str, Any]:
    """Recursively replace {temperature} placeholders in config strings."""
    templated_config = copy.deepcopy(config) # Avoid modifying original

    def _recursive_replace(item):
        if isinstance(item, dict):
            return {k: _recursive_replace(v) for k, v in item.items()}
        elif isinstance(item, list):
            return [_recursive_replace(elem) for elem in item]
        elif isinstance(item, str):
            # Format the string using a dict to avoid errors if placeholder missing
            try:
                 return item.format(temperature=temperature)
            except KeyError:
                 return item # Return original string if placeholder not found
        else:
            return item

    return _recursive_replace(templated_config)


def load_config(
    config_path: Optional[str] = None,
    cli_overrides: Optional[Dict[str, Any]] = None,
    param_overrides: Optional[Tuple[str]] = None,
    resolve_paths_base_dir: Optional[str] = None # Base dir for resolving relative paths
) -> Dict[str, Any]:
    """
    Loads and merges configuration from multiple sources with precedence.

    Precedence order (highest first):
    1. Command-line --param overrides (`param_overrides`)
    2. Command-line specific options (`cli_overrides`, e.g., --temperature, --output-dir)
    3. Environment variables (DRWIGGLE_*)
    4. User-provided config file (`config_path`)
    5. Default config file (`drwiggle/default_config.yaml`)

    Args:
        config_path: Path to a user-specific YAML config file.
        cli_overrides: Dict of overrides from specific CLI options (e.g., {'temperature': {'current': 320}}).
        param_overrides: Tuple of "key.subkey=value" strings from generic --param CLI option.
        resolve_paths_base_dir: The directory relative to which paths in the 'paths' section
                                should be resolved. Defaults to CWD if None.

    Returns:
        The final, merged, path-resolved, and templated configuration dictionary.

    Raises:
        FileNotFoundError: If the default or specified config file cannot be found.
        yaml.YAMLError: If a config file is invalid YAML.
    """
    # 1. Load Default Config
    default_cfg_path = _get_default_config_path()
    if not os.path.exists(default_cfg_path):
        # Try finding relative to current working directory as fallback
        default_cfg_path_cwd = os.path.join(os.getcwd(), "drwiggle", DEFAULT_CONFIG_FILENAME)
        if os.path.exists(default_cfg_path_cwd):
             default_cfg_path = default_cfg_path_cwd
             logger.debug(f"Found default config relative to CWD: {default_cfg_path}")
        else:
             raise FileNotFoundError(f"Default config file not found at package location ({default_cfg_path}) or relative to CWD.")

    try:
        with open(default_cfg_path, 'r') as f:
            config = yaml.safe_load(f)
        if config is None: config = {} # Handle empty default file
        logger.debug(f"Loaded default config from {default_cfg_path}")
    except yaml.YAMLError as e:
        logger.error(f"Error parsing default config file {default_cfg_path}: {e}")
        raise

    # 2. Overlay User Config
    if config_path:
        abs_config_path = os.path.abspath(config_path)
        if not os.path.exists(abs_config_path):
            raise FileNotFoundError(f"User config file not found: {abs_config_path}")
        try:
            with open(abs_config_path, 'r') as f:
                user_config = yaml.safe_load(f)
            if user_config: # Check if file is not empty and YAML is valid dict
                 config = deep_merge(config, user_config)
                 logger.debug(f"Merged user config from {abs_config_path}")
            else:
                 logger.warning(f"User config file {abs_config_path} is empty or invalid YAML dictionary.")
        except yaml.YAMLError as e:
            logger.error(f"Error parsing user config file {abs_config_path}: {e}")
            raise

    # 3. Overlay Environment Variables
    env_config = _get_env_var_config()
    if env_config:
        config = deep_merge(config, env_config)
        logger.debug("Merged configuration from environment variables.")

    # 4. Overlay Specific CLI Overrides (processed by Click)
    if cli_overrides:
         config = deep_merge(config, cli_overrides)
         logger.debug(f"Merged configuration from specific CLI options: {cli_overrides}")

    # 5. Overlay Generic CLI --param Overrides
    cli_param_config = _parse_param_overrides(param_overrides)
    if cli_param_config:
        config = deep_merge(config, cli_param_config)
        logger.debug("Merged configuration from CLI --param overrides.")

    # --- Post-Merge Processing ---

    # 6. Handle Temperature Templating BEFORE resolving paths
    # Determine the temperature to use *after* all overrides
    current_temp = config.get("temperature", {}).get("current") # Might be None if not set
    if current_temp is None:
         # Fallback or default logic if temperature is crucial and not set
         default_temp = 320 # Example default
         config.setdefault("temperature", {})["current"] = default_temp
         current_temp = default_temp
         logger.warning(f"Temperature not specified, defaulting to {current_temp} for templating.")
    else:
         logger.info(f"Using temperature '{current_temp}' for configuration templating.")

    config = _template_config(config, current_temp)

    # 7. Resolve Paths (make paths absolute) AFTER templating
    config = _resolve_paths(config, base_dir=resolve_paths_base_dir)

    # 8. Validate Final Config (Basic Checks)
    required_sections = ['paths', 'binning', 'dataset', 'models', 'evaluation', 'temperature', 'pdb', 'system']
    for section in required_sections:
        if section not in config:
            logger.warning(f"Configuration section '{section}' is missing. Defaults may not apply.")
        elif not isinstance(config[section], dict):
             logger.warning(f"Configuration section '{section}' is not a dictionary. Check YAML structure.")

    # 9. Set Logging Level based on final config
    log_level_str = config.get("system", {}).get("log_level", "INFO").upper()
    log_level = getattr(logging, log_level_str, None)
    if log_level is None:
         log_level = logging.INFO
         logger.warning(f"Invalid log level '{log_level_str}' in config. Defaulting to INFO.")

    # Set level for the root logger AND the package logger
    logging.getLogger().setLevel(log_level)
    logging.getLogger('drwiggle').setLevel(log_level)
    logger.info(f"Logging level set to {log_level_str}")

    # Suppress excessive logging from dependencies if DEBUG is not set
    if log_level > logging.DEBUG:
        warnings.filterwarnings("ignore", category=FutureWarning) # Suppress some common warnings
        logging.getLogger("matplotlib").setLevel(logging.WARNING)
        logging.getLogger("numexpr").setLevel(logging.WARNING) # Often noisy
        # Add others as needed

    return config

# --- Helper functions to access specific config values ---

def get_metric_list(config: Dict[str, Any]) -> List[str]:
    """Extracts the list of enabled evaluation metrics from the config."""
    eval_config = config.get("evaluation", {}).get("metrics", {})
    return [metric for metric, enabled in eval_config.items() if enabled]

def get_visualization_colors(config: Dict[str, Any]) -> Dict[int, str]:
    """Extracts the color map for visualization from the config."""
    vis_config = config.get("visualization", {}).get("colors", {})
    try:
        # Convert keys to integers, ensure values are strings
        return {int(k): str(v) for k, v in vis_config.items()}
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not parse visualization colors correctly: {e}. Returning empty dict.")
        return {}

def get_class_names(config: Dict[str, Any]) -> Dict[int, str]:
    """Extracts the class names map from the config."""
    eval_config = config.get("evaluation", {}).get("class_names", {})
    try:
        # Convert keys to integers, ensure values are strings
        return {int(k): str(v) for k, v in eval_config.items()}
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not parse class names correctly: {e}. Returning empty dict.")
        return {}

def get_model_config(config: Dict[str, Any], model_name: str) -> Dict[str, Any]:
    """Gets the specific config for a model, merging common settings."""
    models_config = config.get("models", {})
    common_config = models_config.get("common", {})
    model_specific_config = models_config.get(model_name, {})

    if not model_specific_config:
         logger.warning(f"No specific config found for model '{model_name}'. Using common settings only.")
         # Return a copy of common to avoid modifying it if specific is empty
         return copy.deepcopy(common_config)

    # Deep merge: model-specific overrides common
    merged_config = copy.deepcopy(common_config)
    merged_config = deep_merge(merged_config, model_specific_config)
    return merged_config

def get_feature_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the feature configuration section."""
    return config.get("dataset", {}).get("features", {})

def get_enabled_features(config: Dict[str, Any]) -> List[str]:
    """Gets the list of features enabled for model input based on 'use_features'."""
    feature_cfg = get_feature_config(config)
    use_features_cfg = feature_cfg.get("use_features", {})
    return [feature for feature, enabled in use_features_cfg.items() if enabled]

def get_window_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the window feature configuration section."""
    feature_cfg = get_feature_config(config)
    return feature_cfg.get("window", {})

def is_pdb_enabled(config: Dict[str, Any]) -> bool:
    """Checks if PDB processing is enabled."""
    return config.get("pdb", {}).get("enabled", False)

def get_pdb_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the PDB configuration section."""
    return config.get("pdb", {})

def get_pdb_feature_config(config: Dict[str, Any]) -> Dict[str, bool]:
    """Gets the configuration for which features to extract from PDB."""
    if not is_pdb_enabled(config):
        return {}
    return config.get("pdb", {}).get("features", {})

def get_system_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the system configuration section."""
    return config.get("system", {})

def get_binning_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the binning configuration section."""
    return config.get("binning", {})

def get_split_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the data splitting configuration section."""
    return config.get("dataset", {}).get("split", {})

def get_temperature_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Gets the temperature configuration section."""
    return config.get("temperature", {})

--- End File: drwiggle/config.py ---

--- File: drwiggle/data/binning.py ---
---------------------------------------
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.exceptions import ConvergenceWarning
from abc import ABC, abstractmethod
import joblib
import logging
import warnings
from typing import List, Union, Dict, Any, Optional, Type
import os

from drwiggle.config import get_binning_config

logger = logging.getLogger(__name__)

class BaseBinner(ABC):
    """Abstract base class for RMSF binning methods."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Extract binning-specific config section
        self.binning_config = get_binning_config(config)
        self.num_classes = self.binning_config.get('num_classes', 5)
        self.boundaries: Optional[List[float]] = None
        self._fitted: bool = False
        self._bin_centers: Optional[np.ndarray] = None

    @abstractmethod
    def fit(self, rmsf_values: np.ndarray):
        """
        Calculate bin boundaries from the provided RMSF values.
        Must set self.boundaries and self._fitted = True.

        Args:
            rmsf_values: 1D NumPy array of continuous RMSF values.
        """
        pass

    def transform(self, rmsf_values: np.ndarray) -> np.ndarray:
        """
        Convert continuous RMSF values to discrete class indices (0 to num_classes-1).

        Args:
            rmsf_values: 1D NumPy array of continuous RMSF values.

        Returns:
            1D NumPy array of integer class labels.

        Raises:
            RuntimeError: If the binner has not been fitted.
            ValueError: If input array is not 1D.
        """
        if not self._fitted or self.boundaries is None:
            raise RuntimeError("Binner must be fitted before transforming data.")
        if rmsf_values.ndim > 1:
             # Allow (n, 1) shape, flatten it
             if rmsf_values.ndim == 2 and rmsf_values.shape[1] == 1:
                 rmsf_values = rmsf_values.flatten()
             else:
                 raise ValueError(f"Input rmsf_values for transform must be 1D, but got shape {rmsf_values.shape}.")

        # Ensure boundaries are monotonically increasing
        if not all(self.boundaries[i] <= self.boundaries[i+1] for i in range(len(self.boundaries)-1)):
             logger.error(f"Boundaries are not monotonically increasing: {self.boundaries}. Check fitting logic.")
             # Attempt to sort? Or raise error? Sorting might hide issues.
             self.boundaries = sorted(list(set(self.boundaries))) # Ensure unique and sorted as safeguard
             if len(self.boundaries) != self.num_classes + 1:
                 raise ValueError(f"Corrected boundaries length mismatch after sorting ({len(self.boundaries)} vs {self.num_classes+1}).")


        # Use pandas.cut for efficient binning
        # Labels are 0 to num_classes-1
        # right=False: bins are [left, right) - includes left edge, excludes right edge.
        # The last bin implicitly includes the right edge because the final boundary is +inf.
        # include_lowest=True: ensures the minimum value is included in the first bin ([min_val, boundary_1)).
        labels = pd.cut(rmsf_values, bins=self.boundaries, labels=False,
                        right=False, include_lowest=True,
                        duplicates='drop') # Handle duplicate boundary edges if they occur

        # Check for NaNs which might occur if a value falls exactly on the upper boundary
        # when right=False and duplicates='drop' hasn't handled it perfectly, or if values
        # are outside the explicit min/max if (-inf, inf) weren't used.
        nan_mask = np.isnan(labels)
        if nan_mask.any():
             nan_indices = np.where(nan_mask)[0]
             # Assign NaNs to the highest class index
             highest_class_index = self.num_classes - 1
             labels[nan_mask] = highest_class_index
             logger.warning(f"Found {len(nan_indices)} NaN labels after binning (potentially values exactly on boundaries or outside range)."
                            f" Assigning them to the highest class ({highest_class_index}). "
                            f"Problematic values (first 5): {rmsf_values[nan_indices][:5]}")

        # Ensure output is integer type
        return labels.astype(int)

    def fit_transform(self, rmsf_values: np.ndarray) -> np.ndarray:
        """Fit the binner and then transform the RMSF values."""
        self.fit(rmsf_values)
        return self.transform(rmsf_values)

    def _calculate_bin_centers(self):
        """Calculate representative center for each bin."""
        if self.boundaries is None: return None
        centers = []
        for i in range(self.num_classes):
            lower = self.boundaries[i]
            upper = self.boundaries[i+1]
            if np.isneginf(lower) and np.isposinf(upper): center = 0.0 # Should not happen
            elif np.isneginf(lower): center = upper * 0.9 # Estimate slightly below upper bound
            elif np.isposinf(upper): center = lower * 1.1 # Estimate slightly above lower bound
            else: center = (lower + upper) / 2.0
            centers.append(center)
        self._bin_centers = np.array(centers)


    def inverse_transform(self, class_indices: np.ndarray) -> np.ndarray:
        """
        Convert class indices back to representative RMSF values (bin centers).

        Args:
            class_indices: 1D NumPy array of integer class labels.

        Returns:
            1D NumPy array of representative RMSF values.
        """
        if not self._fitted or self.boundaries is None:
            raise RuntimeError("Binner must be fitted before inverse transforming data.")
        if self._bin_centers is None:
             self._calculate_bin_centers()
             if self._bin_centers is None: # Still None after calculation attempt
                  raise RuntimeError("Could not calculate bin centers.")

        if len(self._bin_centers) != self.num_classes:
            raise ValueError(f"Mismatch between number of bin centers ({len(self._bin_centers)}) and num_classes ({self.num_classes}).")

        # Map indices to centers
        try:
             representative_values = self._bin_centers[class_indices]
             return representative_values
        except IndexError:
             logger.error(f"Class indices out of bounds. Max index: {np.max(class_indices)}, Num centers: {len(self._bin_centers)}")
             raise

    def get_boundaries(self) -> Optional[List[float]]:
        """Return the calculated bin boundaries."""
        return self.boundaries

    def save(self, path: str):
        """Save the fitted binner state (including boundaries and config) to a file."""
        if not self._fitted:
            raise RuntimeError("Cannot save an unfitted binner.")
            # Get the method key used (e.g., 'kmeans', 'quantile') from the config
        method_key = self.binning_config.get('method', None)
        if method_key is None:
            # Fallback: try to infer from class name if method missing in loaded config (shouldn't happen often)
            method_key = self.__class__.__name__.replace("Binner", "").lower()
            logger.warning(f"Binner method key not found in config, inferring as '{method_key}' for saving.")

        state = {
            'boundaries': self.boundaries,
            'num_classes': self.num_classes,
            'binning_config': self.binning_config, # Save specific config section used
            'fitted': self._fitted,
            'binner_method_key': method_key, # Store the config key used  # <<<--- NEW LINE
            'bin_centers': self._bin_centers # Save calculated centers
        }
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True) # Ensure directory exists
            joblib.dump(state, path)
            logger.info(f"Binner state ({self.__class__.__name__}) saved to {path}")
        except Exception as e:
            logger.error(f"Failed to save binner state to {path}: {e}")
            raise

    @classmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'BaseBinner':
        """
        Load a fitted binner state from a file.

        Args:
            path: Path to the saved binner file.
            config: Optional current main config dictionary. If provided, helps ensure
                    consistency or allows using current config if saved one is minimal.

        Returns:
            A loaded instance of the appropriate BaseBinner subclass.
        """
        try:
            state = joblib.load(path)
            logger.info(f"Loading binner state from {path}")

                # Get the method key used when saving
            method_key = state.get('binner_method_key') # Try loading the new key first
            if not method_key:
                # Fallback for older saves that might have 'class_name'
                class_name = state.get('class_name')
                if class_name:
                    logger.warning("Loading older binner format using class name. Converting to method key.")
                    # Convert class name back to likely key (best effort)
                    method_key = class_name.replace("Binner", "").lower() # e.g., "KMeansBinner" -> "kmeans"
                else:
                    raise ValueError("Saved binner state missing 'binner_method_key' or 'class_name'.")

            # Find the correct class type from the registry using the method key
            binner_cls = _BINNER_REGISTRY.get(method_key) # <<<--- USE method_key FOR LOOKUP
            if not binner_cls:
                raise ValueError(f"Cannot find binner class for method key '{method_key}' in registry for loading.")

            # Use the config stored in the state file primarily
            saved_config = state.get('config') # Older saves might have full config
            saved_binning_config = state.get('binning_config') # Newer saves have specific section

            if saved_binning_config:
                 # Construct a minimal config dict needed for re-instantiation
                 rehydration_config = {'binning': saved_binning_config}
            elif saved_config:
                 logger.warning("Loading binner saved with older format (full config). Using saved config.")
                 rehydration_config = saved_config
            elif config: # Use provided config as fallback if nothing useful saved
                 logger.warning("No config found in saved binner state. Using provided runtime config.")
                 rehydration_config = config
            else: # Cannot reinstantiate without config
                 raise ValueError("Cannot load binner: No configuration found in saved state or provided.")

            # Re-instantiate the class
            instance = binner_cls(config=rehydration_config)

            # Restore state
            instance.boundaries = state.get('boundaries')
            instance.num_classes = state.get('num_classes', instance.num_classes) # Use loaded if available
            instance._fitted = state.get('fitted', False)
            instance._bin_centers = state.get('bin_centers')

            if not instance._fitted or instance.boundaries is None:
                 logger.warning(f"Loaded binner state from {path} appears incomplete or unfitted.")

            return instance
        except FileNotFoundError:
             logger.error(f"Binner state file not found: {path}")
             raise
        except Exception as e:
            logger.error(f"Failed to load binner state from {path}: {e}")
            raise


class KMeansBinner(BaseBinner):
    """Bins RMSF values using K-means clustering on the 1D data."""

    def fit(self, rmsf_values: np.ndarray):
        """Calculate bin boundaries using K-means."""
        if rmsf_values.ndim > 1:
             if rmsf_values.ndim == 2 and rmsf_values.shape[1] == 1:
                 rmsf_values = rmsf_values.flatten()
             else:
                 raise ValueError(f"Input rmsf_values for KMeansBinner fit must be 1D, but got shape {rmsf_values.shape}.")

        if len(np.unique(rmsf_values)) < self.num_classes:
             logger.warning(f"Number of unique RMSF values ({len(np.unique(rmsf_values))}) is less than num_classes ({self.num_classes}). KMeans may produce fewer clusters or fail.")

        rmsf_reshaped = rmsf_values.reshape(-1, 1) # KMeans expects 2D array

        kmeans_params = self.binning_config.get('kmeans', {})
        kmeans = KMeans(
            n_clusters=self.num_classes,
            random_state=kmeans_params.get('random_state', 42),
            max_iter=kmeans_params.get('max_iter', 300),
            n_init=kmeans_params.get('n_init', 10),
            init='k-means++' # Default robust initialization
        )

        logger.info(f"Fitting KMeans with k={self.num_classes}...")
        start_time = pd.Timestamp.now()
        try:
             with warnings.catch_warnings():
                 # Suppress ConvergenceWarning which is common with multiple inits
                 warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn.cluster._kmeans")
                 kmeans.fit(rmsf_reshaped)
        except Exception as e:
             logger.error(f"KMeans fitting failed: {e}")
             logger.warning("Falling back to Quantile binning due to KMeans error.")
             # Fallback logic
             quantile_binner = QuantileBinner(self.config)
             quantile_binner.fit(rmsf_values)
             self.boundaries = quantile_binner.boundaries
             self._fitted = quantile_binner._fitted
             return # Exit after fallback


        elapsed = (pd.Timestamp.now() - start_time).total_seconds()
        logger.info(f"KMeans fitting complete in {elapsed:.2f}s. Inertia: {kmeans.inertia_:.2f}")

        centers = sorted(kmeans.cluster_centers_.flatten())
        actual_num_clusters = len(centers)
        logger.debug(f"KMeans cluster centers found ({actual_num_clusters}): {np.round(centers, 3)}")

        if actual_num_clusters < self.num_classes:
            logger.warning(f"KMeans found only {actual_num_clusters} clusters, less than requested ({self.num_classes}). Bin boundaries might merge classes.")
            # If significantly fewer, might indicate issues with data or k choice.
            if actual_num_clusters < 2:
                 logger.error("KMeans found less than 2 clusters. Cannot define meaningful boundaries. Falling back to Quantiles.")
                 quantile_binner = QuantileBinner(self.config)
                 quantile_binner.fit(rmsf_values)
                 self.boundaries = quantile_binner.boundaries
                 self._fitted = quantile_binner._fitted
                 return

        # Calculate boundaries as midpoints between sorted centers
        if actual_num_clusters > 1:
             boundaries_mid = [(centers[i] + centers[i+1]) / 2.0 for i in range(actual_num_clusters - 1)]
        else: # Only 1 cluster center
             # Create somewhat arbitrary boundaries around the single center
             std_dev = np.std(rmsf_values) if len(rmsf_values) > 1 else 0.1
             boundaries_mid = [centers[0] - std_dev*0.5, centers[0] + std_dev*0.5]
             # Adjust num_classes if only one center found? Or force failure? Forcing Quantile fallback is safer.
             logger.error("KMeans found only 1 cluster center. Falling back to Quantiles.")
             quantile_binner = QuantileBinner(self.config)
             quantile_binner.fit(rmsf_values)
             self.boundaries = quantile_binner.boundaries
             self._fitted = quantile_binner._fitted
             return


        # Add outer boundaries: -inf and +inf for robustness
        self.boundaries = [-np.inf] + boundaries_mid + [np.inf]

        # Ensure the number of boundaries matches num_classes + 1
        # If KMeans found fewer clusters, we might have fewer midpoints.
        # We need to pad the boundaries list. A simple way is to repeat the last finite boundary
        # or add boundaries based on std dev, but this indicates a potential issue.
        # A safer approach if boundaries are insufficient is to fallback to quantile.
        if len(self.boundaries) != self.num_classes + 1:
             logger.warning(f"Generated {len(self.boundaries)} boundaries (expected {self.num_classes + 1}) "
                            f"due to {actual_num_clusters} clusters found. "
                            f"Boundaries: {np.round(self.boundaries, 3)}. "
                            "Falling back to Quantile binning for reliable boundary count.")
             # Fallback logic
             quantile_binner = QuantileBinner(self.config)
             quantile_binner.fit(rmsf_values)
             self.boundaries = quantile_binner.boundaries

        # Final check for monotonicity
        if not all(self.boundaries[i] <= self.boundaries[i+1] for i in range(len(self.boundaries)-1)):
            logger.error(f"KMeans boundaries are not monotonic: {self.boundaries}. Falling back to Quantiles.")
            quantile_binner = QuantileBinner(self.config)
            quantile_binner.fit(rmsf_values)
            self.boundaries = quantile_binner.boundaries

        self._fitted = True
        logger.info(f"KMeansBinner fitted. Boundaries: {np.round(self.boundaries, 3)}")


class QuantileBinner(BaseBinner):
    """Bins RMSF values based on quantiles (percentiles)."""

    def fit(self, rmsf_values: np.ndarray):
        """Calculate bin boundaries using quantiles."""
        if rmsf_values.ndim > 1:
             if rmsf_values.ndim == 2 and rmsf_values.shape[1] == 1:
                 rmsf_values = rmsf_values.flatten()
             else:
                 raise ValueError(f"Input rmsf_values for QuantileBinner fit must be 1D, but got shape {rmsf_values.shape}.")

        quantile_params = self.binning_config.get('quantile', {})
        percentiles = quantile_params.get('percentiles')

        # If percentiles not provided or don't match num_classes, generate defaults
        if percentiles is None or len(percentiles) != self.num_classes + 1:
            if percentiles is not None: # Provided but wrong length
                 logger.warning(f"Number of percentiles ({len(percentiles)}) in config does not match num_classes+1 ({self.num_classes+1}). Ignoring config and generating default percentiles.")
            percentiles = np.linspace(0, 100, self.num_classes + 1)
            logger.info(f"Using default percentiles for {self.num_classes} classes: {np.round(percentiles, 1)}")
        else:
             logger.info(f"Using percentiles from config: {percentiles}")


        # Calculate boundaries using numpy.percentile
        boundaries = np.percentile(rmsf_values, percentiles)

        # Handle non-unique boundaries (can happen with discrete data or many identical values)
        unique_boundaries = np.unique(boundaries)
        if len(unique_boundaries) < len(boundaries):
            logger.warning(f"Quantile calculation resulted in non-unique boundaries ({np.round(boundaries, 3)}). "
                           f"Unique values: {np.round(unique_boundaries, 3)}. Binning might merge classes.")
            # Strategy: Keep unique values. This might reduce the effective number of classes.
            boundaries = unique_boundaries

            # If we have too few boundaries now, log an error, but proceed.
            # pd.cut with duplicates='drop' should handle this, but classes will be merged.
            if len(boundaries) <= self.num_classes: # Need at least num_classes+1 edges initially
                 logger.error(f"Only {len(boundaries)} unique boundaries found after quantile calculation, expected {self.num_classes + 1}. "
                              f"Effective number of classes will be reduced.")
                 # Cannot reliably proceed with the configured number of classes.
                 # Raising an error might be better than silently merging classes.
                 # raise ValueError(f"Could not determine {self.num_classes} distinct quantile bins.")
                 # Alternative: Adjust num_classes? For now, warn and proceed.
                 pass # Let pd.cut handle it, classes will merge

        # Ensure boundaries start at -inf and end at +inf for robustness
        # np.percentile(a, 0) gives min, np.percentile(a, 100) gives max
        # Replace first with -inf and last with +inf
        final_boundaries = [-np.inf] + boundaries[1:-1].tolist() + [np.inf]

        # If boundaries became non-unique *after* replacing with inf (only possible if min/max were equal)
        # Or if the number of boundaries is still wrong (e.g. only 2 unique values found)
        check_boundaries = sorted(list(set(b for b in final_boundaries if np.isfinite(b))))
        if len(check_boundaries) < self.num_classes -1: # Need at least N-1 finite boundaries for N classes
             logger.error(f"Insufficient distinct finite boundaries ({len(check_boundaries)}) found for {self.num_classes} classes. Check data distribution.")
             # Fallback: Create arbitrary linear spacing if quantile fails badly? Risky.
             # Let's raise an error here as the binning is likely meaningless.
             raise ValueError(f"Failed to create sufficient distinct quantile boundaries ({len(final_boundaries)} total) for {self.num_classes} classes.")

        self.boundaries = final_boundaries
        self._fitted = True
        logger.info(f"QuantileBinner fitted. Boundaries: {np.round(self.boundaries, 3)}")


# --- Binner Registry and Factory ---

_BINNER_REGISTRY: Dict[str, Type[BaseBinner]] = {
    "kmeans": KMeansBinner,
    "quantile": QuantileBinner,
}

def get_binner(config: Dict[str, Any]) -> BaseBinner:
    """
    Factory function to instantiate a binner based on the configuration.

    Args:
        config: The main configuration dictionary.

    Returns:
        An instance of the appropriate BaseBinner subclass.

    Raises:
        ValueError: If the specified binning method is unknown.
    """
    binning_config = get_binning_config(config)
    method = binning_config.get('method', 'kmeans').lower() # Default to kmeans if not specified

    binner_cls = _BINNER_REGISTRY.get(method)
    if binner_cls:
        logger.info(f"Creating binner using method: '{method}'")
        return binner_cls(config)
    else:
        raise ValueError(f"Unknown binning method: '{method}'. Available methods: {list(_BINNER_REGISTRY.keys())}")

--- End File: drwiggle/data/binning.py ---

--- File: drwiggle/data/loader.py ---
---------------------------------------
import pandas as pd
import os
import logging
import glob
from typing import Union, List, Dict, Optional, Tuple
import warnings

logger = logging.getLogger(__name__)

def find_data_file(data_dir: str, pattern: str) -> Optional[str]:
    """
    Finds a data file matching a pattern (potentially containing wildcards)
    within a specified directory.

    Args:
        data_dir: The absolute path to the directory to search in.
        pattern: The file pattern (e.g., "temperature_320_*.csv").

    Returns:
        The absolute path to the first matching file found, or None if no match.

    Raises:
        FileNotFoundError: If the data directory itself does not exist.
    """
    if not os.path.isdir(data_dir):
        raise FileNotFoundError(f"Data directory not found: {data_dir}")

    search_pattern = os.path.join(data_dir, pattern)
    matching_files = glob.glob(search_pattern)

    if not matching_files:
        logger.warning(f"No files found matching pattern '{pattern}' in directory '{data_dir}'")
        return None

    if len(matching_files) > 1:
        logger.warning(f"Multiple files found for pattern '{pattern}'. Using the first one found: {matching_files[0]}")

    logger.info(f"Found data file: {matching_files[0]}")
    return matching_files[0]


def load_data(file_path_or_pattern: str, data_dir: Optional[str] = None) -> pd.DataFrame:
    """
    Load data from a CSV file. Handles either a direct path or a pattern search within data_dir.

    Args:
        file_path_or_pattern: Absolute path to a CSV file, or a filename pattern.
        data_dir: Absolute path to the directory to search if `file_path_or_pattern` is a pattern.
                  Required if `file_path_or_pattern` is not an absolute path and not just a filename pattern.

    Returns:
        Loaded pandas DataFrame.

    Raises:
        FileNotFoundError: If the file/pattern cannot be resolved or the file doesn't exist.
        ValueError: If a pattern is given without a data_dir.
        Exception: For pandas CSV reading errors.
    """
    file_path: Optional[str] = None

    if os.path.isabs(file_path_or_pattern) and os.path.isfile(file_path_or_pattern):
        file_path = file_path_or_pattern
        logger.info(f"Loading data from absolute path: {file_path}")
    elif '*' in file_path_or_pattern or '?' in file_path_or_pattern: # Likely a pattern
        if not data_dir:
            raise ValueError("data_dir must be provided when using a file pattern.")
        file_path = find_data_file(data_dir, file_path_or_pattern)
        if file_path is None:
             raise FileNotFoundError(f"No file found matching pattern '{file_path_or_pattern}' in directory '{data_dir}'")
        logger.info(f"Loading data found via pattern: {file_path}")
    else: # Assume it's a relative path or just filename - needs data_dir
        if not data_dir:
             # Try relative to current directory as last resort
             potential_path = os.path.abspath(file_path_or_pattern)
             if os.path.isfile(potential_path):
                  file_path = potential_path
                  logger.warning(f"No data_dir provided. Assuming '{file_path_or_pattern}' is relative to CWD: {file_path}")
             else:
                raise ValueError("data_dir must be provided for relative paths or filename patterns.")
        else:
             potential_path = os.path.join(data_dir, file_path_or_pattern)
             if os.path.isfile(potential_path):
                 file_path = potential_path
                 logger.info(f"Loading data from file '{file_path_or_pattern}' in data_dir: {file_path}")
             else:
                 # Maybe it was a pattern after all? Try find_data_file
                 file_path = find_data_file(data_dir, file_path_or_pattern)
                 if file_path is None:
                     raise FileNotFoundError(f"File '{file_path_or_pattern}' not found in directory '{data_dir}' or as pattern.")
                 logger.info(f"Loading data found via pattern search for '{file_path_or_pattern}': {file_path}")


    if file_path is None or not os.path.exists(file_path):
        # This case should ideally be caught earlier, but acts as a safeguard
        raise FileNotFoundError(f"Could not resolve or find data file: {file_path_or_pattern}")

    try:
        # Basic CSV loading. Add options (sep, header, dtype) as needed.
        # Use low_memory=False for potentially mixed type columns if warnings occur
        with warnings.catch_warnings():
             # Can suppress DtypeWarning if necessary and understood
             # warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)
             df = pd.read_csv(file_path, low_memory=False)
        logger.info(f"Successfully loaded data from {file_path}. Shape: {df.shape}, Columns: {df.columns.tolist()}")
        return df
    except FileNotFoundError: # Should be caught above, but re-raise clearly
         logger.error(f"Data file not found at the determined path: {file_path}")
         raise
    except Exception as e:
        logger.error(f"Failed to load data from {file_path}: {e}")
        raise

def load_rmsf_data(file_path: str, target_column: str) -> pd.Series:
    """
    Loads only the target RMSF column from a data file efficiently.

    Args:
        file_path: Absolute path to the data file (CSV).
        target_column: The name of the column containing RMSF values.

    Returns:
        pandas Series containing RMSF values.

    Raises:
        FileNotFoundError: If the file doesn't exist.
        ValueError: If the target column is not found in the file.
        Exception: For pandas reading errors.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found: {file_path}")

    logger.info(f"Loading RMSF data (column: '{target_column}') from {file_path}...")
    try:
        # Optimize by reading only the necessary column for CSV
        if file_path.lower().endswith('.csv'):
            # Fast check if column exists by reading just the header
            header_df = pd.read_csv(file_path, nrows=0)
            if target_column not in header_df.columns:
                 # Attempt case-insensitive match as fallback
                 target_column_lower = target_column.lower()
                 matching_cols = [col for col in header_df.columns if col.lower() == target_column_lower]
                 if not matching_cols:
                      raise ValueError(f"Target column '{target_column}' not found in {file_path}. Available columns: {header_df.columns.tolist()}")
                 target_column = matching_cols[0] # Use the actual column name case
                 logger.warning(f"Using case-insensitive match for target column: '{target_column}'")

            # Read only the target column
            rmsf_series = pd.read_csv(file_path, usecols=[target_column]).squeeze("columns")
            if not isinstance(rmsf_series, pd.Series):
                 raise TypeError(f"Expected pd.Series after loading column '{target_column}', but got {type(rmsf_series)}.")
        else:
             # Fallback for non-CSV: load full DataFrame and select column
             logger.warning(f"File format not CSV ({file_path}). Loading full file to extract RMSF column.")
             df = load_data(file_path) # Use the general loader
             if target_column not in df.columns:
                  # Case-insensitive check
                 target_column_lower = target_column.lower()
                 matching_cols = [col for col in df.columns if col.lower() == target_column_lower]
                 if not matching_cols:
                      raise ValueError(f"Target column '{target_column}' not found in {file_path}. Available columns: {df.columns.tolist()}")
                 target_column = matching_cols[0]
                 logger.warning(f"Using case-insensitive match for target column: '{target_column}'")

             rmsf_series = df[target_column]

        # Validate data type
        if not pd.api.types.is_numeric_dtype(rmsf_series):
            logger.warning(f"RMSF column '{target_column}' is not numeric (dtype: {rmsf_series.dtype}). Attempting conversion.")
            try:
                rmsf_series = pd.to_numeric(rmsf_series, errors='coerce')
                nans_induced = rmsf_series.isnull().sum()
                if nans_induced > 0:
                     logger.warning(f"Conversion to numeric induced {nans_induced} NaN values in RMSF column.")
            except Exception as conv_err:
                logger.error(f"Failed to convert RMSF column '{target_column}' to numeric: {conv_err}")
                raise ValueError(f"RMSF column '{target_column}' could not be converted to numeric.")

        logger.info(f"Successfully loaded RMSF data. Count: {len(rmsf_series)}, NaN count: {rmsf_series.isnull().sum()}")
        return rmsf_series
    except ValueError as ve: # Re-raise specific errors
        logger.error(f"Value error loading RMSF data: {ve}")
        raise
    except FileNotFoundError:
        logger.error(f"RMSF data file not found: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Failed to load RMSF data from {file_path}: {e}")
        raise

--- End File: drwiggle/data/loader.py ---

--- File: drwiggle/data/processor.py ---
---------------------------------------
import pandas as pd
import numpy as np
import logging
from typing import Dict, Any, Tuple, List, Optional
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.preprocessing import LabelEncoder # For simple categorical encoding

from drwiggle.config import get_feature_config, get_enabled_features, get_window_config, get_split_config, get_system_config

logger = logging.getLogger(__name__)

# --- Default Mappings (can be adjusted or loaded from config/files) ---
# Standard 20 amino acids + UNK (Unknown) + GAP (-) potentially
AA_ORDER = [
    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',
    'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP',
    'TYR', 'VAL', 'UNK', 'GAP' # Add others if needed (e.g., SEC)
]
# Create mapping dictionary {AA_NAME: index}
AA_MAP = {name: i for i, name in enumerate(AA_ORDER)}
AA_UNK_CODE = AA_MAP['UNK'] # Default code for unknown AAs

# DSSP Secondary Structure categories (simplified mapping)
# H=AlphaHelix, G=310Helix, I=PiHelix -> Helix (0)
# E=Strand, B=Bridge -> Sheet (1)
# T=Turn, S=Bend, C=Coil/Loop, -=Unknown/Gap -> Coil/Other (2)
SS_MAP = {'H': 0, 'G': 0, 'I': 0, # Helix-like
          'E': 1, 'B': 1,         # Sheet-like
          'T': 2, 'S': 2, 'C': 2, '-': 2, # Coil/Other/Unknown
          '?': 2 # Handle DSSP '?' for unknown
          }
SS_UNK_CODE = SS_MAP['-']

# Core/Exterior mapping (example)
LOC_MAP = {'CORE': 0, 'SURFACE': 1, 'EXTERIOR': 1, 'UNK': 0} # Map Surface/Exterior to 1, Core/Unknown to 0
LOC_UNK_CODE = LOC_MAP['UNK']

# --- Feature Processing Functions ---

def _get_column_name(df_columns: List[str], base_name: str) -> Optional[str]:
    """Finds column name case-insensitively."""
    base_lower = base_name.lower()
    for col in df_columns:
        if col.lower() == base_lower:
            return col
    return None

def clean_data(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Basic data cleaning: handles NaNs in expected numeric/object columns.
    Uses median for numeric and specific placeholders ('UNK', 'C', 'CORE') for categoricals.
    """
    logger.debug(f"Starting data cleaning. Initial shape: {df.shape}. NaN counts:\n{df.isnull().sum()[df.isnull().sum() > 0]}")
    df_cleaned = df.copy() # Work on a copy

    # Numeric columns: fill with median
    numeric_cols = df_cleaned.select_dtypes(include=np.number).columns
    for col in numeric_cols:
        if df_cleaned[col].isnull().any():
            # Exclude target columns from NaN filling if present (should be handled later)
            target_col_template = config.get("dataset",{}).get("target", "rmsf_{temperature}")
            # This check is basic, might need refinement if multiple targets exist
            if col.lower().startswith(target_col_template.split('_')[0].lower()):
                 logger.debug(f"Skipping NaN fill for potential target column: {col}")
                 continue

            median_val = df_cleaned[col].median()
            # Check if median is NaN (can happen if all values are NaN)
            if pd.isna(median_val):
                 median_val = 0 # Fallback to 0 if median cannot be calculated
                 logger.warning(f"Median for numeric column '{col}' is NaN. Filling with 0.")
            df_cleaned[col].fillna(median_val, inplace=True)
            logger.debug(f"Filled NaNs in numeric column '{col}' with median ({median_val:.3f}).")

    # Object/Categorical columns: fill with specific placeholders
    object_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns
    df_columns_list = df_cleaned.columns.tolist() # For case-insensitive check

    # --- Specific Columns ---
    # Residue Name ('resname', 'residue', etc.)
    resname_col = _get_column_name(df_columns_list, 'resname') or _get_column_name(df_columns_list, 'residue_name')
    if resname_col and df_cleaned[resname_col].isnull().any():
        df_cleaned[resname_col].fillna('UNK', inplace=True)
        logger.debug(f"Filled NaNs in '{resname_col}' with 'UNK'.")

    # Secondary Structure ('dssp', 'ss', 'secondary_structure')
    ss_col = _get_column_name(df_columns_list, 'dssp') or _get_column_name(df_columns_list, 'secondary_structure')
    if ss_col and df_cleaned[ss_col].isnull().any():
        df_cleaned[ss_col].fillna('-', inplace=True) # DSSP standard for unknown/gap
        logger.debug(f"Filled NaNs in '{ss_col}' with '-'.")

    # Core/Exterior ('core_exterior', 'location')
    loc_col = _get_column_name(df_columns_list, 'core_exterior') or _get_column_name(df_columns_list, 'location')
    if loc_col and df_cleaned[loc_col].isnull().any():
        df_cleaned[loc_col].fillna('UNK', inplace=True) # Use UNK placeholder
        logger.debug(f"Filled NaNs in '{loc_col}' with 'UNK'.")

    # --- Generic Object Columns ---
    # For other object columns, fill with 'UNK' or mode if appropriate
    for col in object_cols:
         # Skip already handled specific columns
         if col in [resname_col, ss_col, loc_col]: continue
         if df_cleaned[col].isnull().any():
             fill_val = 'UNK' # General unknown placeholder
             df_cleaned[col].fillna(fill_val, inplace=True)
             logger.debug(f"Filled NaNs in generic object column '{col}' with '{fill_val}'.")

    nan_counts_after = df_cleaned.isnull().sum().sum()
    logger.debug(f"Data cleaning finished. Shape: {df_cleaned.shape}. Total NaNs remaining: {nan_counts_after}")
    if nan_counts_after > 0:
         logger.warning(f"NaNs still present after cleaning. Check target columns or unexpected types:\n{df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0]}")

    return df_cleaned

def encode_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """Encode categorical features based on 'use_features' config."""
    logger.debug("Encoding categorical features...")
    df_encoded = df.copy()
    feature_cfg = get_feature_config(config)
    use_features = feature_cfg.get("use_features", {})
    df_columns = df_encoded.columns.tolist()

    # --- Residue Name Encoding ---
    resname_col = _get_column_name(df_columns, 'resname') or _get_column_name(df_columns, 'residue_name')
    if use_features.get("resname_encoded") and resname_col:
        logger.debug(f"Encoding '{resname_col}' using predefined AA map.")
        df_encoded['resname_encoded'] = df_encoded[resname_col].str.upper().map(AA_MAP).fillna(AA_UNK_CODE).astype(int)
    elif use_features.get("resname_encoded"):
        logger.warning("Feature 'resname_encoded' enabled but 'resname' column not found.")

    # --- Secondary Structure Encoding ---
    ss_col = _get_column_name(df_columns, 'dssp') or _get_column_name(df_columns, 'secondary_structure')
    if use_features.get("secondary_structure_encoded") and ss_col:
        logger.debug(f"Encoding '{ss_col}' using predefined SS map.")
        df_encoded['secondary_structure_encoded'] = df_encoded[ss_col].str.upper().map(SS_MAP).fillna(SS_UNK_CODE).astype(int)
    elif use_features.get("secondary_structure_encoded"):
        logger.warning("Feature 'secondary_structure_encoded' enabled but 'dssp' or 'secondary_structure' column not found.")

    # --- Core/Exterior Encoding ---
    loc_col = _get_column_name(df_columns, 'core_exterior') or _get_column_name(df_columns, 'location')
    if use_features.get("core_exterior_encoded") and loc_col:
        logger.debug(f"Encoding '{loc_col}' using predefined Location map (Core=0, Surface=1).")
        df_encoded['core_exterior_encoded'] = df_encoded[loc_col].str.upper().map(LOC_MAP).fillna(LOC_UNK_CODE).astype(int)
    elif use_features.get("core_exterior_encoded"):
        logger.warning("Feature 'core_exterior_encoded' enabled but 'core_exterior' or 'location' column not found.")

    # Add other encodings here if needed (e.g., OneHot for specific features)

    logger.debug(f"Feature encoding finished. Columns added/modified: {[c for c in df_encoded.columns if c not in df.columns or not df_encoded[c].equals(df[c])]}")
    return df_encoded

def normalize_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """Normalize features like angles and residue index."""
    logger.debug("Normalizing features...")
    df_norm = df.copy()
    feature_cfg = get_feature_config(config)
    use_features = feature_cfg.get("use_features", {})
    df_columns = df_norm.columns.tolist()
    domain_id_col = _get_column_name(df_columns, 'domain_id') or _get_column_name(df_columns, 'protein_id') # Need a domain/protein identifier
    resid_col = _get_column_name(df_columns, 'resid') or _get_column_name(df_columns, 'res_id') or _get_column_name(df_columns, 'residue_number')


    # --- Angle Normalization (using sin/cos for cyclical nature) ---
    phi_col = _get_column_name(df_columns, 'phi')
    psi_col = _get_column_name(df_columns, 'psi')

    if use_features.get("phi_norm") and phi_col:
        logger.debug(f"Normalizing '{phi_col}' using sin/cos.")
        phi_rad = np.radians(df_norm[phi_col].fillna(0)) # Fill NaNs with 0 degrees before conversion
        df_norm['phi_sin'] = np.sin(phi_rad)
        df_norm['phi_cos'] = np.cos(phi_rad)
    elif use_features.get("phi_norm"):
        logger.warning("Feature 'phi_norm' enabled but 'phi' column not found.")

    if use_features.get("psi_norm") and psi_col:
        logger.debug(f"Normalizing '{psi_col}' using sin/cos.")
        psi_rad = np.radians(df_norm[psi_col].fillna(0))
        df_norm['psi_sin'] = np.sin(psi_rad)
        df_norm['psi_cos'] = np.cos(psi_rad)
    elif use_features.get("psi_norm"):
        logger.warning("Feature 'psi_norm' enabled but 'psi' column not found.")

    # --- Protein Size ---
    # Calculate if needed by other features or if explicitly enabled
    protein_size_col = _get_column_name(df_columns, 'protein_size')
    needs_protein_size = use_features.get("protein_size") or use_features.get("normalized_resid")
    if needs_protein_size and not protein_size_col:
        if not domain_id_col:
             logger.warning("Cannot calculate 'protein_size': 'domain_id' column not found.")
        elif not resid_col:
             logger.warning("Cannot calculate 'protein_size': 'resid' column not found.")
        else:
             logger.debug(f"Calculating 'protein_size' based on '{domain_id_col}' and '{resid_col}'.")
             # Use transform to broadcast the count per group back to the original DataFrame
             df_norm['protein_size'] = df_norm.groupby(domain_id_col)[resid_col].transform('count')
             protein_size_col = 'protein_size' # Update column name
    elif use_features.get("protein_size") and not protein_size_col:
         logger.warning("Feature 'protein_size' enabled but column not found and could not be calculated.")


    # --- Normalized Residue Index ---
    if use_features.get("normalized_resid") and resid_col:
        if not domain_id_col:
             logger.warning("Cannot calculate 'normalized_resid': 'domain_id' column not found.")
        elif not protein_size_col or protein_size_col not in df_norm.columns: # Check if calculation succeeded
             logger.warning("Cannot calculate 'normalized_resid': 'protein_size' column not found or calculated.")
        else:
             logger.debug(f"Calculating 'normalized_resid' using '{resid_col}', '{domain_id_col}', and '{protein_size_col}'.")
             # Normalize within each domain: (resid - min_resid) / (max_resid - min_resid)
             # Assumes resid is numeric and sequential within domain for this normalization.
             # If resids are not sequential, this might not be the desired normalization.
             min_res = df_norm.groupby(domain_id_col)[resid_col].transform('min')
             max_res = df_norm.groupby(domain_id_col)[resid_col].transform('max')
             # Avoid division by zero for single-residue domains or constant resid
             denominator = (max_res - min_res).clip(lower=1e-6) # Add small epsilon
             df_norm['normalized_resid'] = (df_norm[resid_col] - min_res) / denominator
             # Clip to [0, 1] just in case, and fill NaNs (e.g., from single residue domains) with 0
             df_norm['normalized_resid'] = df_norm['normalized_resid'].fillna(0).clip(0, 1)
    elif use_features.get("normalized_resid"):
         logger.warning("Feature 'normalized_resid' enabled but 'resid' column not found.")


    # --- B-Factor Normalization (Example: Z-score per protein) ---
    b_factor_col = _get_column_name(df_columns, 'b_factor') or _get_column_name(df_columns, 'bfactor')
    if use_features.get("b_factor") and b_factor_col: # Check if enabled AND column exists
         if domain_id_col:
             logger.debug(f"Z-score normalizing '{b_factor_col}' per domain ('{domain_id_col}').")
             mean_b = df_norm.groupby(domain_id_col)[b_factor_col].transform('mean')
             std_b = df_norm.groupby(domain_id_col)[b_factor_col].transform('std')
             # Avoid division by zero for domains with constant B-factor or single residue
             std_b = std_b.fillna(1e-6).clip(lower=1e-6)
             df_norm['b_factor_norm'] = (df_norm[b_factor_col] - mean_b) / std_b
             # Fill potential NaNs resulting from calculation with 0 (mean)
             df_norm['b_factor_norm'].fillna(0, inplace=True)
         else:
             logger.warning(f"Cannot normalize '{b_factor_col}' per domain: '{domain_id_col}' column not found. Skipping normalization.")
             # Copy original B-factor if normalization fails but feature is requested? Or drop?
             # For now, do nothing, model prep will fail if 'b_factor' is expected but only 'b_factor_norm' might exist
    # Note: The feature name in use_features is 'b_factor'. If normalization is done,
    # the pipeline/model prep needs to know to use 'b_factor_norm' instead.
    # This logic needs refinement - perhaps rename the column to 'b_factor' after normalization,
    # or adjust get_enabled_features based on processing steps.
    # --> Let's rename 'b_factor_norm' back to 'b_factor' to simplify downstream use.
    if 'b_factor_norm' in df_norm.columns:
         df_norm['b_factor'] = df_norm['b_factor_norm']
         df_norm.drop(columns=['b_factor_norm'], inplace=True)
         logger.debug("Renamed 'b_factor_norm' to 'b_factor' for consistency.")


    logger.debug(f"Feature normalization finished. Columns added/modified: {[c for c in df_norm.columns if c not in df.columns or not df_norm[c].equals(df[c])]}")
    return df_norm


def create_window_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """Create window-based features for sequence context."""
    window_cfg = get_window_config(config)
    if not window_cfg.get("enabled", False):
        logger.debug("Window features disabled.")
        return df

    window_size = window_cfg.get("size", 3)
    if not isinstance(window_size, int) or window_size <= 0:
        logger.warning(f"Invalid window size ({window_size}). Must be positive integer. Skipping window features.")
        return df

    # Identify base features to create windows from (must exist in df after processing)
    all_enabled_features = get_enabled_features(config)
    # We need the *encoded* or *normalized* versions if they exist
    potential_window_bases = [
        'resname_encoded', 'secondary_structure_encoded', 'core_exterior_encoded',
        'phi_sin', 'phi_cos', 'psi_sin', 'psi_cos', # Use sin/cos components
        'b_factor', # Use normalized B-factor if calculated
        'relative_accessibility' # Example continuous feature
    ]
    # Filter based on features actually present in df AND enabled in config
    features_to_window = [
        f for f in potential_window_bases
        if f in df.columns and f in all_enabled_features
    ]
    # Handle normalized features replacing originals
    if 'phi_sin' in features_to_window and 'phi_norm' in all_enabled_features: features_to_window.append('phi_norm') # Keep track
    if 'psi_sin' in features_to_window and 'psi_norm' in all_enabled_features: features_to_window.append('psi_norm')

    if not features_to_window:
         logger.warning("Window features enabled, but no suitable base features found in DataFrame. Skipping.")
         return df

    logger.info(f"Creating window features (size {window_size}) for: {features_to_window}")

    df_out = df.copy()
    domain_id_col = _get_column_name(df.columns, 'domain_id') or _get_column_name(df.columns, 'protein_id')
    resid_col = _get_column_name(df.columns, 'resid') or _get_column_name(df.columns, 'res_id')
    if not domain_id_col or not resid_col:
        logger.error("Cannot create window features: 'domain_id' or 'resid' column missing.")
        return df # Return original df

    # Ensure DataFrame is sorted by domain and residue index for correct shifting
    logger.debug(f"Sorting DataFrame by '{domain_id_col}' and '{resid_col}' for windowing.")
    df_out = df_out.sort_values(by=[domain_id_col, resid_col])

    grouped = df_out.groupby(domain_id_col, sort=False) # Use sort=False for potential speedup if already sorted

    window_feature_cols = [] # Keep track of new columns added
    padding_value = 0.0 # Value to use for padding at sequence ends

    for base_feature in features_to_window:
        # Skip the original 'phi_norm'/'psi_norm' if sin/cos used, avoid duplication in window
        if base_feature in ['phi_norm', 'psi_norm'] and f'{base_feature.split("_")[0]}_sin' in features_to_window:
             continue

        for k in range(1, window_size + 1): # Shifts from 1 to window_size
            # Shift backward (preceding residues)
            col_name_prev = f"{base_feature}_prev_{k}"
            df_out[col_name_prev] = grouped[base_feature].shift(k, fill_value=padding_value)
            window_feature_cols.append(col_name_prev)

            # Shift forward (succeeding residues)
            col_name_next = f"{base_feature}_next_{k}"
            df_out[col_name_next] = grouped[base_feature].shift(-k, fill_value=padding_value)
            window_feature_cols.append(col_name_next)

    logger.info(f"Added {len(window_feature_cols)} window feature columns.")

    # Reindex back to the original DataFrame's index if the order might have been disrupted
    # This ensures compatibility if the original index had specific meaning or order elsewhere.
    # However, since we usually split *after* processing, this might not be strictly necessary.
    # For safety, let's reindex if the original df index was not a simple range.
    if not df.index.equals(pd.RangeIndex(start=0, stop=len(df), step=1)):
         logger.debug("Reindexing DataFrame back to original index after windowing.")
         df_out = df_out.reindex(df.index)

    return df_out


def process_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """Main feature processing pipeline: clean -> encode -> normalize -> window."""
    logger.info(f"Starting feature processing pipeline. Initial shape: {df.shape}")
    df_processed = clean_data(df, config)
    df_processed = encode_features(df_processed, config)
    df_processed = normalize_features(df_processed, config)
    df_processed = create_window_features(df_processed, config) # Add window features last
    final_cols = df_processed.columns.tolist()
    logger.info(f"Feature processing complete. Final shape: {df_processed.shape}. Final columns: {final_cols}")
    return df_processed

def split_data(df: pd.DataFrame, config: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Splits data into train, validation, and test sets based on config.
    Handles random splitting and stratified splitting by domain ID.
    """
    split_config = get_split_config(config)
    system_config = get_system_config(config)

    test_size = split_config.get('test_size', 0.2)
    val_size = split_config.get('validation_size', 0.15) # Proportion of original data
    stratify = split_config.get('stratify_by_domain', True)
    random_state = system_config.get('random_state', 42)

    if not (0 < test_size < 1): raise ValueError(f"test_size must be between 0 and 1, got {test_size}")
    if not (0 <= val_size < 1): raise ValueError(f"validation_size must be between 0 and 1, got {val_size}")
    if test_size + val_size >= 1.0:
         raise ValueError(f"Sum of test_size ({test_size}) and validation_size ({val_size}) must be less than 1.")

    logger.info(f"Splitting data: Test={test_size*100:.1f}%, Val={val_size*100:.1f}%, Stratify by domain={stratify}, Seed={random_state}")

    domain_id_col = _get_column_name(df.columns, 'domain_id') or _get_column_name(df.columns, 'protein_id')

    if stratify:
        if not domain_id_col:
            logger.warning("Cannot stratify by domain: 'domain_id'/'protein_id' column missing. Performing random split instead.")
            stratify = False # Fallback to random split
        else:
            domains = df[domain_id_col].unique()
            n_domains = len(domains)
            logger.info(f"Found {n_domains} unique domains for stratified splitting.")

            if n_domains < 3: # Need at least one domain per potential split
                 logger.warning(f"Too few unique domains ({n_domains}) for stratified 3-way splitting. Performing random split instead.")
                 stratify = False # Fallback
            else:
                 # Split domains first using GroupShuffleSplit
                 # Split into Train+Val vs Test domains
                 gss_test = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)
                 train_val_idx, test_idx = next(gss_test.split(df, groups=df[domain_id_col]))

                 df_train_val = df.iloc[train_val_idx]
                 df_test = df.iloc[test_idx]

                 # Calculate relative validation size within the Train+Val split
                 relative_val_size = val_size / (1.0 - test_size)
                 if relative_val_size >= 1.0: # Should not happen due to initial check, but good safeguard
                      logger.warning(f"Relative validation size ({relative_val_size:.3f}) is >= 1. Adjusting validation split.")
                      # Allocate minimum to validation (e.g., 1 domain or small fraction)
                      # This needs careful handling based on domain counts. Simpler: maybe make val smaller?
                      # For now, proceed but it might merge train/val if val_size is too large.

                 # Split Train+Val into Train vs Val domains
                 gss_val = GroupShuffleSplit(n_splits=1, test_size=relative_val_size, random_state=random_state)
                 train_idx_rel, val_idx_rel = next(gss_val.split(df_train_val, groups=df_train_val[domain_id_col]))

                 # Use relative indices to get final train/val DataFrames
                 df_train = df_train_val.iloc[train_idx_rel]
                 df_val = df_train_val.iloc[val_idx_rel]

                 # Verify domain separation
                 train_domains = df_train[domain_id_col].unique()
                 val_domains = df_val[domain_id_col].unique()
                 test_domains = df_test[domain_id_col].unique()
                 if not set(train_domains).isdisjoint(val_domains) or \
                    not set(train_domains).isdisjoint(test_domains) or \
                    not set(val_domains).isdisjoint(test_domains):
                      logger.error("Stratified domain splitting failed! Overlap detected between splits. Check GroupShuffleSplit logic.")
                      # Fallback to random as emergency? Or raise error? Raising for now.
                      raise RuntimeError("Domain overlap detected in stratified split.")
                 logger.debug(f"Stratified domain split: Train={len(train_domains)}, Val={len(val_domains)}, Test={len(test_domains)} domains.")


    if not stratify: # Perform random split if stratification failed or disabled
        logger.debug("Performing random split.")
        # First split into Train+Val and Test
        df_train_val, df_test = train_test_split(
            df, test_size=test_size, random_state=random_state, shuffle=True
        )
        # Then split Train+Val into Train and Val
        # Adjust val_size relative to the remaining data
        relative_val_size = val_size / (1.0 - test_size)
        df_train, df_val = train_test_split(
            df_train_val, test_size=relative_val_size, random_state=random_state, shuffle=True
        )

    logger.info(f"Data split complete: Train={len(df_train)} ({len(df_train)/len(df)*100:.1f}%), "
                f"Val={len(df_val)} ({len(df_val)/len(df)*100:.1f}%), "
                f"Test={len(df_test)} ({len(df_test)/len(df)*100:.1f}%) rows.")

    if df_train.empty or df_test.empty: # Validation can sometimes be empty if val_size is very small
         logger.error("Train or Test split resulted in an empty DataFrame! Check split sizes and data.")
         raise ValueError("Empty Train or Test split created.")
    if df_val.empty and val_size > 0:
         logger.warning("Validation split is empty. Check validation_size and data.")


    # Return copies to avoid SettingWithCopyWarning downstream
    return df_train.copy(), df_val.copy(), df_test.copy()


def prepare_data_for_model(
    df: pd.DataFrame,
    config: Dict[str, Any],
    target_col: Optional[str] = None, # Target column name (e.g., 'flexibility_class')
    features: Optional[List[str]] = None # Optional: Explicit list of features to use
) -> Tuple[pd.DataFrame, Optional[pd.Series], List[str]]:
    """
    Selects features and optionally the target column for model input.
    Returns DataFrames/Series.

    Args:
        df: DataFrame with processed features and potentially the target class.
        config: Configuration dictionary.
        target_col: Name of the target column. If None, only X is returned.
        features: Optional list of feature names to use. If None, derived from config and window features.

    Returns:
        Tuple (X_df, y_series | None, final_feature_names).
        y_series is None if target_col is None.
    """
    logger.debug("Preparing DataFrame/Series for model input...")

    if features is None:
        # Determine feature columns automatically from config and generated columns
        base_enabled_features = get_enabled_features(config)
        window_cfg = get_window_config(config)
        final_feature_names = []

        # Add base features (considering normalized versions)
        for feature in base_enabled_features:
             if feature == 'phi_norm' and 'phi_sin' in df.columns:
                 final_feature_names.extend(['phi_sin', 'phi_cos'])
             elif feature == 'psi_norm' and 'psi_sin' in df.columns:
                 final_feature_names.extend(['psi_sin', 'psi_cos'])
             elif feature == 'b_factor' and 'b_factor_norm' in df.columns: # Check if normalized version exists
                 final_feature_names.append('b_factor_norm') # Prefer normalized if exists
             elif feature in df.columns:
                 final_feature_names.append(feature)
             else:
                 logger.warning(f"Enabled base feature '{feature}' not found in DataFrame columns. Skipping.")

        # Add window features if enabled
        if window_cfg.get("enabled", False) and window_cfg.get("size", 0) > 0:
            window_size = window_cfg["size"]
            potential_window_bases = [
                 f for f in [ # List encoded/normalized features that form windows
                      'resname_encoded', 'secondary_structure_encoded', 'core_exterior_encoded',
                      'phi_sin', 'phi_cos', 'psi_sin', 'psi_cos',
                      'b_factor', 'relative_accessibility' ]
                 if f in final_feature_names # Check if the base was actually included
             ]
            for base_feature in potential_window_bases:
                 for k in range(1, window_size + 1):
                     col_prev = f"{base_feature}_prev_{k}"
                     col_next = f"{base_feature}_next_{k}"
                     if col_prev in df.columns: final_feature_names.append(col_prev)
                     if col_next in df.columns: final_feature_names.append(col_next)

        # Remove duplicates just in case
        final_feature_names = sorted(list(set(final_feature_names)))

        if not final_feature_names:
             raise ValueError("No features selected based on config or found in the DataFrame.")
        logger.debug(f"Automatically selected {len(final_feature_names)} features based on config and DataFrame columns.")

    else:
         # Use provided feature list, ensuring they exist
         final_feature_names = [f for f in features if f in df.columns]
         if len(final_feature_names) != len(features):
              missing = set(features) - set(final_feature_names)
              logger.warning(f"Provided feature names not found in DataFrame: {missing}. Using {len(final_feature_names)} available features.")
         if not final_feature_names:
             raise ValueError("None of the explicitly provided feature names were found in the DataFrame.")
         logger.debug(f"Using {len(final_feature_names)} explicitly provided feature names.")

    # Extract features (X)
    try:
        X_df = df[final_feature_names]
        # Check for non-numeric types in features
        numeric_types = ['int64', 'float64', 'int32', 'float32']
        non_numeric_cols = X_df.select_dtypes(exclude=numeric_types).columns
        if not non_numeric_cols.empty:
             logger.warning(f"Selected features contain non-numeric columns: {non_numeric_cols.tolist()}. Model training might fail.")
             # Attempt conversion? Or raise error? For now, just warn.

    except KeyError as e:
        logger.error(f"Missing feature column during data preparation: {e}. Available columns: {df.columns.tolist()}")
        raise ValueError(f"Feature column required for model input not found: {e}")
    except Exception as e:
        logger.error(f"Error extracting features: {e}")
        raise

    # Extract target (y) if requested
    y_series: Optional[pd.Series] = None
    if target_col:
        target_col_actual = _get_column_name(df.columns, target_col)
        if not target_col_actual:
             raise ValueError(f"Target column '{target_col}' not found in DataFrame columns: {df.columns.tolist()}")
        try:
            y_series = df[target_col_actual]
            # Ensure target is integer type for classification
            if not pd.api.types.is_integer_dtype(y_series):
                 logger.warning(f"Target column '{target_col_actual}' is not integer type ({y_series.dtype}). Attempting conversion.")
                 try:
                      # Check for NaNs before conversion
                      if y_series.isnull().any():
                           logger.error(f"Target column '{target_col_actual}' contains NaN values. Cannot convert to integer.")
                           raise ValueError(f"NaNs found in target column '{target_col_actual}'.")
                      y_series = y_series.astype(int)
                 except (ValueError, TypeError) as e:
                      logger.error(f"Could not convert target column '{target_col_actual}' to integer: {e}")
                      raise ValueError(f"Target column '{target_col_actual}' must be integer or convertible to integer.")
        except KeyError:
            logger.error(f"Target column '{target_col_actual}' not found after name resolution check. This shouldn't happen.")
            raise ValueError(f"Target column '{target_col_actual}' unexpectedly not found.")
        except Exception as e:
             logger.error(f"Error extracting target column '{target_col_actual}': {e}")
             raise

    logger.debug(f"Model data prepared: X shape={X_df.shape}, y shape={y_series.shape if y_series is not None else 'N/A'}")
    return X_df, y_series, final_feature_names

--- End File: drwiggle/data/processor.py ---

--- File: drwiggle/models/base.py ---
---------------------------------------
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple
import numpy as np
import pandas as pd
import joblib # Common choice for saving sklearn-based models
import logging
import os

from drwiggle.config import get_model_config # Assumes this helper exists

logger = logging.getLogger(__name__)

class BaseClassifier(ABC):
    """
    Abstract Base Class for all drWiggle classification models.

    Defines the common interface for training, prediction, saving, loading,
    and accessing feature importances or performing hyperparameter optimization.
    """

    def __init__(self, config: Dict[str, Any], model_name: str):
        """
        Initialize the base classifier.

        Args:
            config: The main configuration dictionary, potentially already
                    path-resolved and templated.
            model_name: The specific name of this model (e.g., 'random_forest'),
                        used to fetch its configuration subsection.
        """
        self.config = config
        self.model_name = model_name
        # Get the specific configuration for this model, merged with common settings
        self.model_config = get_model_config(config, model_name)
        self.model = None # Placeholder for the actual underlying ML model instance
        self.feature_names_in_: Optional[List[str]] = None # Store feature names during fit
        self._fitted: bool = False # Track if the model has been successfully fitted

        logger.debug(f"BaseClassifier '{self.model_name}' initialized with config: {self.model_config}")

    @abstractmethod
    def fit(self, X: pd.DataFrame, y: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """
        Train the classifier on the input data.

        Implementations should handle preprocessing specific to the model (like scaling),
        store the feature names used for training in `self.feature_names_in_`,
        train the underlying `self.model`, and set `self._fitted = True`.

        Args:
            X: DataFrame of features (n_samples, n_features).
            y: Series of target class indices (n_samples,).
            X_val: Optional validation feature DataFrame for early stopping or evaluation during training.
            y_val: Optional validation target Series.

        Returns:
            self: The fitted classifier instance.
        """
        pass

    @abstractmethod
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Generate class predictions for new data.

        Implementations must ensure the input DataFrame `X` has the same features
        (in name and order) as used during training (`self.feature_names_in_`),
        apply any necessary preprocessing (like scaling using a fitted scaler),
        and return predictions from `self.model`.

        Args:
            X: DataFrame of features (n_samples, n_features) matching training features.

        Returns:
            NumPy array of predicted class indices (n_samples,).

        Raises:
            RuntimeError: If the model has not been fitted.
            ValueError: If input features mismatch training features.
        """
        if not self._fitted:
            raise RuntimeError(f"Model '{self.model_name}' has not been fitted yet. Call fit() first.")
        self._check_input_features(X) # Validate features before prediction
        pass

    @abstractmethod
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Generate class probability predictions for new data.

        Similar requirements as `predict` regarding feature consistency and preprocessing.

        Args:
            X: DataFrame of features (n_samples, n_features) matching training features.

        Returns:
            NumPy array of shape (n_samples, n_classes) with class probabilities.
            If probabilities are not supported, should raise NotImplementedError or return default values.

        Raises:
            RuntimeError: If the model has not been fitted.
            ValueError: If input features mismatch training features.
            NotImplementedError: If the underlying model doesn't support probabilities.
        """
        if not self._fitted:
            raise RuntimeError(f"Model '{self.model_name}' has not been fitted yet. Call fit() first.")
        self._check_input_features(X) # Validate features before prediction
        pass

    @abstractmethod
    def save(self, path: str):
        """
        Save the trained model state to disk.

        Implementations should save the `self.model` instance, `self.feature_names_in_`,
        any necessary preprocessing objects (like scalers), and potentially relevant
        configuration details (`self.model_config` or `self.config`).

        Args:
            path: The file path where the model should be saved. The directory
                  will be created if it doesn't exist.

        Raises:
            RuntimeError: If the model has not been fitted.
            IOError: If saving fails.
        """
        if not self._fitted:
            raise RuntimeError(f"Cannot save model '{self.model_name}' because it has not been fitted.")
        try:
             os.makedirs(os.path.dirname(path), exist_ok=True)
             logger.info(f"Saving model '{self.model_name}' to {path}...")
        except OSError as e:
             logger.error(f"Failed to create directory for saving model at {path}: {e}")
             raise IOError(f"Could not create directory for model file: {path}") from e
        pass # Implement actual saving logic in subclasses

    @classmethod
    @abstractmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'BaseClassifier':
        """
        Load a trained model state from disk.

        Implementations should load all necessary components saved by the `save` method
        (model, feature names, scalers, config) and return a fully functional,
        fitted instance of the classifier class.

        Args:
            path: Path to the saved model file.
            config: Optional current runtime config, may be used if saved config is incomplete
                    or for context during loading.

        Returns:
            A loaded, fitted instance of the classifier subclass.

        Raises:
            FileNotFoundError: If the model file doesn't exist.
            IOError: If loading fails.
            ValueError: If the loaded state is incompatible or corrupt.
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found at {path}")
        logger.info(f"Loading model '{cls.__name__}' from {path}...")
        pass # Implement actual loading logic in subclasses


    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """
        Get feature importance values if the underlying model supports it.

        Returns:
            A dictionary mapping feature names (from `self.feature_names_in_`)
            to their importance scores (float). Returns None if the model doesn't
            support feature importance or hasn't been fitted.
        """
        if not self._fitted:
            logger.warning(f"Cannot get feature importance for '{self.model_name}': Model not fitted.")
            return None
        # Implementation depends on the specific model (e.g., rf.feature_importances_)
        # Subclasses should override this method.
        logger.warning(f"Feature importance not implemented for model type '{self.__class__.__name__}'.")
        return None

    def hyperparameter_optimize(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization (HPO).

        This method should use the provided training and validation data to search
        for the best hyperparameters according to the HPO configuration specified
        in `self.model_config`.

        After finding the best parameters, it should ideally update `self.model_config`
        or the internal state of the classifier instance so that a subsequent call
        to `fit` uses these optimal parameters for the final training run.

        Args:
            X_train: Training feature DataFrame.
            y_train: Training target Series.
            X_val: Validation feature DataFrame.
            y_val: Validation target Series.

        Returns:
            A dictionary containing the best hyperparameters found during the search.

        Raises:
            NotImplementedError: If HPO is not configured or supported for this model.
            ValueError: If HPO configuration is invalid.
        """
        hpo_enabled = self.model_config.get('randomized_search', {}).get('enabled', False) or \
                      self.model_config.get('hyperparameter_optimization', {}).get('enabled', False)

        if not hpo_enabled:
            raise NotImplementedError(f"Hyperparameter optimization is not enabled in the configuration for model '{self.model_name}'.")

        # Subclasses must implement the specific HPO logic (e.g., using RandomizedSearchCV, Optuna).
        raise NotImplementedError(f"Hyperparameter optimization logic not implemented in subclass '{self.__class__.__name__}'.")


    def _check_input_features(self, X: pd.DataFrame):
        """Internal helper to validate input features against trained features."""
        if self.feature_names_in_ is None:
             # Should not happen if model is fitted, but safeguard
             raise RuntimeError(f"Model '{self.model_name}' is fitted but feature names are missing.")

        input_features = X.columns.tolist()
        if len(input_features) != len(self.feature_names_in_):
            raise ValueError(f"Input feature count mismatch for '{self.model_name}'. "
                             f"Expected {len(self.feature_names_in_)} features, got {len(input_features)}. "
                             f"Expected: {self.feature_names_in_}. Got: {input_features}")

        if input_features != self.feature_names_in_:
            # Check if sets are the same but order differs (more flexible)
            if set(input_features) == set(self.feature_names_in_):
                 logger.warning(f"Input features for '{self.model_name}' are in a different order than training. Reordering input.")
                 # Reorder DataFrame columns - ensure this doesn't cause issues downstream
                 try:
                     X_reordered = X[self.feature_names_in_]
                     # This modifies the DataFrame passed in if it's not a copy - careful!
                     # It might be safer to return the reordered df or work on a copy.
                     # For now, assume the caller handles the input DataFrame appropriately.
                     # If prediction methods work on copies or numpy arrays derived after this check, it's fine.
                     if not X.columns.equals(X_reordered.columns): # Check if reordering actually happened
                           # This might be too intrusive, commenting out direct modification
                           # X = X_reordered
                           pass # Let the calling method use X[self.feature_names_in_]

                 except Exception as e:
                     logger.error(f"Failed to reorder input features: {e}")
                     raise ValueError(f"Could not reorder input features for '{self.model_name}'.") from e
            else:
                 missing = set(self.feature_names_in_) - set(input_features)
                 extra = set(input_features) - set(self.feature_names_in_)
                 err_msg = f"Input feature mismatch for '{self.model_name}'."
                 if missing: err_msg += f" Missing: {missing}."
                 if extra: err_msg += f" Unexpected: {extra}."
                 raise ValueError(err_msg)

--- End File: drwiggle/models/base.py ---

--- File: drwiggle/models/lightgbm.py ---
---------------------------------------
# --- File: drwiggle/models/lightgbm_model.py ---
import logging
import os
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import pandas as pd
import joblib
import lightgbm as lgb # Import LightGBM
from sklearn.model_selection import RandomizedSearchCV
# LightGBM handles balanced classes internally, so sample_weight might not be needed
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.exceptions import NotFittedError
from sklearn.preprocessing import LabelEncoder

from .base import BaseClassifier
from drwiggle.utils.helpers import progress_bar, save_object, load_object

logger = logging.getLogger(__name__)

class LightGBMClassifier(BaseClassifier):
    """
    LightGBM Classifier for protein flexibility, integrating with BaseClassifier.
    Handles hyperparameter optimization via RandomizedSearchCV if configured.
    """

    def __init__(self, config: Dict[str, Any], model_name: str):
        super().__init__(config, model_name)
        self.num_classes = self.config.get('binning', {}).get('num_classes', 5)
        if self.num_classes <= 0:
            raise ValueError("Number of classes must be positive for LightGBM multi-class classification.")
        # LightGBM uses 'num_class' parameter
        logger.debug(f"LightGBMClassifier initialized for {self.num_classes} classes.")

    def fit(self, X: pd.DataFrame, y: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """
        Train the LightGBM classifier. Handles HPO if enabled and uses early stopping.

        Args:
            X: DataFrame of features.
            y: Series of target labels (must be 0-indexed integers).
            X_val: Validation features for early stopping.
            y_val: Validation labels for early stopping.
        """
        self.feature_names_in_ = X.columns.tolist()

        # --- Data Preparation ---
        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y.astype(int))
        if not np.array_equal(le.classes_, np.arange(self.num_classes)):
             logger.warning(f"Labels were re-encoded by LabelEncoder. Original unique labels: {np.unique(y.values)}, Encoded classes: {le.classes_}. Ensure labels match 0 to num_classes-1.")
        self.label_encoder_ = le

        eval_set = None
        eval_metric = self.model_config.get('eval_metric', 'multi_logloss') # Default metric
        fit_params = {} # Parameters passed directly to lgb.fit()

        if X_val is not None and y_val is not None:
            y_val_encoded = self.label_encoder_.transform(y_val.astype(int))
            # LightGBM needs eval_set as a list of tuples
            eval_set = [(X_val[self.feature_names_in_].values, y_val_encoded)]
            fit_params['eval_set'] = eval_set
            fit_params['eval_names'] = ['validation'] # Name for the eval set
            fit_params['eval_metric'] = eval_metric # Metric for early stopping
            logger.info(f"Validation set prepared for LightGBM early stopping. Shape: {X_val.shape}")

            early_stopping_rounds = self.model_config.get('training', {}).get('early_stopping_rounds', None)
            if early_stopping_rounds:
                 # Use callbacks for early stopping in LightGBM >= 4.0
                 # For older versions (<4.0), early_stopping_rounds was a direct fit parameter
                 try:
                     # Try using the callback API (LGBM >= 4.0)
                     fit_params['callbacks'] = [lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=self.model_config.get('training', {}).get('verbose', False))]
                     logger.info(f"Using LightGBM callback API for early stopping with rounds={early_stopping_rounds}")
                 except AttributeError:
                      # Fallback for older LightGBM versions
                      fit_params['early_stopping_rounds'] = early_stopping_rounds
                      fit_params['verbose'] = self.model_config.get('training', {}).get('verbose', False)
                      logger.info(f"Using LightGBM fit parameter for early stopping with rounds={early_stopping_rounds} (older LGBM version detected).")


        else:
            logger.warning("No validation set provided for LightGBM. Early stopping disabled.")


        # --- HPO Phase ---
        hpo_cfg = self.model_config.get('randomized_search', {})
        hpo_enabled = hpo_cfg.get('enabled', False)

        if hpo_enabled:
            logger.info(f"Starting hyperparameter optimization for {self.model_name} using RandomizedSearchCV...")
            try:
                best_params_found = self.hyperparameter_optimize(X, y, X_val, y_val)
                logger.info(f"Updating model config with best HPO params: {best_params_found}")
                self.model_config.update(best_params_found)
            except (NotImplementedError, ValueError) as e:
                logger.error(f"Hyperparameter optimization failed: {e}. Falling back to default parameters.")


        # --- Final Model Training ---
        logger.info(f"Fitting final {self.model_name} model...")

        # Extract parameters from config (potentially updated by HPO)
        lgb_params = {
            'objective': self.model_config.get('objective', 'multiclass'),
            'metric': eval_metric, # Use the same metric for internal tracking
            'num_class': self.num_classes,
            'n_estimators': self.model_config.get('n_estimators', 100),
            'learning_rate': self.model_config.get('learning_rate', 0.1),
            'num_leaves': self.model_config.get('num_leaves', 31), # Important LGBM param
            'max_depth': self.model_config.get('max_depth', -1), # -1 means no limit (use num_leaves)
            'subsample': self.model_config.get('subsample', 0.8), # Aliases: bagging_fraction
            'colsample_bytree': self.model_config.get('colsample_bytree', 0.8), # Aliases: feature_fraction
            'reg_alpha': self.model_config.get('reg_alpha', 0), # L1
            'reg_lambda': self.model_config.get('reg_lambda', 0), # L2
            # Class weight handling
            'class_weight': 'balanced' if self.model_config.get('class_weight', 'balanced') == 'balanced' else None,
            # 'is_unbalance': True if self.model_config.get('is_unbalance', False) else False, # Alternative way to handle imbalance
            'random_state': self.config.get('system', {}).get('random_state', 42),
            'n_jobs': self.config.get('system', {}).get('n_jobs', -1),
            'verbose': -1 # Suppress LightGBM default verbosity unless verbose in fit_params is set
        }
        logger.debug(f"Final LightGBM training parameters: {lgb_params}")

        self.model = lgb.LGBMClassifier(**lgb_params)

        try:
            # Fit the model with numpy arrays and optional early stopping params
            self.model.fit(X.values, y_train_encoded, **fit_params)
            logger.info(f"Final {self.model_name} training complete.")
            # Log best score if early stopping was used
            if 'callbacks' in fit_params or 'early_stopping_rounds' in fit_params:
                if hasattr(self.model, 'best_score_') and self.model.best_score_:
                     # LGBM stores best scores per eval set and metric
                     best_score_dict = self.model.best_score_
                     val_metric_key = next(iter(best_score_dict.get('validation', {})), None) # Get the first metric name used
                     if val_metric_key:
                         best_score = best_score_dict['validation'][val_metric_key]
                         logger.info(f"Best score ({val_metric_key}) during early stopping: {best_score:.4f} at iteration {self.model.best_iteration_}")
            self._fitted = True
        except Exception as e:
            logger.error(f"Failed to train final {self.model_name} model: {e}", exc_info=True)
            self._fitted = False
            raise

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class predictions."""
        super().predict(X)
        try:
            encoded_predictions = self.model.predict(X[self.feature_names_in_].values)
            # Return 0-indexed predictions directly
            return encoded_predictions
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict called but underlying LightGBM model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} prediction: {e}", exc_info=True)
            raise

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class probabilities."""
        super().predict_proba(X)
        try:
            if not hasattr(self.model, 'predict_proba'):
                raise NotImplementedError(f"The underlying {self.model.__class__.__name__} model does not support probability prediction.")
            probabilities = self.model.predict_proba(X[self.feature_names_in_].values)
            return probabilities
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict_proba called but underlying LightGBM model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} probability prediction: {e}", exc_info=True)
            raise

    def save(self, path: str):
        """Save the trained model state using joblib."""
        super().save(path)
        if not hasattr(self, 'label_encoder_'):
            logger.warning(f"Label encoder not found for model '{self.model_name}'. Saving without it.")
            label_encoder_state = None
        else:
            label_encoder_state = self.label_encoder_

        state = {
            'model': self.model, # LightGBM model object
            'label_encoder_': label_encoder_state,
            'feature_names_in_': self.feature_names_in_,
            'config': self.config,
            'model_config': self.model_config,
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'fitted': self._fitted
        }
        save_object(state, path)

    @classmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'LightGBMClassifier':
        """Load a trained model state using joblib."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found at {path}")
        logger.info(f"Loading model '{cls.__name__}' from {path}...")
        state = load_object(path)

        required_keys = ['model', 'feature_names_in_', 'model_config', 'model_name', 'fitted', 'num_classes']
        if not all(key in state for key in required_keys):
             raise ValueError(f"Loaded model state from {path} is missing required keys. Found: {list(state.keys())}")

        load_config = state.get('config', config)
        if load_config is None:
             raise ValueError("Cannot load model: No configuration found in saved state or provided at runtime.")

        instance = cls(config=load_config, model_name=state['model_name'])

        instance.model = state['model']
        instance.label_encoder_ = state.get('label_encoder_')
        instance.feature_names_in_ = state['feature_names_in_']
        instance.num_classes = state['num_classes']
        instance._fitted = state['fitted']
        # instance.model_config = state['model_config']

        if not instance._fitted:
             logger.warning(f"Loaded model '{instance.model_name}' from {path} indicates it was not fitted.")
        if not isinstance(instance.model, lgb.LGBMClassifier):
             raise TypeError(f"Loaded model is not a LightGBM LGBMClassifier instance (got {type(instance.model)}).")

        logger.info(f"Model '{instance.model_name}' loaded successfully.")
        return instance

    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """Get feature importance values from the trained LightGBM model."""
        if not self._fitted or not hasattr(self.model, 'feature_importances_'):
            logger.warning(f"Cannot get feature importance for '{self.model_name}': Model not fitted or importances not available.")
            return None

        importances = self.model.feature_importances_

        if self.feature_names_in_ and len(self.feature_names_in_) == len(importances):
            importance_dict = dict(zip(self.feature_names_in_, importances))
            sorted_importances = dict(sorted(importance_dict.items(), key=lambda item: item[1], reverse=True))
            return sorted_importances
        else:
            logger.warning(f"Feature names mismatch importance values for '{self.model_name}'. Returning indexed importances.")
            return {f"feature_{i}": imp for i, imp in enumerate(importances)}

    def hyperparameter_optimize(self, X_train, y_train, X_val, y_val) -> Dict[str, Any]:
        """
        Performs Hyperparameter Optimization using RandomizedSearchCV for LightGBM.
        """
        hpo_cfg = self.model_config.get('randomized_search')
        if not hpo_cfg or not hpo_cfg.get('enabled', False):
             raise NotImplementedError(f"RandomizedSearch HPO not enabled or configured for model '{self.model_name}'.")

        param_dist = hpo_cfg.get('param_distributions')
        n_iter = hpo_cfg.get('n_iter', 20)
        cv = hpo_cfg.get('cv', 3)
        scoring = hpo_cfg.get('scoring', 'balanced_accuracy')

        if not param_dist:
            raise ValueError("Parameter distributions ('param_distributions') not defined in HPO config.")

        logger.info(f"Running RandomizedSearchCV for {self.model_name}: n_iter={n_iter}, cv={cv}, scoring='{scoring}'")
        logger.debug(f"Search space: {param_dist}")

        # Base LightGBM estimator for search
        base_estimator = lgb.LGBMClassifier(
             objective='multiclass',
             metric='multi_logloss', # Internal metric, scoring param used by CV
             num_class=self.num_classes,
             class_weight='balanced', # Apply balancing within CV fits
             random_state=self.config.get('system', {}).get('random_state', 42),
             n_jobs=1, # Let RandomizedSearchCV handle parallelization
             verbose=-1 # Suppress verbosity during HPO fits
        )

        # Encode labels for CV
        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y_train.astype(int))

        # Note: RandomizedSearchCV doesn't easily support sample_weight per fold AND early stopping based on external val set.
        # Relying on 'class_weight' in the estimator is the simplest approach here for CV.

        search = RandomizedSearchCV(
            estimator=base_estimator,
            param_distributions=param_dist,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            random_state=self.config.get('system', {}).get('random_state', 42),
            n_jobs=self.config.get('system', {}).get('n_jobs', -1),
            verbose=1,
            error_score='raise'
        )

        try:
            # Run the search on training data
            search.fit(X_train.values, y_train_encoded) # No fit_params needed if using class_weight

            logger.info(f"RandomizedSearch complete. Best score ({scoring}): {search.best_score_:.4f}")
            logger.info(f"Best parameters found: {search.best_params_}")

            # Return only the best hyperparameters
            return search.best_params_

        except Exception as e:
            logger.error(f"RandomizedSearchCV failed for {self.model_name}: {e}", exc_info=True)
            raise ValueError(f"Hyperparameter optimization failed during RandomizedSearch.") from e

--- End File: drwiggle/models/lightgbm.py ---

--- File: drwiggle/models/neural_network.py ---
---------------------------------------
import logging
import os
import time
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import pandas as pd
import joblib # Add this import
import warnings

# PyTorch Imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau

# Other necessary imports
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.exceptions import NotFittedError

# Optuna for HPO
try:
    import optuna
    # Check if progress bar integration is available (Optuna v3+)
    _optuna_supports_progbar = hasattr(optuna.study.Study, 'optimize') and \
                               'show_progress_bar' in optuna.study.Study.optimize.__code__.co_varnames
    _optuna_available = True
except ImportError:
    optuna = None
    _optuna_supports_progbar = False
    _optuna_available = False

# Local imports
from .base import BaseClassifier
# Use helpers for joblib saving/loading within the class methods now
from drwiggle.utils.helpers import progress_bar, save_object, load_object, ensure_dir

logger = logging.getLogger(__name__)

# --- Neural Network Module Definition ---
class DrWiggleNN(nn.Module):
    """Feed-forward Neural Network for drWiggle classification."""
    def __init__(self, input_dim: int, num_classes: int, nn_config: Dict[str, Any]):
        """
        Args:
            input_dim: Number of input features.
            num_classes: Number of output classes.
            nn_config: The specific configuration dictionary for the neural_network model.
        """
        super().__init__()
        self.nn_config = nn_config # Store config for easy access
        self.num_classes = num_classes
        arch_cfg = nn_config.get('architecture', {})
        hidden_layers = arch_cfg.get('hidden_layers', [64, 32])
        dropout_rate = arch_cfg.get('dropout', 0.2)
        activation_str = arch_cfg.get('activation', 'relu').lower()
        self.is_ordinal = arch_cfg.get('ordinal_output', False) # Check config

        # Activation function
        if activation_str == 'relu':
            activation_fn = nn.ReLU()
        elif activation_str == 'leaky_relu':
            activation_fn = nn.LeakyReLU()
        # Add other activations like Tanh, Sigmoid if needed
        # elif activation_str == 'tanh':
        #    activation_fn = nn.Tanh()
        else:
            logger.warning(f"Activation function '{activation_str}' not recognized. Using ReLU.")
            activation_fn = nn.ReLU()

        # Build layers dynamically
        layers = []
        last_dim = input_dim
        for i, hidden_dim in enumerate(hidden_layers):
            layers.append(nn.Linear(last_dim, hidden_dim))
            # Consider LayerNorm or BatchNorm here if needed
            # layers.append(nn.BatchNorm1d(hidden_dim)) # Use BatchNorm for NNs
            layers.append(activation_fn)
            layers.append(nn.Dropout(dropout_rate))
            last_dim = hidden_dim

        # Create the sequential network
        self.network = nn.Sequential(*layers)

        # Define the final output layer
        if self.is_ordinal:
             # For ordinal regression, a common approach is to output num_classes - 1 logits
             # representing the boundaries between classes. A simpler (less correct) approach
             # used as a placeholder might output a single value to regress against class index.
             # We'll use the single output for simplicity here, requiring MSE loss later.
             # Proper ordinal methods (CORAL, CORN) are more involved.
             self.output_layer = nn.Linear(last_dim, 1) # Predict single value (class index proxy)
             logger.info("Configured NN output layer for Ordinal Regression (predicting single value).")
        else:
            # Standard classification: output logits for each class
            self.output_layer = nn.Linear(last_dim, num_classes)
            logger.info(f"Configured NN output layer for Standard Classification ({num_classes} classes).")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network."""
        x = self.network(x)
        x = self.output_layer(x)
        return x

# --- Main Classifier Class ---
class NeuralNetworkClassifier(BaseClassifier):
    """Neural Network classifier implementing the BaseClassifier interface."""

    def __init__(self, config: Dict[str, Any], model_name: str):
        super().__init__(config, model_name)
        self.device = self._get_device()
        self.scaler = StandardScaler() # Scaler for input features
        self.num_classes = self.config.get('binning', {}).get('num_classes', 5)
        self.history: Dict[str, List[float]] = {'train_loss': [], 'val_loss': [], 'val_accuracy': []} # Track training progress
        self.best_val_metric = float('inf') if self._is_loss_objective() else float('-inf') # Track best validation metric during training
        self.best_model_state_dict: Optional[Dict[str, torch.Tensor]] = None

    def _get_device(self) -> torch.device:
        """Determine compute device (GPU or CPU) based on config and availability."""
        gpu_enabled_cfg = self.config.get("system", {}).get("gpu_enabled", "auto")
        force_cpu = gpu_enabled_cfg == False
        force_gpu = gpu_enabled_cfg == True
        auto_detect = gpu_enabled_cfg == "auto"

        if not force_cpu and torch.cuda.is_available():
             logger.info("CUDA (Nvidia GPU) is available and selected.")
             return torch.device("cuda")
        elif not force_cpu and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
             # Check MPS availability specifically for Apple Silicon
             is_built = torch.backends.mps.is_built()
             if is_built:
                logger.info("MPS (Apple Silicon GPU) is available and selected.")
                return torch.device("mps")
             else:
                logger.warning("MPS available but not built correctly? Falling back.")

        elif force_gpu:
             # If GPU explicitly requested but none found
             raise RuntimeError("GPU usage requested (system.gpu_enabled=true), but no compatible GPU (CUDA/MPS) found.")

        # Default to CPU if auto and no GPU, or if explicitly forced
        logger.info("Using CPU.")
        return torch.device("cpu")

    def _is_loss_objective(self) -> bool:
        """Check if the HPO/early stopping objective is loss (needs minimization)."""
        # Default HPO objective is val_loss
        hpo_metric = self.model_config.get('hyperparameter_optimization', {}).get('objective_metric', 'val_loss')
        # Early stopping is usually based on val_loss
        return 'loss' in hpo_metric.lower()

    def _get_optimizer(self, model_params) -> torch.optim.Optimizer:
        """Create optimizer based on config."""
        optimizer_name = self.model_config.get('training', {}).get('optimizer', 'adam').lower()
        lr = float(self.model_config.get('training', {}).get('learning_rate', 0.001)) # Ensure float

        if optimizer_name == 'adam':
            return optim.Adam(model_params, lr=lr)
        elif optimizer_name == 'adamw': # Add AdamW as good alternative
             return optim.AdamW(model_params, lr=lr)
        elif optimizer_name == 'sgd':
             # Add momentum commonly used with SGD
             momentum = float(self.model_config.get('training', {}).get('sgd_momentum', 0.9))
             return optim.SGD(model_params, lr=lr, momentum=momentum)
        else:
            logger.warning(f"Optimizer '{optimizer_name}' not recognized. Using Adam.")
            return optim.Adam(model_params, lr=lr)

    def _get_loss_function(self, y_train: Optional[np.ndarray] = None) -> nn.Module:
        """
        Create loss function. Handles class weights for classification and
        uses MSE as a *placeholder* for ordinal regression.
        """
        arch_cfg = self.model_config.get('architecture', {})
        is_ordinal = arch_cfg.get('ordinal_output', False)

        if is_ordinal:
            # Using MSE on predicted value vs target class index.
            # !!! THIS IS NOT CORRECT ORDINAL REGRESSION but a simple starting point. !!!
            # Replace with proper ordinal loss like CORN or CORAL if implementing seriously.
            logger.warning("Using MSE loss as a placeholder for ordinal regression. This is not theoretically sound. Implement CORN/CORAL loss for proper ordinal handling.")
            return nn.MSELoss()
        else:
            # Standard CrossEntropyLoss for classification
            use_class_weights = self.model_config.get('training', {}).get('class_weights', True)
            weights = None
            if use_class_weights and y_train is not None and len(y_train) > 0:
                try:
                    unique_classes = np.unique(y_train)
                    if len(unique_classes) > 1: # Avoid error if only one class present
                        weights_np = compute_class_weight('balanced', classes=unique_classes, y=y_train)
                        weights = torch.tensor(weights_np, dtype=torch.float32).to(self.device)
                        logger.info(f"Using balanced class weights: {weights_np.round(3)}")
                    else:
                         logger.warning("Only one class present in training data. Cannot compute balanced weights.")
                except Exception as e:
                    logger.warning(f"Could not compute class weights: {e}. Using uniform weights.")

            return nn.CrossEntropyLoss(weight=weights)

    def _prepare_dataloaders(self, X: pd.DataFrame, y: pd.Series,
                             X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None
                             ) -> Tuple[DataLoader, Optional[DataLoader]]:
        """Scales data and creates PyTorch DataLoaders."""
        logger.debug("Scaling data and creating DataLoaders...")
        # Scale features - fit scaler only on training data
        X_train_scaled = self.scaler.fit_transform(X.values) # Fit and transform train
        logger.info(f"Scaler fitted on training data. Mean: {self.scaler.mean_[:5].round(3)}..., Scale: {self.scaler.scale_[:5].round(3)}...")

        X_val_scaled = self.scaler.transform(X_val.values) if X_val is not None else None

        # Convert to tensors
        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
        # Target type depends on loss function
        is_ordinal = self.model_config.get('architecture', {}).get('ordinal_output', False)
        y_dtype = torch.float32 if is_ordinal else torch.long # Float for MSE, Long for CrossEntropy
        y_train_tensor = torch.tensor(y.values, dtype=y_dtype)

        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        batch_size = self.model_config.get('training', {}).get('batch_size', 64)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Drop last to avoid small batches

        val_loader = None
        if X_val_scaled is not None and y_val is not None:
            y_val_tensor = torch.tensor(y_val.values, dtype=y_dtype)
            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            # Use larger batch size for validation as gradients aren't computed
            val_loader = DataLoader(val_dataset, batch_size=batch_size * 4, shuffle=False)
        else:
             logger.warning("No validation data provided. Early stopping and validation metrics will be unavailable.")

        logger.debug(f"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader) if val_loader else 0}")
        return train_loader, val_loader

    def fit(self, X: pd.DataFrame, y: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """Train the Neural Network classifier."""
        start_time = time.time()
        self.feature_names_in_ = X.columns.tolist() # Store feature names

        # --- HPO Phase (if enabled) ---
        hpo_cfg = self.model_config.get('hyperparameter_optimization', {})
        hpo_enabled = hpo_cfg.get('enabled', False)

        if hpo_enabled:
             logger.info(f"Starting hyperparameter optimization for {self.model_name}...")
             if not _optuna_available:
                 logger.error("Optuna not available, cannot perform hyperparameter optimization. Please install optuna (`pip install optuna`).")
                 raise ImportError("Optuna is required for NN hyperparameter optimization but not installed.")
             try:
                 # Use provided validation set for HPO trials
                 if X_val is None or y_val is None:
                      raise ValueError("Validation data (X_val, y_val) is required for hyperparameter optimization.")
                 best_hpo_params = self.hyperparameter_optimize(X, y, X_val, y_val)
                 # Update self.model_config with the best params found for the final training run
                 logger.info(f"Updating model config with best HPO params for final training: {best_hpo_params}")
                 # This requires carefully merging best_hpo_params into self.model_config structure
                 arch_cfg = self.model_config.setdefault('architecture', {})
                 train_cfg = self.model_config.setdefault('training', {})
                 arch_cfg['hidden_layers'] = best_hpo_params.get('hidden_layers', arch_cfg.get('hidden_layers'))
                 arch_cfg['activation'] = best_hpo_params.get('activation', arch_cfg.get('activation'))
                 arch_cfg['dropout'] = best_hpo_params.get('dropout', arch_cfg.get('dropout'))
                 train_cfg['learning_rate'] = best_hpo_params.get('learning_rate', train_cfg.get('learning_rate'))
                 train_cfg['batch_size'] = best_hpo_params.get('batch_size', train_cfg.get('batch_size'))
                 train_cfg['optimizer'] = best_hpo_params.get('optimizer', train_cfg.get('optimizer'))

             except (NotImplementedError, ValueError, ImportError) as e:
                 logger.error(f"Hyperparameter optimization failed: {e}. Proceeding with default parameters from config.", exc_info=True)
                 # Continue with default parameters

        # --- Final Model Training ---
        logger.info(f"Starting final training for {self.model_name}...")
        train_loader, val_loader = self._prepare_dataloaders(X, y, X_val, y_val)

        # Initialize model, loss, optimizer based on potentially updated config
        self.model = DrWiggleNN(len(self.feature_names_in_), self.num_classes, self.model_config).to(self.device)
        criterion = self._get_loss_function(y.values) # Pass train labels for class weights
        optimizer = self._get_optimizer(self.model.parameters())

        # Optional: Learning rate scheduler
        scheduler = None
        lr_scheduler_cfg = self.model_config.get('training', {}).get('lr_scheduler', {}) # Use get for safety
        if lr_scheduler_cfg and lr_scheduler_cfg.get('enabled', False):
             scheduler_params = lr_scheduler_cfg
             scheduler = ReduceLROnPlateau(optimizer,
                                           mode='min' if self._is_loss_objective() else 'max',
                                           factor=scheduler_params.get('factor', 0.1),
                                           patience=scheduler_params.get('patience', 5), # Scheduler patience
                                           verbose=True)
             logger.info(f"Enabled ReduceLROnPlateau LR scheduler (factor={scheduler.factor}, patience={scheduler.patience}).")


        # Training loop config
        train_cfg = self.model_config.get('training', {})
        epochs = train_cfg.get('epochs', 100)
        early_stopping_enabled = train_cfg.get('early_stopping', True) and val_loader is not None
        patience = train_cfg.get('patience', 10) # Early stopping patience
        patience_counter = 0
        minimize_metric = self._is_loss_objective() # True if we monitor loss, False if accuracy

        # Reset history and best state before training loop
        self.history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}
        self.best_val_metric = float('inf') if minimize_metric else float('-inf')
        self.best_model_state_dict = None

        logger.info(f"Training {self.model_name} for {epochs} epochs on device {self.device}...")
        for epoch in range(epochs):
            self.model.train()
            epoch_train_loss = 0.0
            # Training Batches
            for batch_X, batch_y in progress_bar(train_loader, desc=f"Epoch {epoch+1}/{epochs} Train", leave=False):
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                optimizer.zero_grad()
                outputs = self.model(batch_X)

                # Adjust target shape for MSELoss if needed
                if isinstance(criterion, nn.MSELoss): # Ordinal placeholder
                     loss = criterion(outputs, batch_y.unsqueeze(-1)) # Target needs shape (batch_size, 1)
                else: # Classification
                    loss = criterion(outputs, batch_y)

                loss.backward()
                # Optional: Gradient clipping
                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                epoch_train_loss += loss.item() * batch_X.size(0) # Accumulate loss scaled by batch size

            avg_train_loss = epoch_train_loss / len(train_loader.dataset)
            self.history['train_loss'].append(avg_train_loss)

            # Validation Phase
            avg_val_loss = np.nan
            val_accuracy = np.nan
            current_val_metric = np.nan # Metric used for early stopping/best model saving

            if val_loader:
                 self.model.eval()
                 epoch_val_loss = 0.0
                 correct_preds = 0
                 total_preds = 0
                 with torch.no_grad():
                      for batch_X_val, batch_y_val in val_loader:
                           batch_X_val, batch_y_val = batch_X_val.to(self.device), batch_y_val.to(self.device)
                           outputs_val = self.model(batch_X_val)

                           # Calculate validation loss
                           if isinstance(criterion, nn.MSELoss): # Ordinal placeholder
                                loss_val = criterion(outputs_val, batch_y_val.unsqueeze(-1))
                           else: # Classification
                                loss_val = criterion(outputs_val, batch_y_val)
                           epoch_val_loss += loss_val.item() * batch_X_val.size(0)

                           # Calculate accuracy (adapt for ordinal placeholder)
                           if isinstance(criterion, nn.MSELoss):
                                # Simple accuracy for ordinal: check if rounded prediction matches target index
                                preds = torch.round(outputs_val.squeeze()).clamp(0, self.num_classes - 1).long()
                           else:
                                _, preds = torch.max(outputs_val, 1)

                           correct_preds += (preds == batch_y_val).sum().item()
                           total_preds += batch_y_val.size(0)

                 avg_val_loss = epoch_val_loss / len(val_loader.dataset)
                 val_accuracy = correct_preds / total_preds if total_preds > 0 else 0.0
                 self.history['val_loss'].append(avg_val_loss)
                 self.history['val_accuracy'].append(val_accuracy)

                 # Determine the metric for comparison (loss or accuracy)
                 current_val_metric = avg_val_loss if minimize_metric else val_accuracy

                 log_msg = (f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.5f}, "
                            f"Val Loss: {avg_val_loss:.5f}, Val Acc: {val_accuracy:.4f}")

                 # LR Scheduler step
                 if scheduler:
                     scheduler.step(current_val_metric)
                     # Log current LR
                     current_lr = optimizer.param_groups[0]['lr']
                     log_msg += f", LR: {current_lr:.1e}"

                 logger.info(log_msg)

                 # Early Stopping & Best Model Saving Check
                 improvement = False
                 if np.isnan(current_val_metric):
                     logger.warning(f"Validation metric is NaN at epoch {epoch+1}. Cannot determine improvement.")
                 elif minimize_metric: # Lower is better (loss)
                     if current_val_metric < self.best_val_metric:
                         improvement = True
                 else: # Higher is better (accuracy)
                     if current_val_metric > self.best_val_metric:
                         improvement = True

                 if improvement:
                      self.best_val_metric = current_val_metric
                      patience_counter = 0
                      # Save the state dict of the best model found so far
                      self.best_model_state_dict = self.model.state_dict()
                      logger.debug(f"Validation metric improved to {self.best_val_metric:.4f}. Resetting patience. Saved best model state.")
                 else:
                      patience_counter += 1
                      logger.debug(f"Validation metric did not improve. Patience: {patience_counter}/{patience}")

                 if early_stopping_enabled and patience_counter >= patience:
                     logger.info(f"Early stopping triggered at epoch {epoch+1} due to lack of improvement for {patience} epochs.")
                     break # Exit training loop

            else: # No validation loader
                 self.history['val_loss'].append(np.nan)
                 self.history['val_accuracy'].append(np.nan)
                 logger.info(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.5f}")


        # After loop finishes (or breaks), load the best model state if available
        if self.best_model_state_dict:
            logger.info(f"Restoring model weights from epoch with best validation metric ({self.best_val_metric:.4f}).")
            self.model.load_state_dict(self.best_model_state_dict)
        else:
             # This can happen if no validation data was provided, or if training stopped at epoch 0,
             # or if the validation metric never improved.
             logger.warning("No best model state was saved during training. Using the final model state (which might not be optimal).")

        self._fitted = True
        logger.info(f"NN training finished in {time.time() - start_time:.2f} seconds.")
        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class predictions."""
        super().predict(X) # Checks fitted state and features
        if self.model is None: raise RuntimeError("Model is not initialized.")
        self.model.eval() # Set model to evaluation mode

        try:
             X_scaled = self.scaler.transform(X[self.feature_names_in_].values) # Scale using fitted scaler
        except NotFittedError:
             raise RuntimeError("Scaler has not been fitted. Call fit() before predicting.")
        except Exception as e:
             logger.error(f"Error scaling input data during prediction: {e}", exc_info=True)
             raise

        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)

        predictions = []
        # Predict in batches to avoid OOM errors on large inputs
        # Use a potentially larger batch size for inference
        batch_size = self.model_config.get('training', {}).get('batch_size', 64) * 4
        pred_dataset = TensorDataset(X_tensor)
        # Disable num_workers for simple inference if causing issues
        pred_loader = DataLoader(pred_dataset, batch_size=batch_size, num_workers=0, pin_memory=False)


        with torch.no_grad():
            for batch_X_list in progress_bar(pred_loader, desc="Predicting", leave=False):
                 batch_X = batch_X_list[0].to(self.device) # DataLoader returns list/tuple, move to device
                 outputs = self.model(batch_X)
                 # Convert outputs to class indices
                 if self.model_config.get('architecture', {}).get('ordinal_output', False):
                     # Placeholder: Round output for ordinal prediction, clamp to valid range
                     preds = torch.round(outputs.squeeze()).clamp(0, self.num_classes - 1).long()
                 else:
                     # Standard classification: argmax
                     _, preds = torch.max(outputs, 1)
                 predictions.append(preds.cpu().numpy())

        return np.concatenate(predictions)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class probabilities."""
        super().predict_proba(X) # Checks fitted state and features
        if self.model is None: raise RuntimeError("Model is not initialized.")
        self.model.eval() # Set model to evaluation mode

        try:
             X_scaled = self.scaler.transform(X[self.feature_names_in_].values)
        except NotFittedError:
             raise RuntimeError("Scaler has not been fitted. Call fit() before predicting probabilities.")
        except Exception as e:
             logger.error(f"Error scaling input data during probability prediction: {e}", exc_info=True)
             raise

        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)

        probabilities = []
        # Predict in batches
        batch_size = self.model_config.get('training', {}).get('batch_size', 64) * 4
        pred_dataset = TensorDataset(X_tensor)
        pred_loader = DataLoader(pred_dataset, batch_size=batch_size, num_workers=0, pin_memory=False)


        with torch.no_grad():
            for batch_X_list in progress_bar(pred_loader, desc="Predicting Probs", leave=False):
                batch_X = batch_X_list[0].to(self.device)
                outputs = self.model(batch_X)

                # Calculate probabilities
                if self.model_config.get('architecture', {}).get('ordinal_output', False):
                    # Probabilities are not well-defined for the MSE ordinal placeholder.
                    # Return uniform probabilities or raise error? Returning uniform for now.
                    logger.warning("Probability prediction not well-defined for current ordinal placeholder. Returning uniform distribution.")
                    # Need to ensure output shape is correct for broadcasting
                    uniform_prob = 1.0 / self.num_classes
                    probs = torch.full((outputs.shape[0], self.num_classes), uniform_prob, device=self.device, dtype=torch.float32)
                else:
                    # Apply Softmax for standard classification probabilities
                    probs = torch.softmax(outputs, dim=1)

                probabilities.append(probs.cpu().numpy())

        return np.concatenate(probabilities)

    # <<<--- START MODIFIED SAVE METHOD --- >>>
    def save(self, path: str):
        """Save the model state_dict using torch.save and the rest using joblib."""
        # Derive paths for state_dict (.pt) and metadata (.joblib)
        base_path, _ = os.path.splitext(path)
        state_dict_path = f"{base_path}_statedict.pt"
        # Ensure meta_path IS the original path passed (expected to be .joblib)
        meta_path = path # Assume path is e.g. .../neural_network.joblib

        # Check fitted status (from BaseClassifier.save logic)
        if not self._fitted:
            raise RuntimeError(f"Cannot save model '{self.model_name}' because it has not been fitted.")
        try: # Add basic error handling around ensure_dir too
            ensure_dir(os.path.dirname(path)) # Ensure directory for meta_path exists
        except OSError as e:
             logger.error(f"Failed to create directory for saving model at {path}: {e}")
             raise IOError(f"Could not create directory for model file: {path}") from e

        # 1. Save Model State Dict using torch.save
        if self.model is None:
            logger.warning(f"Model object is None for '{self.model_name}'. Cannot save model state dict.")
        else:
            try:
                ensure_dir(os.path.dirname(state_dict_path)) # Ensure dir exists
                torch.save(self.model.state_dict(), state_dict_path)
                logger.info(f"NeuralNetwork state_dict saved to {state_dict_path}")
            except Exception as e:
                logger.error(f"Failed to save NeuralNetwork state_dict to {state_dict_path}: {e}", exc_info=True)
                raise IOError(f"Could not save NN state_dict to {state_dict_path}") from e

        # 2. Save Metadata (scaler, config, etc.) using joblib
        if not hasattr(self, 'scaler') or not hasattr(self.scaler, 'mean_'):
             logger.warning(f"Scaler not found or not fitted for model '{self.model_name}'. Saving metadata without scaler state.")
             scaler_state = None
        else:
             scaler_state = self.scaler

        meta_state = {
            # DO NOT save model_state_dict here
            'scaler_state': scaler_state,
            'feature_names_in_': self.feature_names_in_,
            'config': self.config,
            'model_config': self.model_config,
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'history': self.history,
            'fitted': self._fitted # Still save fitted status based on training completion
        }
        try:
            save_object(meta_state, meta_path) # Use helper which uses joblib
            logger.info(f"NeuralNetwork metadata saved to {meta_path}")
        except Exception as e:
            logger.error(f"Failed to save NeuralNetwork metadata to {meta_path}: {e}", exc_info=True)
            # Should we delete the state_dict if meta fails? Maybe.
            if self.model is not None and os.path.exists(state_dict_path):
                 try: os.remove(state_dict_path)
                 except OSError: pass
            raise IOError(f"Could not save NN metadata state to {meta_path}") from e
    # <<<--- END MODIFIED SAVE METHOD --- >>>

    # <<<--- START MODIFIED LOAD METHOD --- >>>
    @classmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'NeuralNetworkClassifier':
        """Load metadata using joblib and model state_dict using torch.load."""
        # --- NEW Simpler Path Derivation ---
        meta_path = path # Assume path is the .joblib file
        base_path, _ = os.path.splitext(meta_path)
        state_dict_path = f"{base_path}_statedict.pt" # Derive state_dict path from joblib path
        # --- End NEW Path Derivation ---

        if not os.path.exists(meta_path):
            raise FileNotFoundError(f"Metadata file not found at {meta_path}")
        if not os.path.exists(state_dict_path):
             raise FileNotFoundError(f"Model state_dict file not found at {state_dict_path}")

        logger.info(f"Loading model '{cls.__name__}' metadata from {meta_path} and state_dict from {state_dict_path}...")

        # 1. Load Metadata using joblib (via helper)
        try:
            state = load_object(meta_path)
        except Exception as e:
            logger.error(f"Failed to load metadata from {meta_path}: {e}", exc_info=True)
            raise IOError(f"Could not load NN metadata from {meta_path}") from e

        # Basic validation of loaded metadata state
        required_keys = ['scaler_state', 'feature_names_in_', 'model_config', 'model_name', 'num_classes', 'fitted']
        if not all(key in state for key in required_keys):
             raise ValueError(f"Loaded NN metadata from {meta_path} is missing required keys. Found: {list(state.keys())}")

        # Use loaded config if available, otherwise fall back to provided runtime config
        load_config = state.get('config', config)
        if load_config is None:
             raise ValueError("Cannot load NN model: No configuration found in saved state or provided at runtime.")

        # Re-instantiate the class
        instance = cls(config=load_config, model_name=state['model_name'])

        # Restore state from metadata
        instance.scaler = state['scaler_state']
        instance.feature_names_in_ = state['feature_names_in_']
        instance.num_classes = state['num_classes']
        instance.history = state.get('history', {'train_loss': [], 'val_loss': [], 'val_accuracy': []}) # Handle older saves
        instance._fitted = state['fitted']

        # --- Determine device for loading state_dict ---
        runtime_config_system = load_config.get("system", {}) # Use potentially loaded config
        gpu_enabled_cfg = runtime_config_system.get("gpu_enabled", "auto")
        force_cpu = gpu_enabled_cfg == False
        if not force_cpu and torch.cuda.is_available(): device = torch.device("cuda")
        elif not force_cpu and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and torch.backends.mps.is_built(): device = torch.device("mps")
        else: device = torch.device("cpu")
        instance.device = device
        logger.info(f"Loading NeuralNetwork state_dict to device {device}")

        # 2. Re-create model architecture & Load State Dict using torch.load
        input_dim = len(instance.feature_names_in_) if instance.feature_names_in_ else None
        if input_dim is None and instance.scaler and hasattr(instance.scaler, 'n_features_in_'):
              input_dim = instance.scaler.n_features_in_
        if input_dim is None:
              raise ValueError("Cannot determine input dimension for loading NN model (missing feature names and scaler info).")

        try:
            instance.model = DrWiggleNN(input_dim, instance.num_classes, instance.model_config).to(instance.device)
            # Load state dict - use weights_only=False explicitly if needed for compatibility or trust source
            # Using weights_only=True (default in PyTorch >= 2.6) is safer if possible
            try:
                # Try default first (weights_only=True in newer PyTorch)
                 model_state_dict = torch.load(state_dict_path, map_location=device)
            except (TypeError, RuntimeError, AttributeError, EOFError) as e:
                 # Fallback for older PyTorch versions or if weights_only=True fails unexpectedly
                 # Check the error message - if it explicitly mentions weights_only, try False
                 if 'weights_only' in str(e):
                      logger.warning(f"torch.load with weights_only=True failed. Attempting with weights_only=False. Ensure the file source '{state_dict_path}' is trusted.")
                      model_state_dict = torch.load(state_dict_path, map_location=device, weights_only=False)
                 else:
                      raise # Re-raise other torch load errors

            instance.model.load_state_dict(model_state_dict)
            instance.model.eval() # Set to evaluation mode
            logger.info("Model architecture recreated and state dict loaded successfully.")
        except FileNotFoundError:
             logger.error(f"Model state_dict file not found: {state_dict_path}")
             raise
        except Exception as e:
            logger.error(f"Failed to load torch state_dict from {state_dict_path}: {e}", exc_info=True)
            # Mark as not fitted if state dict loading fails
            instance.model = None
            instance._fitted = False
            raise IOError(f"Could not load NN state_dict from {state_dict_path}") from e

        # Validate scaler (moved check here)
        if instance.scaler and not hasattr(instance.scaler, 'mean_'):
             logger.warning(f"Loaded scaler for model '{instance.model_name}' appears not to be fitted.")
             # Mark as not fitted if scaler missing/unfitted?
             # instance._fitted = False # Let's assume fitted status from meta_state is primary indicator

        if not instance._fitted:
             logger.warning(f"Loaded model '{instance.model_name}' from {meta_path} is marked as not fitted (check metadata file).")

        logger.info(f"Model '{instance.model_name}' loaded successfully.")
        return instance
    # <<<--- END MODIFIED LOAD METHOD --- >>>


    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """Feature importance for NNs requires advanced methods (SHAP, Captum, Permutation). Not implemented."""
        logger.warning(f"Feature importance calculation is not implemented for '{self.__class__.__name__}'. Requires methods like SHAP or Permutation Importance.")
        return None

    def hyperparameter_optimize(self, X_train, y_train, X_val, y_val) -> Dict[str, Any]:
         """Perform hyperparameter optimization using Optuna."""
         hpo_cfg = self.model_config.get('hyperparameter_optimization', {})
         if not hpo_cfg.get('enabled', False):
              raise NotImplementedError(f"Hyperparameter optimization not enabled in config for '{self.model_name}'.")
         if not _optuna_available:
              raise ImportError("Optuna is required for NN HPO but not installed (`pip install optuna`).")

         method = hpo_cfg.get('method', 'random').lower() # TODO: Add support for TPE etc.
         n_trials = hpo_cfg.get('trials', 20)
         param_space_cfg = hpo_cfg.get('parameters', {})
         objective_metric = hpo_cfg.get('objective_metric', 'val_loss').lower()
         hpo_epochs = hpo_cfg.get('epochs_per_trial', 50) # Limit epochs per trial
         hpo_patience = hpo_cfg.get('patience_per_trial', 5) # Shorter patience for HPO

         if not param_space_cfg:
              raise ValueError(f"Hyperparameter optimization enabled but no 'parameters' space defined in config for '{self.model_name}'.")

         logger.info(f"Starting Optuna HPO for '{self.model_name}' ({method} search, {n_trials} trials, objective: {objective_metric}, {hpo_epochs} epochs/trial)...")

         # --- Pre-prepare DataLoaders once outside the objective function ---
         # This saves repeated scaling and tensor conversion in each trial
         logger.debug("Preparing DataLoaders for HPO trials...")
         # Fit scaler on the full training set passed to HPO
         X_train_scaled_hpo = self.scaler.fit_transform(X_train.values)
         X_val_scaled_hpo = self.scaler.transform(X_val.values)
         is_ordinal_hpo = self.model_config.get('architecture', {}).get('ordinal_output', False)
         y_dtype_hpo = torch.float32 if is_ordinal_hpo else torch.long
         y_train_tensor_hpo = torch.tensor(y_train.values, dtype=y_dtype_hpo)
         y_val_tensor_hpo = torch.tensor(y_val.values, dtype=y_dtype_hpo)

         logger.debug("Data scaling and tensor conversion complete for HPO.")

         # --- Objective Function for Optuna ---
         def objective(trial: optuna.Trial) -> float:
             # Suggest hyperparameters based on param_space_cfg
             trial_arch_params = {}
             trial_train_params = {}

             # Architecture Params
             hs_choices = param_space_cfg.get('hidden_layers')
             trial_arch_params['hidden_layers'] = trial.suggest_categorical('hidden_layers', hs_choices) if hs_choices else [64, 32]
             act_choices = param_space_cfg.get('activation')
             trial_arch_params['activation'] = trial.suggest_categorical('activation', act_choices) if act_choices else 'relu'
             drp_range = param_space_cfg.get('dropout')
             trial_arch_params['dropout'] = trial.suggest_float('dropout', drp_range[0], drp_range[1]) if drp_range and len(drp_range)==2 else trial.suggest_float('dropout', 0.1, 0.5) # Default range

             # Training Params
             lr_range = param_space_cfg.get('learning_rate')
             trial_train_params['learning_rate'] = trial.suggest_float('learning_rate', lr_range[0], lr_range[1], log=True) if lr_range and len(lr_range)==2 else trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
             bs_choices = param_space_cfg.get('batch_size')
             trial_train_params['batch_size'] = trial.suggest_categorical('batch_size', bs_choices) if bs_choices else 64
             opt_choices = param_space_cfg.get('optimizer')
             trial_train_params['optimizer'] = trial.suggest_categorical('optimizer', opt_choices) if opt_choices else 'adam'

             # Create temporary config for this trial by overlaying suggestions
             # Important: Create deep copies to avoid modifying original config
             trial_model_config = joblib.load(joblib.dump(self.model_config, 'temp_config.joblib')) # Deep copy via dump/load hack or use copy.deepcopy
             trial_model_config['architecture'].update(trial_arch_params)
             trial_model_config['training'].update(trial_train_params)
             # Use limited epochs/patience for HPO trial
             trial_model_config['training']['epochs'] = hpo_epochs
             trial_model_config['training']['patience'] = hpo_patience

             # Create a temporary config object for the trial model instance
             trial_full_config = joblib.load(joblib.dump(self.config, 'temp_config.joblib'))
             trial_full_config['models'][self.model_name] = trial_model_config # Update model section
             os.remove('temp_config.joblib') # Clean up temporary file


             logger.debug(f"Trial {trial.number}: Params: {trial.params}")

             # Instantiate and train a temporary model for this trial
             try:
                 # --- Create DataLoaders INSIDE trial using pre-scaled data ---
                 X_train_tensor_trial = torch.tensor(X_train_scaled_hpo, dtype=torch.float32)
                 X_val_tensor_trial = torch.tensor(X_val_scaled_hpo, dtype=torch.float32)
                 train_dataset_trial = TensorDataset(X_train_tensor_trial, y_train_tensor_hpo)
                 val_dataset_trial = TensorDataset(X_val_tensor_trial, y_val_tensor_hpo)
                 trial_batch_size = trial_train_params['batch_size']
                 train_loader_trial = DataLoader(train_dataset_trial, batch_size=trial_batch_size, shuffle=True, drop_last=True)
                 val_loader_trial = DataLoader(val_dataset_trial, batch_size=trial_batch_size * 4, shuffle=False)
                 logger.debug(f"Trial {trial.number}: Created DataLoaders.")

                 # Instantiate model using trial config
                 temp_model = DrWiggleNN(len(self.feature_names_in_), self.num_classes, trial_model_config).to(self.device)
                 temp_criterion = self._get_loss_function(y_train.values) # Use original y_train for weights
                 temp_optimizer = self._get_optimizer(temp_model.parameters()) # Optimizer based on trial params

                 # --- Simplified Training Loop for HPO Trial ---
                 trial_history = {'val_loss': [], 'val_accuracy': []}
                 best_trial_metric = float('inf') if self._is_loss_objective() else float('-inf')
                 trial_patience_counter = 0

                 for epoch in range(hpo_epochs):
                     temp_model.train()
                     for batch_X, batch_y in train_loader_trial: # Use trial loader
                          batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                          temp_optimizer.zero_grad()
                          outputs = temp_model(batch_X)
                          if isinstance(temp_criterion, nn.MSELoss): loss = temp_criterion(outputs, batch_y.unsqueeze(-1))
                          else: loss = temp_criterion(outputs, batch_y)
                          loss.backward()
                          temp_optimizer.step()

                     # Simplified Validation
                     temp_model.eval()
                     epoch_val_loss = 0.0
                     correct_preds = 0
                     total_preds = 0
                     with torch.no_grad():
                          for batch_X_val, batch_y_val in val_loader_trial: # Indent Level 2
                               batch_X_val, batch_y_val = batch_X_val.to(self.device), batch_y_val.to(self.device)
                               outputs_val = temp_model(batch_X_val)
                               if isinstance(temp_criterion, nn.MSELoss): # Indent Level 3
                                    loss_val = temp_criterion(outputs_val, batch_y_val.unsqueeze(-1))
                                    preds = torch.round(outputs_val.squeeze()).clamp(0, self.num_classes - 1).long()
                               else: # Indent Level 3
                                    loss_val = temp_criterion(outputs_val, batch_y_val)
                                    _, preds = torch.max(outputs_val, 1)
                               # Correct indentation for these lines:
                               epoch_val_loss += loss_val.item() * batch_X_val.size(0) # <-- Must be Level 3
                               correct_preds += (preds == batch_y_val).sum().item() # <-- Must be Level 3
                               total_preds += batch_y_val.size(0) # <-- Must be Level 3

                     # This part should be at Indent Level 2
                     avg_val_loss = epoch_val_loss / len(val_loader_trial.dataset)
                     val_accuracy = correct_preds / total_preds if total_preds > 0 else 0.0
                     trial_history['val_loss'].append(avg_val_loss)
                     trial_history['val_accuracy'].append(val_accuracy)
                     current_trial_metric = avg_val_loss if self._is_loss_objective() else val_accuracy

                     # Pruning & Early stopping check within trial
                     trial.report(current_trial_metric, epoch)
                     if trial.should_prune():
                          logger.debug(f"Trial {trial.number} pruned at epoch {epoch+1}.")
                          raise optuna.TrialPruned()

                     # Check for improvement
                     improved = False
                     if np.isnan(current_trial_metric): continue # Skip comparison if NaN
                     if self._is_loss_objective():
                          if current_trial_metric < best_trial_metric: improved = True
                     else:
                          if current_trial_metric > best_trial_metric: improved = True

                     if improved:
                          best_trial_metric = current_trial_metric
                          trial_patience_counter = 0
                     else:
                          trial_patience_counter += 1

                     if trial_patience_counter >= hpo_patience:
                          logger.debug(f"Trial {trial.number} stopped early at epoch {epoch+1} due to patience.")
                          break # Stop trial early

                 # --- Return the best metric achieved in this trial ---
                 final_trial_result = best_trial_metric # Return the best metric found during the trial
                 logger.debug(f"Trial {trial.number}: Result ({objective_metric}) = {final_trial_result:.5f}")
                 return final_trial_result

             except optuna.TrialPruned:
                 raise # Re-raise prune signals for Optuna
             except Exception as e:
                  logger.warning(f"Trial {trial.number} failed with error: {e}. Returning worst possible score.", exc_info=True)
                  # Return worst score depending on optimization direction
                  return float('inf') if self._is_loss_objective() else float('-inf')


         # --- Run Optuna Study ---
         study_direction = 'minimize' if self._is_loss_objective() else 'maximize'
         study = optuna.create_study(direction=study_direction)
         optimize_kwargs = {'n_trials': n_trials}
         if _optuna_supports_progbar: optimize_kwargs['show_progress_bar'] = True
         study.optimize(objective, **optimize_kwargs)

         best_params = study.best_params
         best_value = study.best_value
         logger.info(f"Optuna HPO complete. Best validation {objective_metric}: {best_value:.5f}")
         logger.info(f"Best parameters found: {best_params}")

         # Return the best parameters found
         return best_params

--- End File: drwiggle/models/neural_network.py ---

--- File: drwiggle/models/random_forest.py ---
---------------------------------------
import logging
import os
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier as SklearnRF
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.class_weight import compute_class_weight
from sklearn.exceptions import NotFittedError

from .base import BaseClassifier
from drwiggle.utils.helpers import progress_bar, save_object, load_object # Use helpers for save/load

logger = logging.getLogger(__name__)

class RandomForestClassifier(BaseClassifier):
    """
    Random Forest Classifier for protein flexibility, integrating with BaseClassifier.
    Handles hyperparameter optimization via RandomizedSearchCV if configured.
    """

    def fit(self, X: pd.DataFrame, y: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """
        Train the Random Forest classifier. Handles HPO if enabled.

        Args:
            X: DataFrame of features.
            y: Series of target labels.
            X_val: Validation features (currently unused by RF fit/HPO but kept for interface consistency).
            y_val: Validation labels (currently unused).
        """
        self.feature_names_in_ = X.columns.tolist() # Store feature names

        # Determine if HPO is enabled for RF
        hpo_cfg = self.model_config.get('randomized_search', {})
        hpo_enabled = hpo_cfg.get('enabled', False)

        best_params_found = None

        if hpo_enabled:
            logger.info(f"Starting hyperparameter optimization for {self.model_name} using RandomizedSearchCV...")
            try:
                 # Run HPO - this method now returns the best params but doesn't fit the final model yet
                 best_params_found = self.hyperparameter_optimize(X, y, X_val, y_val)
                 # Update model_config with best parameters found for final fit
                 logger.info(f"Updating model config with best HPO params: {best_params_found}")
                 for key, value in best_params_found.items():
                     # Need to place params correctly in config structure (e.g., n_estimators directly)
                      if key in self.model_config:
                          self.model_config[key] = value
                      else:
                           # Handle nested params if necessary, though RF params are usually top-level
                           logger.warning(f"Best HPO param '{key}' not directly in model_config, check structure.")

            except (NotImplementedError, ValueError) as e:
                 logger.error(f"Hyperparameter optimization failed: {e}. Falling back to default parameters.")
                 hpo_enabled = False # Disable HPO for the final fit stage


        # --- Final Model Training ---
        # Use either default params or best params found from HPO
        logger.info(f"Fitting final {self.model_name} model...")
        final_params = {
            'n_estimators': self.model_config.get('n_estimators', 100),
            'max_depth': self.model_config.get('max_depth', None),
            'min_samples_split': self.model_config.get('min_samples_split', 2),
            'min_samples_leaf': self.model_config.get('min_samples_leaf', 1),
            'class_weight': self.model_config.get('class_weight', 'balanced'),
            'random_state': self.config.get('system', {}).get('random_state', 42),
            'n_jobs': self.config.get('system', {}).get('n_jobs', -1),
            'oob_score': True # Useful for assessing performance without a separate val set during fit
        }
        logger.debug(f"Final RF training parameters: {final_params}")

        self.model = SklearnRF(**final_params)

        try:
            # Ensure y has the correct integer type expected by sklearn
            y_train_values = y.astype(int).values
            self.model.fit(X.values, y_train_values) # Sklearn models typically work best with numpy arrays
            logger.info(f"Final {self.model_name} training complete. OOB Score: {self.model.oob_score_:.4f}")
            self._fitted = True
        except Exception as e:
            logger.error(f"Failed to train final {self.model_name} model: {e}", exc_info=True)
            self._fitted = False
            raise # Re-raise the exception

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class predictions."""
        super().predict(X) # Check if fitted and features match
        try:
            # Use .values to ensure numpy array input for sklearn model
            predictions = self.model.predict(X[self.feature_names_in_].values)
            return predictions
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict called but underlying sklearn model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} prediction: {e}", exc_info=True)
            raise

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class probabilities."""
        super().predict_proba(X) # Check if fitted and features match
        try:
            if not hasattr(self.model, 'predict_proba'):
                raise NotImplementedError(f"The underlying {self.model.__class__.__name__} model does not support probability prediction.")
            # Use .values to ensure numpy array input
            probabilities = self.model.predict_proba(X[self.feature_names_in_].values)
            return probabilities
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict_proba called but underlying sklearn model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} probability prediction: {e}", exc_info=True)
            raise

    def save(self, path: str):
        """Save the trained model state using joblib via helper."""
        super().save(path) # Creates dir, checks fitted state
        state = {
            'model': self.model,
            'feature_names_in_': self.feature_names_in_,
            'config': self.config, # Save full config used during training instance creation
            'model_config': self.model_config, # Save specific model config state at time of saving
            'model_name': self.model_name,
            'fitted': self._fitted
        }
        save_object(state, path) # Use helper

    @classmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'RandomForestClassifier':
        """Load a trained model state using joblib via helper."""
        # Base class load method only checks file existence in this implementation
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found at {path}")
        logger.info(f"Loading model '{cls.__name__}' from {path}...")
        state = load_object(path) # Use helper

        # Validate loaded state (basic checks)
        required_keys = ['model', 'feature_names_in_', 'model_config', 'model_name', 'fitted']
        if not all(key in state for key in required_keys):
             raise ValueError(f"Loaded model state from {path} is missing required keys. Found: {list(state.keys())}")

        # Use loaded config if available, otherwise fall back to provided runtime config
        load_config = state.get('config', config)
        if load_config is None:
             raise ValueError("Cannot load model: No configuration found in saved state or provided at runtime.")

        # Re-instantiate the class
        instance = cls(config=load_config, model_name=state['model_name'])

        # Restore state
        instance.model = state['model']
        instance.feature_names_in_ = state['feature_names_in_']
        instance._fitted = state['fitted']
        # Optionally restore model_config from state if needed, but typically re-derived from main config
        # instance.model_config = state['model_config']

        if not instance._fitted:
             logger.warning(f"Loaded model '{instance.model_name}' from {path} indicates it was not fitted.")
        if not isinstance(instance.model, SklearnRF):
             raise TypeError(f"Loaded model is not a scikit-learn RandomForestClassifier instance (got {type(instance.model)}).")

        logger.info(f"Model '{instance.model_name}' loaded successfully.")
        return instance

    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """Get feature importance values from the trained RF model."""
        if not self._fitted or not hasattr(self.model, 'feature_importances_'):
            logger.warning(f"Cannot get feature importance for '{self.model_name}': Model not fitted or importances not available.")
            return None

        importances = self.model.feature_importances_

        if self.feature_names_in_ and len(self.feature_names_in_) == len(importances):
            # Create dict mapping names to scores
            importance_dict = dict(zip(self.feature_names_in_, importances))
            # Sort by importance (descending)
            sorted_importances = dict(sorted(importance_dict.items(), key=lambda item: item[1], reverse=True))
            return sorted_importances
        else:
            logger.warning(f"Feature names ({len(self.feature_names_in_) if self.feature_names_in_ else 'None'}) "
                           f"mismatch importance values ({len(importances)}) for model '{self.model_name}'. Returning indexed importances.")
            # Fallback to indexed importances if names mismatch
            return {f"feature_{i}": imp for i, imp in enumerate(importances)}

    def hyperparameter_optimize(self, X_train, y_train, X_val, y_val) -> Dict[str, Any]:
        """
        Performs Hyperparameter Optimization using RandomizedSearchCV.

        Note: Validation data (X_val, y_val) is not directly used by RandomizedSearchCV's
        internal cross-validation, but the method signature requires them for consistency
        with the base class.

        Returns:
            Dictionary containing the best hyperparameters found.
        """
        hpo_cfg = self.model_config.get('randomized_search')
        if not hpo_cfg or not hpo_cfg.get('enabled', False):
             raise NotImplementedError(f"RandomizedSearch HPO not enabled or configured for model '{self.model_name}'.")

        param_dist = hpo_cfg.get('param_distributions')
        n_iter = hpo_cfg.get('n_iter', 20)
        cv = hpo_cfg.get('cv', 3)
        scoring = hpo_cfg.get('scoring', 'balanced_accuracy') # Default scoring metric

        if not param_dist:
            raise ValueError("Parameter distributions ('param_distributions') not defined in HPO config.")

        logger.info(f"Running RandomizedSearchCV for {self.model_name}: n_iter={n_iter}, cv={cv}, scoring='{scoring}'")
        logger.debug(f"Search space: {param_dist}")

        # Base estimator (parameters here are defaults, will be overridden by search)
        base_estimator = SklearnRF(
             random_state=self.config.get('system', {}).get('random_state', 42),
             n_jobs=1 # Use n_jobs in RandomizedSearchCV for parallelism across CV folds/iters
        )

        # Ensure y has the correct integer type
        y_train_values = y_train.astype(int).values

        search = RandomizedSearchCV(
            estimator=base_estimator,
            param_distributions=param_dist,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            random_state=self.config.get('system', {}).get('random_state', 42),
            n_jobs=self.config.get('system', {}).get('n_jobs', -1), # Parallelize search
            verbose=1 # Show some progress from RandomizedSearchCV
        )

        try:
            # Run the search on training data (uses internal CV)
            # Need .values as sklearn works best with numpy
            search.fit(X_train.values, y_train_values)

            logger.info(f"RandomizedSearch complete. Best score ({scoring}): {search.best_score_:.4f}")
            logger.info(f"Best parameters found: {search.best_params_}")

            # This method *only* finds the best params, it doesn't fit the final model here.
            # The calling `fit` method will use these params to train the final model.
            return search.best_params_

        except Exception as e:
            logger.error(f"RandomizedSearchCV failed for {self.model_name}: {e}", exc_info=True)
            raise ValueError(f"Hyperparameter optimization failed during RandomizedSearch.") from e

--- End File: drwiggle/models/random_forest.py ---

--- File: drwiggle/models/xgboost.py ---
---------------------------------------
# --- File: drwiggle/models/xgboost_model.py ---
import logging
import os
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import pandas as pd
import joblib
import xgboost as xgb # Import XGBoost
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.class_weight import compute_sample_weight # Use sample_weight for XGBoost
from sklearn.exceptions import NotFittedError
from sklearn.preprocessing import LabelEncoder # Needed if labels aren't 0-indexed

from .base import BaseClassifier
from drwiggle.utils.helpers import progress_bar, save_object, load_object # Use helpers for save/load

logger = logging.getLogger(__name__)

class XGBoostClassifier(BaseClassifier):
    """
    XGBoost Classifier for protein flexibility, integrating with BaseClassifier.
    Handles hyperparameter optimization via RandomizedSearchCV if configured.
    """

    def __init__(self, config: Dict[str, Any], model_name: str):
        super().__init__(config, model_name)
        # Ensure num_classes is set for XGBoost objective
        self.num_classes = self.config.get('binning', {}).get('num_classes', 5)
        if self.num_classes <= 0:
            raise ValueError("Number of classes must be positive for XGBoost multi-class classification.")
        logger.debug(f"XGBoostClassifier initialized for {self.num_classes} classes.")

    def fit(self, X: pd.DataFrame, y: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """
        Train the XGBoost classifier. Handles HPO if enabled and uses early stopping if configured.
        (Version compatible with direct early_stopping_rounds parameter)
        """
        self.feature_names_in_ = X.columns.tolist() # Store feature names

        # --- Data Preparation ---
        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y.astype(int))
        if not np.array_equal(le.classes_, np.arange(self.num_classes)):
             logger.warning(f"Labels were re-encoded by LabelEncoder. Original unique labels: {np.unique(y.values)}, Encoded classes: {le.classes_}. Ensure labels match 0 to num_classes-1.")
        # Store label encoder if needed for inverse transform later
        self.label_encoder_ = le

        # Prepare validation set and early stopping parameters if applicable
        eval_set_list = None
        early_stopping_rounds_config = None
        verbose_eval = False # Default verbosity for fit method

        if X_val is not None and y_val is not None:
            y_val_encoded = self.label_encoder_.transform(y_val.astype(int))
            # XGBoost eval_set needs to be a list of tuples: [(X_val, y_val)]
            # Use the feature names stored in self.feature_names_in_
            eval_set_list = [(X_val[self.feature_names_in_].values, y_val_encoded)]
            logger.info(f"Validation set prepared for early stopping. Shape: {X_val.shape}")

            early_stopping_rounds_config = self.model_config.get('training', {}).get('early_stopping_rounds', None)
            if early_stopping_rounds_config and early_stopping_rounds_config > 0:
                 # Get verbosity setting from config ONLY if early stopping is active
                 verbose_eval = self.model_config.get('training', {}).get('verbose', False)
                 logger.info(f"Using early stopping parameter with rounds={early_stopping_rounds_config}")
            else:
                 # Ensure it's None if not valid, so it's not passed to fit
                 early_stopping_rounds_config = None
                 logger.info("Early stopping rounds not configured or set to zero.")
        else:
            logger.warning("No validation set provided for XGBoost. Early stopping disabled.")

        # Handle class weights using sample_weight
        sample_weights = None
        use_weights = self.model_config.get('use_sample_weights', True)
        if use_weights:
            logger.debug("Calculating sample weights for imbalanced classes.")
            try:
                sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_encoded)
            except Exception as e:
                 logger.warning(f"Could not compute sample weights: {e}. Proceeding without weights.")

        # --- HPO Phase ---
        hpo_cfg = self.model_config.get('randomized_search', {})
        # Check config if HPO should run (make sure the 'enabled' flag is false if you don't want it)
        hpo_enabled = hpo_cfg.get('enabled', False)

        if hpo_enabled:
            logger.info(f"Starting hyperparameter optimization for {self.model_name} using RandomizedSearchCV...")
            try:
                best_params_found = self.hyperparameter_optimize(X, y, X_val, y_val)
                logger.info(f"Updating model config with best HPO params: {best_params_found}")
                self.model_config.update(best_params_found) # Update top-level params
            except (NotImplementedError, ValueError) as e:
                logger.error(f"Hyperparameter optimization failed: {e}. Falling back to default parameters.")


        # --- Final Model Training ---
        logger.info(f"Fitting final {self.model_name} model...")

        # Extract parameters from config (potentially updated by HPO)
        xgb_params = {
            'objective': self.model_config.get('objective', 'multi:softprob'),
            'eval_metric': self.model_config.get('eval_metric', 'mlogloss'),
            'n_estimators': self.model_config.get('n_estimators', 100),
            'learning_rate': self.model_config.get('learning_rate', 0.1),
            'max_depth': self.model_config.get('max_depth', 6),
            'subsample': self.model_config.get('subsample', 0.8),
            'colsample_bytree': self.model_config.get('colsample_bytree', 0.8),
            'gamma': self.model_config.get('gamma', 0),
            'reg_alpha': self.model_config.get('reg_alpha', 0),
            'reg_lambda': self.model_config.get('reg_lambda', 1),
            'use_label_encoder': False, # Recommended False for modern XGBoost constructor
            'seed': self.config.get('system', {}).get('random_state', 42),
            'n_jobs': self.config.get('system', {}).get('n_jobs', -1),
            'num_class': self.num_classes
        }
        logger.debug(f"Final XGBoost training parameters: {xgb_params}")

        self.model = xgb.XGBClassifier(**xgb_params)

        try:
            # --- CORRECTED FIT CALL ---
            # Pass early stopping related args directly IF eval_set_list is not None
            if eval_set_list and early_stopping_rounds_config:
                 # Call fit with explicit early stopping arguments
                 self.model.fit(
                     X.values,
                     y_train_encoded,
                     sample_weight=sample_weights,
                     eval_set=eval_set_list,
                     early_stopping_rounds=early_stopping_rounds_config, # Pass value directly
                     verbose=verbose_eval
                 )
            else:
                 # Fit without early stopping args if no validation set or rounds not set
                 self.model.fit(
                     X.values,
                     y_train_encoded,
                     sample_weight=sample_weights,
                     verbose=verbose_eval # Still control general verbosity
                 )
            # --- END CORRECTED FIT CALL ---

            logger.info(f"Final {self.model_name} training complete.")
            # Log best score if early stopping was used and the attribute exists
            if early_stopping_rounds_config and hasattr(self.model, 'best_score'):
                 try:
                      best_score_val = self.model.best_score
                      best_iter_val = self.model.best_iteration
                      logger.info(f"Best score ({xgb_params['eval_metric']}) during early stopping: {best_score_val:.4f} at iteration {best_iter_val}")
                 except AttributeError:
                      logger.info("Early stopping enabled, but best_score/best_iteration attribute not found or populated.")

            self._fitted = True
        except Exception as e:
            logger.error(f"Failed to train final {self.model_name} model: {e}", exc_info=True)
            self._fitted = False
            raise # Re-raise the exception

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class predictions."""
        super().predict(X) # Check if fitted and features match
        try:
            # Predict returns encoded labels (0 to num_classes-1)
            encoded_predictions = self.model.predict(X[self.feature_names_in_].values)
            # Assuming pipeline works with 0-indexed classes directly:
            return encoded_predictions
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict called but underlying XGBoost model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} prediction: {e}", exc_info=True)
            raise

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Generate class probabilities."""
        super().predict_proba(X) # Check if fitted and features match
        try:
            if not hasattr(self.model, 'predict_proba'):
                raise NotImplementedError(f"The underlying {self.model.__class__.__name__} model does not support probability prediction (check objective).")
            probabilities = self.model.predict_proba(X[self.feature_names_in_].values)
            return probabilities
        except NotFittedError:
             raise RuntimeError(f"Internal error: Model '{self.model_name}' predict_proba called but underlying XGBoost model is not fitted.")
        except Exception as e:
            logger.error(f"Error during {self.model_name} probability prediction: {e}", exc_info=True)
            raise

    def save(self, path: str):
        """Save the trained model state using joblib via helper."""
        super().save(path) # Creates dir, checks fitted state
        if not hasattr(self, 'label_encoder_'):
            logger.warning(f"Label encoder not found for model '{self.model_name}'. Saving without it.")
            label_encoder_state = None
        else:
            label_encoder_state = self.label_encoder_

        state = {
            'model': self.model, # XGBoost model object itself
            'label_encoder_': label_encoder_state,
            'feature_names_in_': self.feature_names_in_,
            'config': self.config,
            'model_config': self.model_config,
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'fitted': self._fitted
        }
        save_object(state, path) # Use helper

    @classmethod
    def load(cls, path: str, config: Optional[Dict[str, Any]] = None) -> 'XGBoostClassifier':
        """Load a trained model state using joblib via helper."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found at {path}")
        logger.info(f"Loading model '{cls.__name__}' from {path}...")
        state = load_object(path) # Use helper

        required_keys = ['model', 'feature_names_in_', 'model_config', 'model_name', 'fitted', 'num_classes']
        if not all(key in state for key in required_keys):
             raise ValueError(f"Loaded model state from {path} is missing required keys. Found: {list(state.keys())}")

        load_config = state.get('config', config)
        if load_config is None:
             raise ValueError("Cannot load model: No configuration found in saved state or provided at runtime.")

        instance = cls(config=load_config, model_name=state['model_name'])

        # Restore state
        instance.model = state['model']
        instance.label_encoder_ = state.get('label_encoder_')
        instance.feature_names_in_ = state['feature_names_in_']
        instance.num_classes = state['num_classes']
        instance._fitted = state['fitted']
        # instance.model_config = state['model_config'] # Optionally restore exact config

        if not instance._fitted:
             logger.warning(f"Loaded model '{instance.model_name}' from {path} indicates it was not fitted.")
        if not isinstance(instance.model, xgb.XGBClassifier):
             raise TypeError(f"Loaded model is not an XGBoostClassifier instance (got {type(instance.model)}).")

        logger.info(f"Model '{instance.model_name}' loaded successfully.")
        return instance

    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """Get feature importance values from the trained XGBoost model."""
        if not self._fitted or not hasattr(self.model, 'feature_importances_'):
            logger.warning(f"Cannot get feature importance for '{self.model_name}': Model not fitted or importances not available.")
            return None

        importances = self.model.feature_importances_

        if self.feature_names_in_ and len(self.feature_names_in_) == len(importances):
            importance_dict = dict(zip(self.feature_names_in_, importances))
            sorted_importances = dict(sorted(importance_dict.items(), key=lambda item: item[1], reverse=True))
            return sorted_importances
        else:
            logger.warning(f"Feature names mismatch importance values for '{self.model_name}'. Returning indexed importances.")
            return {f"feature_{i}": imp for i, imp in enumerate(importances)}

    def hyperparameter_optimize(self, X_train, y_train, X_val, y_val) -> Dict[str, Any]:
        """
        Performs Hyperparameter Optimization using RandomizedSearchCV for XGBoost.
        """
        hpo_cfg = self.model_config.get('randomized_search')
        if not hpo_cfg or not hpo_cfg.get('enabled', False):
             raise NotImplementedError(f"RandomizedSearch HPO not enabled or configured for model '{self.model_name}'.")

        param_dist = hpo_cfg.get('param_distributions')
        n_iter = hpo_cfg.get('n_iter', 20)
        cv = hpo_cfg.get('cv', 3)
        scoring = hpo_cfg.get('scoring', 'balanced_accuracy') # Appropriate scoring

        if not param_dist:
            raise ValueError("Parameter distributions ('param_distributions') not defined in HPO config.")

        logger.info(f"Running RandomizedSearchCV for {self.model_name}: n_iter={n_iter}, cv={cv}, scoring='{scoring}'")
        logger.debug(f"Search space: {param_dist}")

        # Base XGBoost estimator for search - use minimal settings here
        base_estimator = xgb.XGBClassifier(
             objective='multi:softprob', # Consistent objective
             eval_metric='mlogloss',     # Consistent metric
             use_label_encoder=False,    # Remove this parameter entirely if it causes warnings
             seed=self.config.get('system', {}).get('random_state', 42),
             n_jobs=1, # Let RandomizedSearchCV handle parallelization
             num_class=self.num_classes
        )

        # Encode labels for RandomizedSearchCV internal fitting
        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y_train.astype(int))

        # Calculate sample weights for CV folds if desired
        fit_params_cv = {} # Parameters passed to fit within CV
        use_weights_hpo = self.model_config.get('use_sample_weights', True)
        if use_weights_hpo:
             sample_weights_hpo = compute_sample_weight(class_weight='balanced', y=y_train_encoded)
             fit_params_cv['sample_weight'] = sample_weights_hpo
             logger.info("Using sample weights during HPO cross-validation.")

        search = RandomizedSearchCV(
            estimator=base_estimator,
            param_distributions=param_dist,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            random_state=self.config.get('system', {}).get('random_state', 42),
            n_jobs=self.config.get('system', {}).get('n_jobs', -1),
            verbose=1,
            error_score='raise' # Catch errors during search
        )

        try:
            # Run the search on training data, passing fit_params to the estimator within CV
            search.fit(X_train.values, y_train_encoded, **fit_params_cv)

            logger.info(f"RandomizedSearch complete. Best score ({scoring}): {search.best_score_:.4f}")
            logger.info(f"Best parameters found: {search.best_params_}")

            # Return only the best hyperparameters
            return search.best_params_

        except Exception as e:
            logger.error(f"RandomizedSearchCV failed for {self.model_name}: {e}", exc_info=True)
            raise ValueError(f"Hyperparameter optimization failed during RandomizedSearch.") from e
# --- End File: drwiggle/models/xgboost_model.py ---

--- End File: drwiggle/models/xgboost.py ---

--- File: drwiggle/pipeline.py ---
---------------------------------------
import logging
import os
import time
import traceback
from typing import Dict, Any, Optional, List, Union, Tuple
import sys
import glob      # <<<--- Ensure this import is present

import pandas as pd
import numpy as np
import joblib # For saving/loading binners/models (used by helpers)

# Local Imports
from drwiggle.config import (
    load_config, get_model_config, get_enabled_features, get_class_names,
    is_pdb_enabled, get_binning_config, get_pdb_config, get_visualization_colors,
    get_split_config, get_system_config, get_pdb_feature_config
)
from drwiggle.data.loader import load_data, find_data_file, load_rmsf_data
from drwiggle.data.processor import process_features, split_data, prepare_data_for_model
from drwiggle.data.binning import get_binner, BaseBinner
# <<<--- Ensure get_model_class is imported --- >>>
from drwiggle.models import get_model_instance, get_enabled_models, BaseClassifier, get_model_class
from drwiggle.utils.metrics import evaluate_classification, generate_confusion_matrix_df, generate_classification_report_dict
from drwiggle.utils.visualization import (
    plot_bin_distribution, plot_confusion_matrix, plot_feature_importance,
    plot_class_distribution, plot_metric_vs_temperature, _plotting_available
    # plot_protein_visualization_data # Placeholder for PDB vis trigger
)
from drwiggle.utils.pdb_tools import parse_pdb, extract_pdb_features, generate_pymol_script, color_pdb_by_flexibility
from drwiggle.temperature.comparison import run_temperature_comparison_analysis # Avoid name clash
from drwiggle.utils.helpers import ensure_dir, save_object, load_object, timer # Use helpers

logger = logging.getLogger(__name__)

class Pipeline:
    """
    Main pipeline orchestrator for the drWiggle workflow.

    Handles data loading, processing, binning, model training, evaluation,
    prediction, and visualization based on the provided configuration.
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the pipeline with a validated and resolved configuration.

        Args:
            config: The configuration dictionary, assumed to be loaded, merged,
                    templated, and path-resolved via `drwiggle.config.load_config`.
        """
        self.config = config
        self.binner: Optional[BaseBinner] = None
        # Store trained models: {model_name: model_instance}
        self.models: Dict[str, BaseClassifier] = {}
        # Store feature names used during training for consistency
        self.feature_names_in_: Optional[List[str]] = None
        self._prepare_directories()
        logger.info("drWiggle Pipeline initialized.")
        # Avoid logging full config at INFO level if it contains sensitive info or is too large
        # logger.debug(f"Pipeline using resolved config: {self.config}")


    def _prepare_directories(self):
        """Create output directories defined in the config if they don't exist."""
        logger.debug("Ensuring required directories exist...")
        paths_config = self.config.get('paths', {})
        required_dirs = ['output_dir', 'models_dir'] # Minimum required dirs
        if is_pdb_enabled(self.config): required_dirs.append('pdb_cache_dir')

        for key in required_dirs:
            path = paths_config.get(key)
            if path:
                try:
                    ensure_dir(path) # Use helper to create dir
                except Exception as e:
                    logger.error(f"Failed to create or access directory '{key}': {path}. Error: {e}")
                    # Decide whether to raise or just warn
                    raise # Critical failure if output dirs cannot be created
            else:
                logger.error(f"Required directory path '{key}' not found in configuration paths: {paths_config}")
                raise ValueError(f"Missing required directory path in config: paths.{key}")


    @timer # Time this method
    def load_and_process_data(self, data_path_or_pattern: Optional[str] = None) -> pd.DataFrame:
        """Loads data using load_data and processes features using process_features."""
        logger.info("--- Loading and Processing Data ---")
        data_dir = self.config.get('paths', {}).get('data_dir') # May be None if not specified
        file_pattern = self.config.get('dataset', {}).get('file_pattern') # Templated already

        # Determine input path: use specific path, pattern from config, or fail
        input_source = data_path_or_pattern
        if input_source is None:
            if not file_pattern:
                raise ValueError("No input data path/pattern provided via CLI (--input) and 'dataset.file_pattern' not set in config.")
            if not data_dir:
                 raise ValueError("Config 'paths.data_dir' must be set when using 'dataset.file_pattern' without explicit --input.")
            input_source = file_pattern # Use pattern from config
            logger.info(f"No explicit input path provided, using pattern from config: '{input_source}' in dir '{data_dir}'")
        else:
             logger.info(f"Using provided input path/pattern: '{input_source}'")
             # Check if data_dir is needed for relative path or pattern
             if not os.path.isabs(input_source) and ('*' in input_source or '?' in input_source or not os.path.dirname(input_source)):
                  if not data_dir: logger.warning(f"Input '{input_source}' looks like a pattern or relative filename, but 'paths.data_dir' is not set. Searching relative to CWD.")


        # Load data using the loader function
        try:
            df_raw = load_data(input_source, data_dir=data_dir)
        except FileNotFoundError as e:
            logger.error(f"Input data not found using source '{input_source}' and data_dir '{data_dir}': {e}")
            raise
        except ValueError as e:
             logger.error(f"Error determining input data source: {e}")
             raise

        # Process features using the processor module
        df_processed = process_features(df_raw, self.config)

        # Store feature names used for training (if this is the main data loading step)
        # This assumes process_features defines the final set of features.
        # We need X, y separated later, but store potential feature names now.
        # self.feature_names_in_ = df_processed.columns.tolist() # Store all columns for now? No, get from prepare_data

        logger.info(f"Data loading and processing finished. Shape: {df_processed.shape}")
        return df_processed


    @timer
    def calculate_and_apply_binning(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates RMSF bin boundaries and adds a 'flexibility_class' column.

        Args:
            df: DataFrame containing the target RMSF column (defined in config).

        Returns:
            DataFrame with the added 'flexibility_class' column.
        """
        logger.info("--- Calculating and Applying RMSF Binning ---")
        target_col = self.config['dataset']['target'] # Already templated
        target_col_actual = None
        # Try to find the target column case-insensitively
        target_col_lower = target_col.lower()
        for col in df.columns:
            if col.lower() == target_col_lower:
                target_col_actual = col
                break
        if not target_col_actual:
            raise ValueError(f"Target RMSF column matching '{target_col}' not found in DataFrame columns: {df.columns.tolist()}")
        if target_col_actual != target_col:
             logger.warning(f"Using case-insensitive match for target column: '{target_col_actual}' (requested '{target_col}')")

        if df[target_col_actual].isnull().any():
             nan_count = df[target_col_actual].isnull().sum()
             logger.warning(f"Target RMSF column '{target_col_actual}' contains {nan_count} NaN values. These rows will likely be dropped or cause errors during binning/training.")
             # Optionally drop NaNs here? Or let binning handle it? Let binning handle for now.
             # df = df.dropna(subset=[target_col_actual])

        rmsf_values = df[target_col_actual].values

        # Get binner instance based on config
        self.binner = get_binner(self.config) # Factory function gets appropriate binner
        logger.info(f"Using binner type: {self.binner.__class__.__name__}")

        # Fit binner and transform data
        start_time_bin = time.time()
        try:
             class_labels = self.binner.fit_transform(rmsf_values)
        except Exception as e:
             logger.error(f"Error during binning process: {e}", exc_info=True)
             raise # Re-raise error after logging

        df_binned = df.copy()
        df_binned['flexibility_class'] = class_labels

        logger.info(f"Binning complete in {time.time() - start_time_bin:.2f} seconds.")
        boundaries = self.binner.get_boundaries()
        logger.info(f"Calculated bin boundaries: {[f'{b:.3f}' for b in boundaries] if boundaries else 'N/A'}")

        # Plot distribution if enabled and plotting available
        if _plotting_available and self.config.get('visualization', {}).get('plots', {}).get('bin_distribution', False):
             dist_plot_path = os.path.join(self.config['paths']['output_dir'], 'rmsf_distribution_with_bins.png')
             try:
                 plot_bin_distribution(rmsf_values, boundaries, self.config, dist_plot_path)
             except Exception as e:
                 logger.warning(f"Failed to plot RMSF distribution with bins: {e}", exc_info=True)

        # Save binner if configured
        if self.config['binning'].get('store_boundaries', True):
            binner_path = os.path.join(self.config['paths']['models_dir'], 'binner.joblib')
            try:
                self.binner.save(binner_path)
            except Exception as e:
                 logger.error(f"Failed to save binner state to {binner_path}: {e}", exc_info=True)
                 # Decide whether to raise error or just warn
                 # raise IOError(f"Failed to save binner state") from e

        return df_binned

    @timer
    def train(self, model_names: Optional[List[str]] = None, data_path: Optional[str] = None):
        """
        Full training pipeline: Load data -> Process -> Bin -> Split -> Train Models -> Save Models.

        Args:
            model_names: Optional list of specific model names (from config) to train.
                         If None, trains all enabled models.
            data_path: Optional path/pattern to specific input data file. Overrides config pattern.
        """
        logger.info("====== Starting Training Pipeline ======")
        # 1. Load and Process Data
        try:
            df_processed = self.load_and_process_data(data_path)
        except (FileNotFoundError, ValueError) as e:
             logger.error(f"Failed to load or process data: {e}")
             sys.exit(1) # Exit if data loading fails

        # 2. Calculate and Apply Binning (Fit binner on full processed dataset)
        try:
            df_binned = self.calculate_and_apply_binning(df_processed)
        except (ValueError, RuntimeError) as e:
             logger.error(f"Failed during RMSF binning: {e}")
             sys.exit(1)

        # 3. Split Data (Train/Validation/Test)
        try:
            train_df, val_df, test_df = split_data(df_binned, self.config)
        except ValueError as e:
             logger.error(f"Failed to split data: {e}")
             sys.exit(1)

        # 4. Prepare Data for Models (extract features and target)
        # Do this *after* splitting to avoid data leakage if scaling depends on data
        try:
            # Use the target 'flexibility_class' created by binning
            X_train, y_train, self.feature_names_in_ = prepare_data_for_model(train_df, self.config, target_col='flexibility_class')
            X_val, y_val, _ = prepare_data_for_model(val_df, self.config, target_col='flexibility_class', features=self.feature_names_in_)
            X_test, y_test, _ = prepare_data_for_model(test_df, self.config, target_col='flexibility_class', features=self.feature_names_in_)
        except (ValueError, KeyError) as e:
             logger.error(f"Failed to prepare data arrays for models: {e}", exc_info=True)
             sys.exit(1)

        logger.info(f"Data shapes: Train X={X_train.shape}, y={y_train.shape} | Val X={X_val.shape}, y={y_val.shape} | Test X={X_test.shape}, y={y_test.shape}")
        logger.info(f"Using {len(self.feature_names_in_)} features: {', '.join(self.feature_names_in_[:15])}{'...' if len(self.feature_names_in_)>15 else ''}")

        # 5. Determine Models to Train
        if not model_names:
            enabled_model_info = get_enabled_models(self.config)
            model_names = list(enabled_model_info.keys())
            if not model_names:
                 logger.error("No models are enabled in the configuration. Nothing to train.")
                 return # Or sys.exit(1) ?

        logger.info(f"Models selected for training: {model_names}")

        # 6. Train Each Selected Model
        training_successful = {} # Track success per model
        for model_name in model_names:
            logger.info(f"--- Training Model: {model_name} ---")
            start_time_model = time.time()
            try:
                # Get model instance (handles config extraction)
                model = get_model_instance(self.config, model_name)

                # Fit the model (HPO happens inside model.fit if enabled)
                model.fit(X_train, y_train, X_val, y_val) # Pass DataFrames/Series

                if model._fitted:
                     self.models[model_name] = model # Store fitted model instance
                     training_successful[model_name] = True
                     logger.info(f"Training {model_name} complete in {time.time() - start_time_model:.2f} seconds.")

                     # Save model immediately after successful training
                     model_path = os.path.join(self.config['paths']['models_dir'], f'{model_name}.joblib')
                     model.save(model_path) # Model's save method handles logging

                     # Plot feature importance if available and configured
                     if _plotting_available and self.config.get('visualization', {}).get('plots', {}).get('feature_importance', False):
                         try:
                             importances = model.get_feature_importance()
                             if importances:
                                  fi_plot_path = os.path.join(self.config['paths']['output_dir'], f'{model_name}_feature_importance.png')
                                  plot_feature_importance(importances, fi_plot_path, model_name=model_name) # Pass dict directly
                         except Exception as e:
                             logger.warning(f"Failed to plot feature importance for {model_name}: {e}", exc_info=True)
                else:
                     logger.error(f"Model {model_name} fitting reported as unsuccessful (model._fitted is False).")
                     training_successful[model_name] = False

            except NotImplementedError as e: # Catch HPO not implemented etc.
                logger.error(f"Configuration error for model {model_name}: {e}")
                training_successful[model_name] = False
            except Exception as e:
                logger.error(f"Critical error during training of model {model_name}: {e}", exc_info=True)
                logger.error(f"Traceback: {traceback.format_exc()}")
                training_successful[model_name] = False
                # Optionally continue to next model or exit? Continuing for now.

        logger.info("--- Training Phase Summary ---")
        for name, success in training_successful.items():
             logger.info(f"Model '{name}': {'Successfully Trained' if success else 'Training FAILED'}")
        logger.info("====== Training Pipeline Finished ======")


    @timer
    def evaluate(self, model_names: Optional[List[str]] = None, data_path: Optional[str] = None):
        """
        Evaluate trained models on the test set.

        Loads data (if needed), loads models, performs prediction, calculates metrics,
        and saves results/plots.

        Args:
            model_names: Optional list of model names to evaluate. If None, tries to find
                         and evaluate all models saved in the models directory.
            data_path: Optional path/pattern to specific data file for evaluation.
                       If None, uses the test split derived from the main data source.
        """
        logger.info("====== Starting Evaluation Pipeline ======")

        # 1. Determine Models to Evaluate
        models_dir = self.config['paths']['models_dir']
        if not model_names:
            # Find saved models in the directory (looking for .joblib as primary save format)
            found_files = glob.glob(os.path.join(models_dir, "*.joblib")) # Use glob here
            # Exclude binner file
            model_files = [f for f in found_files if not os.path.basename(f).startswith('binner')]
            # Exclude NN metadata file if naming convention is used
            model_files = [f for f in model_files if not os.path.basename(f).endswith('_meta.joblib')]
            if not model_files:
                 logger.error(f"No models specified and no primary '.joblib' model files found in {models_dir}. Cannot evaluate.")
                 return
            # Extract model names from filenames
            model_names = [os.path.splitext(os.path.basename(f))[0] for f in model_files]
            logger.info(f"No models specified via CLI. Found saved models to evaluate: {model_names}")
        else:
             logger.info(f"Evaluating specified models: {model_names}")

        # 2. Load Binner (required for consistent class labels if evaluating on external data)
        if not self.binner:
            binner_path = os.path.join(models_dir, 'binner.joblib')
            if os.path.exists(binner_path):
                try:
                    self.binner = BaseBinner.load(binner_path, config=self.config)
                    logger.info(f"Loaded binner ({self.binner.__class__.__name__}) from {binner_path}")
                except Exception as e:
                    logger.error(f"Failed to load required binner from {binner_path}: {e}", exc_info=True)
                    sys.exit(1)
            else:
                logger.error(f"Binner file not found at {binner_path}. Binner is required for evaluation. Run training first or ensure binner exists.")
                sys.exit(1)

        # 3. Load and Prepare Evaluation Data
        # If data_path is given, load and process that. Otherwise, need to regenerate test split.
        if data_path:
            logger.info(f"Evaluating using explicitly provided data: {data_path}")
            df_processed = self.load_and_process_data(data_path)
            # Apply binning using the loaded binner
            logger.info("Applying loaded binner to evaluation data...")
            rmsf_col = self.config['dataset']['target'] # Assumes target col name is consistent
            rmsf_col_actual = next((col for col in df_processed.columns if col.lower() == rmsf_col.lower()), None)
            if not rmsf_col_actual: raise ValueError(f"Target RMSF column '{rmsf_col}' not found in evaluation data.")
            df_processed['flexibility_class'] = self.binner.transform(df_processed[rmsf_col_actual].values)
            eval_df = df_processed # Use the entire processed & binned external dataset
        else:
            logger.info("Evaluating on the test split derived from the training data source.")
            # Need to reload, reprocess, re-bin, and re-split the original data
            # This ensures consistency but might be slow if data is large
            df_processed = self.load_and_process_data() # Load default data source
            df_binned = self.calculate_and_apply_binning(df_processed) # Re-bin (using loaded binner if available)
            _, _, eval_df = split_data(df_binned, self.config) # Get the test split
            logger.info(f"Using test split for evaluation ({len(eval_df)} rows).")

        if eval_df.empty:
             logger.error("Evaluation dataset is empty. Cannot proceed.")
             return

        # 4. Evaluate Each Model
        all_metrics = {}
        evaluation_results = [] # Store detailed results per model

        for model_name in model_names:
            logger.info(f"--- Evaluating Model: {model_name} ---")
            model = self.models.get(model_name) # Check if already in memory

            # Load model if not already loaded
            if not model:
                # Assume primary file is .joblib (holds RF object or NN metadata)
                model_path = os.path.join(models_dir, f'{model_name}.joblib')
                if os.path.exists(model_path):
                    try:
                        # <<<--- START CORRECTED MODEL LOADING --- >>>
                        # 1. Get the correct model class based on name
                        ModelClass = get_model_class(self.config, model_name) # Get e.g., RandomForestClassifier class type
                        if ModelClass is None:
                            # Log error and skip this model
                            logger.error(f"Could not determine model class for '{model_name}'. Skipping.")
                            continue # Go to the next model in the loop

                        # 2. Call the load method specific to that class
                        # This path (.joblib) works for RF and is the meta_path for NN
                        model = ModelClass.load(model_path, config=self.config) # Calls the correct .load()
                        # <<<--- END CORRECTED MODEL LOADING --- >>>

                        self.models[model_name] = model # Store loaded model
                        # Store feature names from loaded model if pipeline doesn't have them yet
                        # Access feature_names_in_ AFTER model is loaded
                        if model and self.feature_names_in_ is None and model.feature_names_in_:
                             self.feature_names_in_ = model.feature_names_in_
                             logger.info(f"Loaded feature names ({len(self.feature_names_in_)}) from model {model_name}.")
                    except Exception as e:
                        logger.error(f"Failed to load model {model_name} from {model_path}: {e}", exc_info=True)
                        continue # Skip evaluation for this model
                else:
                    logger.warning(f"Model {model_name} not found in memory or at {model_path}. Skipping evaluation.")
                    continue

            # Check if feature names are available (needed for data prep)
            if self.feature_names_in_ is None:
                 logger.error(f"Feature names required for evaluation of {model_name} are not available (neither from training run nor loaded model). Skipping.")
                 continue

            # Prepare evaluation data using the stored feature names
            try:
                X_eval, y_eval, _ = prepare_data_for_model(eval_df, self.config, target_col='flexibility_class', features=self.feature_names_in_)
            except (ValueError, KeyError) as e:
                 logger.error(f"Failed to prepare evaluation data for model {model_name}: {e}", exc_info=True)
                 continue

            if y_eval is None:
                logger.error(f"Target column 'flexibility_class' missing in evaluation data. Cannot evaluate {model_name}.")
                continue

            # Perform prediction
            try:
                y_pred = model.predict(X_eval)
                y_prob = None
                if hasattr(model, 'predict_proba'):
                    try:
                        y_prob = model.predict_proba(X_eval)
                    except NotImplementedError:
                         logger.debug(f"Model {model_name} does not support predict_proba.")
                    except Exception as e:
                         logger.warning(f"Error calling predict_proba for {model_name}: {e}")

            except Exception as e:
                logger.error(f"Error during prediction with model {model_name}: {e}", exc_info=True)
                continue # Skip metrics calculation if prediction fails

            # Calculate metrics
            metrics = evaluate_classification(y_eval, y_pred, self.config, y_prob, model_name)
            all_metrics[model_name] = metrics

            # Generate and save detailed reports/plots per model
            output_dir = self.config['paths']['output_dir']
            model_output_prefix = os.path.join(output_dir, f"{model_name}_eval")

            # Confusion Matrix
            if self.config.get("evaluation", {}).get("metrics", {}).get("confusion_matrix"):
                 cm_df = generate_confusion_matrix_df(y_eval, y_pred, config=self.config)
                 if cm_df is not None:
                     cm_path = f"{model_output_prefix}_confusion_matrix.csv"
                     cm_df.to_csv(cm_path)
                     logger.info(f"Confusion matrix saved to {cm_path}")
                     if _plotting_available and self.config.get('visualization', {}).get('plots', {}).get('confusion_matrix'):
                          cm_plot_path = f"{model_output_prefix}_confusion_matrix.png"
                          plot_confusion_matrix(cm_df, cm_plot_path, normalize=True) # Plot normalized version

            # Classification Report
            if self.config.get("evaluation", {}).get("metrics", {}).get("classification_report"):
                 report_dict = generate_classification_report_dict(y_eval, y_pred, config=self.config)
                 if report_dict:
                      report_path = f"{model_output_prefix}_classification_report.json"
                      save_object(report_dict, report_path) # Save dict using helper

            # Save test predictions with actual labels
            pred_df = pd.DataFrame({
                 'actual_class': y_eval.values,
                 'predicted_class': y_pred
             })
            # Add identifiers back from eval_df if possible
            id_cols = [col for col in ['domain_id', 'chain_id', 'resid', 'icode'] if col in eval_df.columns]
            if id_cols:
                 pred_df = pd.concat([eval_df[id_cols].reset_index(drop=True), pred_df], axis=1)

            if y_prob is not None:
                 prob_cols = [f'prob_class_{i}' for i in range(y_prob.shape[1])]
                 prob_df = pd.DataFrame(y_prob, columns=prob_cols)
                 pred_df = pd.concat([pred_df, prob_df], axis=1)

            pred_path = os.path.join(output_dir, f'{model_name}_test_predictions.csv') # Save in main output dir
            pred_df.to_csv(pred_path, index=False, float_format='%.4f')
            logger.info(f"Test predictions saved to {pred_path}")

            # Append results for summary table
            metrics['model'] = model_name
            evaluation_results.append(metrics)


        # 7. Save Overall Metrics Summary
        if evaluation_results:
            summary_df = pd.DataFrame(evaluation_results)
            # Reorder columns to put 'model' first
            cols = ['model'] + [col for col in summary_df.columns if col != 'model']
            summary_df = summary_df[cols]
            summary_path = os.path.join(self.config['paths']['output_dir'], 'evaluation_summary.csv')
            summary_df.to_csv(summary_path, index=False, float_format='%.4f')
            logger.info(f"Evaluation summary saved to {summary_path}")
        else:
             logger.warning("No evaluation results generated to save in summary.")


        logger.info("====== Evaluation Pipeline Finished ======")
        return all_metrics


    @timer
    def predict(self, data: Union[str, pd.DataFrame], model_name: Optional[str] = None, output_path: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        Generate flexibility class predictions for new data using a specified or default model.

        Args:
            data: Input data as a file path/pattern or a pandas DataFrame.
                  Must contain the features required by the model.
            model_name: Name of the trained model to use (must exist in models_dir).
                        If None, attempts to load a default model (e.g., 'random_forest').
            output_path: Optional path to save the predictions CSV file. If None, returns DataFrame.

        Returns:
            DataFrame containing predictions (and probabilities if supported/enabled),
            or None if prediction fails or output_path is specified.
        """
        logger.info("====== Starting Prediction Pipeline ======")

        # 1. Load and Prepare Data
        if isinstance(data, (str, os.PathLike)):
             logger.info(f"Loading data for prediction from: {data}")
             # Assume data is already processed or contains necessary raw features
             df_input = self.load_and_process_data(data_path_or_pattern=str(data))
        elif isinstance(data, pd.DataFrame):
             logger.info("Using provided DataFrame for prediction.")
             # Assume DataFrame is already processed, or process it here?
             # For now, assume it contains necessary features directly.
             df_input = data.copy() # Work on a copy
        else:
            raise TypeError(f"Unsupported input data type for prediction: {type(data)}. Provide path or DataFrame.")

        if df_input.empty:
             logger.error("Input data for prediction is empty.")
             return None

        # 2. Load Model
        models_dir = self.config['paths']['models_dir']
        if not model_name:
            # Try loading a default model, e.g., RandomForest, if not specified
            default_model_to_try = "random_forest" # Make this configurable?
            model_name = default_model_to_try
            logger.warning(f"No model specified for prediction. Attempting to load default: '{model_name}'.")

        # Assume primary file is .joblib (holds RF object or NN metadata)
        model_path = os.path.join(models_dir, f"{model_name}.joblib")
        if not os.path.exists(model_path):
            logger.error(f"Model file for '{model_name}' not found at {model_path}. Cannot predict.")
            return None

        try:
            # Check if model already loaded in pipeline instance
            model = self.models.get(model_name)
            if not model:
                 # <<<--- START CORRECTED MODEL LOADING (Predict) --- >>>
                 # 1. Get the correct model class based on name
                 ModelClass = get_model_class(self.config, model_name) # Get e.g., RandomForestClassifier class type
                 if ModelClass is None:
                      # Log error and exit
                      logger.error(f"Could not determine model class for '{model_name}'. Cannot predict.")
                      return None # Exit prediction

                 # 2. Call the load method specific to that class
                 model = ModelClass.load(model_path, config=self.config) # Calls the correct .load()
                 # <<<--- END CORRECTED MODEL LOADING (Predict) --- >>>
                 self.models[model_name] = model # Store loaded model
            # Ensure feature names are available from the loaded model
            if not model.feature_names_in_:
                 raise ValueError(f"Loaded model '{model_name}' does not contain feature names. Cannot ensure prediction consistency.")
            self.feature_names_in_ = model.feature_names_in_ # Use features from loaded model
        except Exception as e:
            logger.error(f"Failed to load model '{model_name}' from {model_path}: {e}", exc_info=True)
            return None

        # 3. Prepare Dataframe for Prediction (Select features)
        try:
            # Ensure only necessary features in correct order are passed
            missing_features = set(self.feature_names_in_) - set(df_input.columns)
            if missing_features:
                 # Try to recalculate missing features if possible (e.g., process the input df)
                 logger.warning(f"Input data missing required features: {missing_features}. Attempting to process input DataFrame.")
                 df_input_processed = process_features(df_input, self.config)
                 # Check again after processing
                 missing_features = set(self.feature_names_in_) - set(df_input_processed.columns)
                 if missing_features:
                      raise ValueError(f"Input data still missing required features for model '{model_name}' after processing: {missing_features}")
                 df_to_use = df_input_processed
            else:
                 df_to_use = df_input # Use original if features already present

            X_pred, _, _ = prepare_data_for_model(df_to_use, self.config, target_col=None, features=self.feature_names_in_)

        except (ValueError, KeyError) as e:
            logger.error(f"Failed to prepare input data for prediction with model '{model_name}': {e}", exc_info=True)
            return None

        # 4. Perform Prediction
        logger.info(f"Predicting using model '{model_name}' on {len(X_pred)} samples...")
        try:
            y_pred = model.predict(X_pred)
            y_prob = None
            if self.config.get("cli_options", {}).get("predict_probabilities", False): # Check if probs requested
                if hasattr(model, 'predict_proba'):
                    try:
                        y_prob = model.predict_proba(X_pred)
                        logger.info("Probabilities calculated.")
                    except NotImplementedError:
                         logger.warning(f"Model {model_name} does not support predict_proba, probabilities will not be included.")
                    except Exception as e:
                         logger.warning(f"Error calling predict_proba for {model_name}: {e}")
                else:
                     logger.warning(f"Model {model_name} does not have predict_proba method.")

        except Exception as e:
            logger.error(f"Error during prediction with model {model_name}: {e}", exc_info=True)
            return None

        # 5. Format Output
        # Include original identifiers if possible, using the potentially processed df_to_use
        id_cols = [col for col in ['domain_id', 'chain_id', 'resid', 'icode', 'resname'] if col in df_to_use.columns]
        result_df = df_to_use[id_cols].reset_index(drop=True).copy()
        result_df['predicted_class'] = y_pred

        if y_prob is not None:
             prob_cols = [f'prob_class_{i}' for i in range(y_prob.shape[1])]
             prob_df = pd.DataFrame(y_prob, columns=prob_cols)
             result_df = pd.concat([result_df, prob_df], axis=1)

        # 6. Save or Return Results
        if output_path:
            try:
                ensure_dir(os.path.dirname(output_path))
                result_df.to_csv(output_path, index=False, float_format='%.4f')
                logger.info(f"Predictions saved to: {output_path}")
                return None # Return None when saving to file
            except Exception as e:
                 logger.error(f"Failed to save predictions to {output_path}: {e}", exc_info=True)
                 return result_df # Return df even if saving failed
        else:
             logger.info("Prediction finished. Returning DataFrame.")
             return result_df


    @timer
    def analyze_rmsf_distribution(self, input_data_path: str, output_plot_path: Optional[str] = None):
        """Analyze RMSF distribution and visualize binning."""
        logger.info("--- Analyzing RMSF Distribution ---")
        if not _plotting_available:
             logger.warning("Plotting libraries not available. Skipping RMSF distribution analysis plot.")
             return

        # 1. Load RMSF data
        target_col = self.config['dataset']['target']
        try:
            # Find the actual file path using the pattern or direct path
            data_dir = self.config['paths'].get('data_dir')
            actual_path = input_data_path
            if not os.path.isabs(input_data_path) and data_dir and not os.path.exists(input_data_path):
                 potential_path = os.path.join(data_dir, input_data_path)
                 if os.path.exists(potential_path):
                      actual_path = potential_path
                 else: # Try finding via pattern if it wasn't a direct path
                      found = find_data_file(data_dir, input_data_path)
                      if found: actual_path = found
                      else: raise FileNotFoundError(f"Cannot find input data: {input_data_path}")

            rmsf_series = load_rmsf_data(actual_path, target_column=target_col)
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"Failed to load RMSF data for analysis: {e}")
            return

        # 2. Get Binner (either from instance or load)
        if not self.binner:
            binner_path = os.path.join(self.config['paths']['models_dir'], 'binner.joblib')
            if os.path.exists(binner_path):
                try:
                    self.binner = BaseBinner.load(binner_path, config=self.config)
                    logger.info(f"Loaded binner ({self.binner.__class__.__name__}) for analysis.")
                except Exception as e:
                    logger.error(f"Failed to load binner from {binner_path}: {e}. Cannot show boundaries.")
                    self.binner = None # Ensure binner is None if load failed
            else:
                logger.warning(f"Binner file not found at {binner_path}. Plotting distribution without bin boundaries.")
                self.binner = None

        # 3. Plot Distribution
        if not output_plot_path:
            output_plot_path = os.path.join(self.config['paths']['output_dir'], 'rmsf_distribution_analysis.png')

        boundaries = self.binner.get_boundaries() if self.binner else None
        try:
            plot_bin_distribution(
                rmsf_series.values,
                boundaries=boundaries, # Pass boundaries if available
                config=self.config,
                output_path=output_plot_path,
                title="RMSF Distribution Analysis" + (f" ({self.binner.__class__.__name__} Bins)" if self.binner else "")
            )
        except Exception as e:
            logger.error(f"Failed to plot RMSF distribution: {e}", exc_info=True)


    @timer
    def process_pdb(self, pdb_input: str, model_name: Optional[str] = None, output_prefix: Optional[str] = None):
        """
        Process a PDB file: Fetch/Parse -> Extract Features -> Predict -> Visualize.

        Args:
            pdb_input: PDB ID (e.g., "1AKE") or path to a local PDB file.
            model_name: Name of the model to use for prediction. Defaults to 'random_forest'.
            output_prefix: Base path and filename prefix for output files (e.g., colored PDB, PyMOL script).
                           Defaults to './pdb_output/{pdb_id}_flexibility'.
        """
        logger.info("====== Starting PDB Processing Pipeline ======")
        if not is_pdb_enabled(self.config):
             logger.error("PDB processing is disabled in configuration ('pdb.enabled=false'). Cannot proceed.")
             return

        if not model_name: model_name = "random_forest" # Default model

        # 1. Parse PDB Structure
        pdb_config = get_pdb_config(self.config)
        try:
            structure_model = parse_pdb(pdb_input, pdb_config)
            if structure_model is None: return # Error handled in parse_pdb
            structure_id = structure_model.get_parent().id # Get ID (e.g., PDB code or filename stem)
        except Exception as e:
            logger.error(f"Failed during PDB parsing stage for '{pdb_input}': {e}", exc_info=True)
            return

        # 2. Define Output Prefix
        if not output_prefix:
             output_dir = os.path.join(self.config['paths']['output_dir'], "pdb_visualizations")
             output_prefix = os.path.join(output_dir, f"{structure_id}_{model_name}_flexibility")
        ensure_dir(os.path.dirname(output_prefix))
        logger.info(f"Output prefix set to: {output_prefix}")


        # 3. Extract PDB Features
        logger.info(f"Extracting features from PDB structure: {structure_id}...")
        try:
            pdb_features_df = extract_pdb_features(structure_model, self.config)
            if pdb_features_df.empty:
                 logger.error("No features extracted from the PDB structure. Cannot proceed with prediction.")
                 return
            # Save extracted features for inspection
            features_out_path = f"{output_prefix}_extracted_features.csv"
            pdb_features_df.to_csv(features_out_path, index=False)
            logger.info(f"Extracted PDB features saved to {features_out_path}")
        except Exception as e:
            logger.error(f"Failed during PDB feature extraction for {structure_id}: {e}", exc_info=True)
            return

        # 4. Process Extracted Features (if needed - e.g., encoding, normalization, windowing)
        # The 'process_features' function expects certain columns. Ensure they match.
        # If processing is needed, apply it here. For now, assume prediction works directly on extracted features.
        # Potentially: df_processed_pdb = process_features(pdb_features_df, self.config) -> Use this for predict.
        # This depends heavily on whether models were trained *with* PDB features included.
        # Let's assume the predict function handles the necessary feature selection based on the loaded model.
        df_for_prediction = pdb_features_df

        # 5. Predict Flexibility
        logger.info(f"Predicting flexibility for {structure_id} using model '{model_name}'...")
        try:
            # Use the main predict method, ensuring it returns the DataFrame
            # Need to handle the case where predict saves to file instead of returning df
            # Modify predict or call internal prediction logic? Calling predict is cleaner.
            # We need the DataFrame back, so don't specify an output_path here.
            predictions_df = self.predict(data=df_for_prediction, model_name=model_name, output_path=None)

            if predictions_df is None or predictions_df.empty:
                 logger.error(f"Prediction failed or returned empty results for {structure_id}.")
                 return

            # Save predictions for inspection
            preds_out_path = f"{output_prefix}_predictions.csv"
            predictions_df.to_csv(preds_out_path, index=False)
            logger.info(f"Predictions saved to {preds_out_path}")

        except Exception as e:
            logger.error(f"Failed during prediction stage for {structure_id}: {e}", exc_info=True)
            return

        # 6. Generate Visualizations
        logger.info(f"Generating visualizations for {structure_id}...")
        # Ensure predictions_df has the necessary columns ('chain_id', 'resid', 'predicted_class')
        required_vis_cols = ['chain_id', 'resid', 'predicted_class']
        if not all(col in predictions_df.columns for col in required_vis_cols):
             logger.error(f"Predictions DataFrame is missing required columns for visualization: {required_vis_cols}")
             return

        # Generate PyMOL script
        try:
            pml_path = f"{output_prefix}.pml"
            pdb_filename_for_script = f"{structure_id}.pdb" # Assume standard naming if fetched
            generate_pymol_script(predictions_df, self.config, pml_path, pdb_filename=pdb_filename_for_script)
        except Exception as e:
             logger.error(f"Failed to generate PyMOL script: {e}", exc_info=True)

        # Generate colored PDB file (using B-factor)
        try:
             colored_pdb_path = f"{output_prefix}_colored.pdb"
             color_pdb_by_flexibility(structure_model, predictions_df, colored_pdb_path)
        except Exception as e:
            logger.error(f"Failed to generate colored PDB file: {e}", exc_info=True)


        logger.info(f"====== PDB Processing Finished for {structure_id} ======")


    @timer
    def run_temperature_comparison(self, model_name: Optional[str] = None):
        """Runs the temperature comparison analysis script."""
        logger.info("====== Starting Temperature Comparison Analysis ======")
        try:
             run_temperature_comparison_analysis(self.config, model_name)
        except Exception as e:
            logger.error(f"Temperature comparison analysis failed: {e}", exc_info=True)
        logger.info("====== Temperature Comparison Analysis Finished ======")

    # Potential method for the 'visualize' command (if needed beyond evaluation plots)
    def visualize_results(self, predictions_path: str, output_dir: Optional[str] = None):
         """Generates standalone visualizations from prediction results."""
         logger.info("--- Generating Standalone Visualizations ---")
         if not _plotting_available:
              logger.error("Plotting libraries not available. Cannot generate visualizations.")
              return

         if output_dir is None:
              output_dir = self.config['paths']['output_dir']
         ensure_dir(output_dir)

         try:
              pred_df = pd.read_csv(predictions_path)
              if 'predicted_class' not in pred_df.columns:
                   raise ValueError("Predictions file must contain 'predicted_class' column.")

              # Example: Plot class distribution from predictions
              dist_plot_path = os.path.join(output_dir, f"{os.path.basename(predictions_path).split('.')[0]}_class_distribution.png")
              plot_class_distribution(pred_df['predicted_class'], self.config, dist_plot_path, title="Predicted Class Distribution")

              # Add other visualizations based on prediction file content here...
              # e.g., plot confidence if probabilities are present

              logger.info(f"Standalone visualizations saved in {output_dir}")

         except FileNotFoundError:
              logger.error(f"Predictions file not found: {predictions_path}")
         except ValueError as ve:
              logger.error(f"Error processing predictions file for visualization: {ve}")
         except Exception as e:
              logger.error(f"Failed to generate standalone visualizations: {e}", exc_info=True)

--- End File: drwiggle/pipeline.py ---

--- File: drwiggle/temperature/comparison.py ---
---------------------------------------
import logging
import os
import glob
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Tuple, Union

from drwiggle.config import get_temperature_config, get_binning_config, get_class_names
from drwiggle.utils.helpers import ensure_dir, load_object, save_object
from drwiggle.utils.metrics import generate_confusion_matrix_df
from drwiggle.utils.visualization import plot_metric_vs_temperature, plot_transition_matrix, _plotting_available

logger = logging.getLogger(__name__)

def find_result_files(
    base_output_dir: str,
    temperatures: List[Union[int, str]],
    model_name: Optional[str] = None,
    file_type: str = "predictions", # "predictions", "metrics", "binner"
    pattern_suffix: str = "_test_predictions.csv", # Example pattern
    temperature_prefix: str = "run_temp_" # Expected prefix for temp-specific dirs
) -> Dict[Union[int, str], str]:
    """
    Scans output directories for specific result files for given temperatures.

    Looks for files in temperature-specific subdirectories first (e.g., run_temp_320/),
    then falls back to the base output directory.

    Args:
        base_output_dir: The main output directory potentially containing subdirs per temp.
        temperatures: List of temperatures to look for.
        model_name: Specific model name if file is model-specific (used in filename pattern).
        file_type: Type of file to search for (used in logging).
        pattern_suffix: Suffix pattern of the file (e.g., "_metrics.csv", ".joblib").
        temperature_prefix: Prefix used for temperature-specific subdirectories.

    Returns:
        Dictionary mapping temperature to the found absolute file path.
    """
    result_files = {}
    logger.info(f"Scanning '{base_output_dir}' for {file_type} files (pattern suffix: '{pattern_suffix}') for temperatures: {temperatures}")

    if not os.path.isdir(base_output_dir):
        logger.error(f"Base output directory for scanning does not exist: {base_output_dir}")
        return result_files

    for temp in temperatures:
        # Construct expected temp-specific directory path
        temp_output_dir = os.path.join(base_output_dir, f"{temperature_prefix}{temp}")
        # Directories to search, prioritizing temp-specific
        search_dirs = [temp_output_dir, base_output_dir]

        found_path = None
        for search_dir in search_dirs:
            if not os.path.isdir(search_dir):
                 # Only log if it's the temp-specific dir missing, base dir was checked earlier
                 if search_dir == temp_output_dir:
                     logger.debug(f"Temperature-specific directory not found: {search_dir}")
                 continue

            # Construct search pattern (handle cases where model_name is None)
            base_filename_pattern = f"{model_name or '*'}{pattern_suffix}"
            search_path = os.path.join(search_dir, base_filename_pattern)
            matching_files = glob.glob(search_path)

            if matching_files:
                 if len(matching_files) > 1:
                      logger.warning(f"Multiple {file_type} files found for temp {temp} in {search_dir} matching '{base_filename_pattern}'. Using first found: {matching_files[0]}")
                 # Return absolute path
                 found_path = os.path.abspath(matching_files[0])
                 logger.info(f"Found {file_type} file for temperature {temp} in {search_dir}: {found_path}")
                 break # Found in this directory, stop searching for this temp

        if found_path:
            result_files[temp] = found_path
        else:
            logger.warning(f"Could not find {file_type} file for temperature {temp} in expected locations: {search_dirs} with pattern '{base_filename_pattern}'")

    return result_files


def calculate_transition_matrix(
     predictions1: pd.Series, # Predictions at T1
     predictions2: pd.Series, # Predictions at T2 (must match index with T1)
     num_classes: int,
     class_names: Optional[List[str]] = None
 ) -> Optional[pd.DataFrame]:
     """
     Calculates a class transition matrix between two sets of predictions using generate_confusion_matrix_df.
     Requires predictions Series to be aligned (e.g., same residues/index).

     Args:
         predictions1: Series of predicted classes at Temperature 1.
         predictions2: Series of predicted classes at Temperature 2.
         num_classes: Total number of classes.
         class_names: Optional list of class names for labeling the matrix axes.

     Returns:
         DataFrame representing the transition matrix (counts), or None on error. Rows=T1, Cols=T2.
     """
     if predictions1.shape != predictions2.shape:
         logger.error("Prediction Series must have the same shape for transition matrix calculation.")
         return None
     if not predictions1.index.equals(predictions2.index):
         logger.warning("Prediction Series indices do not match. Results may be incorrect if rows don't represent the same items. Ensure alignment before calling.")
         # Attempting to proceed, but results might be meaningless

     # Use the confusion matrix utility, treating T1 as 'true' and T2 as 'pred'
     logger.debug(f"Calculating transition matrix for {len(predictions1)} aligned predictions.")
     # Create a dummy config just for passing class names if needed
     temp_config = {"evaluation": {"class_names": {i: name for i, name in enumerate(class_names)}} if class_names else {}}

     matrix_df = generate_confusion_matrix_df(predictions1, predictions2, config=temp_config) # Use helper

     if matrix_df is not None:
          # Rename axes for clarity
          matrix_df.index.name = "Class at T1"
          matrix_df.columns.name = "Class at T2"
          logger.debug("Transition matrix calculated successfully.")
     else:
          logger.error("Failed to calculate transition matrix using generate_confusion_matrix_df.")

     return matrix_df


def run_temperature_comparison_analysis(config: Dict[str, Any], model_name: Optional[str] = None):
    """
    Main function to perform temperature comparison analysis.

    Loads predictions and metrics from previous runs (identified by temperature)
    and calculates comparative statistics and generates plots.

    Args:
        config: The main configuration dictionary (already loaded and resolved).
        model_name: Specific model to compare. If None, attempts to find results for all enabled models.
                   Currently, analysis focuses on one model at a time or combined metrics.
    """
    temp_config = get_temperature_config(config)
    if not temp_config.get("comparison", {}).get("enabled", False):
        logger.info("Temperature comparison is disabled in the configuration ('temperature.comparison.enabled=false').")
        return

    base_output_dir = config['paths']['output_dir'] # Base directory where results are stored
    # Create a dedicated subdir for comparison results
    comparison_output_dir = os.path.join(base_output_dir, "temperature_comparison")
    ensure_dir(comparison_output_dir)

    temperatures = temp_config.get("available", [])
    if not temperatures or len(temperatures) < 2:
        logger.warning("Temperature comparison requires at least two temperatures defined in config ('temperature.available').")
        return

    # Sort temperatures numerically if possible for plotting order
    try:
        # Convert to float, handle strings like 'average' by placing them last
        sorted_temps = sorted(temperatures, key=lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else float('inf'))
    except ValueError:
        logger.warning("Could not sort temperatures numerically. Using order from config.")
        sorted_temps = temperatures

    logger.info(f"--- Running Temperature Comparison ---")
    logger.info(f"Model focus: {model_name or 'All available models'}")
    logger.info(f"Temperatures: {sorted_temps}")
    logger.info(f"Results directory: {comparison_output_dir}")

    # --- Load necessary data ---
    # We need predictions for transition matrices and metrics for performance plots.
    # Handle case where model_name is not specified - try to load combined metrics.

    # 1. Load Metrics
    all_metrics_list = []
    # Try finding a combined summary file first (e.g., from multiple runs saved together)
    # Or look for model-specific metrics files per temperature run.
    # Let's assume a common pattern: evaluation_summary.csv or {model}_metrics.csv
    # If model_name is given, prioritize that pattern.
    metrics_suffix = f"{model_name}_evaluation_summary.csv" if model_name else "evaluation_summary.csv"
    metric_files = find_result_files(base_output_dir, sorted_temps, None, "metrics", metrics_suffix) # Search without model name in pattern?

    # If model-specific file not found, try a general one
    if not metric_files and model_name:
         metrics_suffix_generic = "evaluation_summary.csv"
         metric_files = find_result_files(base_output_dir, sorted_temps, None, "metrics", metrics_suffix_generic)

    for temp in sorted_temps:
        if temp in metric_files:
            path = metric_files[temp]
            try:
                metrics_df = pd.read_csv(path)
                # Check if 'model' column exists, if not, assume it's for the specified model_name
                if 'model' not in metrics_df.columns and model_name:
                    metrics_df['model'] = model_name
                elif 'model' not in metrics_df.columns and not model_name:
                     logger.warning(f"Metrics file {path} has no 'model' column and no model_name specified. Skipping.")
                     continue

                metrics_df['temperature'] = temp # Add temperature column for merging
                all_metrics_list.append(metrics_df)
                logger.info(f"Loaded metrics summary for temperature {temp} from {path}")
            except Exception as e:
                logger.warning(f"Failed to load metrics file {path} for temp {temp}: {e}")
        else:
             logger.warning(f"Metrics file not found for temperature {temp} using suffixes '{metrics_suffix}' or 'evaluation_summary.csv'.")


    combined_metrics_df = pd.concat(all_metrics_list, ignore_index=True) if all_metrics_list else pd.DataFrame()

    if not combined_metrics_df.empty:
         # Save the combined metrics dataframe
         combined_metrics_path = os.path.join(comparison_output_dir, f"{model_name or 'all'}_combined_metrics.csv")
         combined_metrics_df.to_csv(combined_metrics_path, index=False)
         logger.info(f"Combined metrics saved to {combined_metrics_path}")
    else:
         logger.warning("No metrics data loaded. Cannot generate metric comparison plots.")


    # 2. Load Predictions (only if transition matrices needed and model_name specified)
    plot_transitions = temp_config.get("comparison", {}).get("plot_transition_matrix", False) and _plotting_available
    all_predictions: Dict[Union[int, str], pd.DataFrame] = {}
    if plot_transitions:
        if not model_name:
            logger.warning("Cannot calculate transition matrices without a specific 'model_name' specified.")
            plot_transitions = False # Disable if no model specified
        else:
            pred_suffix = f"{model_name}_test_predictions.csv"
            prediction_files = find_result_files(base_output_dir, sorted_temps, None, "predictions", pred_suffix) # Search without model name in pattern first?
            if not prediction_files: # Try with model name explicitly if first failed
                 prediction_files = find_result_files(base_output_dir, sorted_temps, model_name, "predictions", "_test_predictions.csv")


            if len(prediction_files) < 2:
                 logger.warning(f"Found predictions for fewer than 2 temperatures for model '{model_name}'. Cannot calculate transitions.")
                 plot_transitions = False # Disable if not enough data
            else:
                # Load predictions and align them
                loaded_preds_list = []
                for temp, path in prediction_files.items():
                    try:
                        # Load necessary columns and create unique ID
                        preds_df = pd.read_csv(path, usecols=lambda col: col.lower() in ['domain_id', 'resid', 'predicted_class'])
                        preds_df['unique_id'] = preds_df['domain_id'].astype(str) + "_" + preds_df['resid'].astype(str)
                        preds_df = preds_df.set_index('unique_id')
                        # Rename prediction column to include temperature
                        preds_df = preds_df.rename(columns={'predicted_class': f'pred_{temp}'})
                        loaded_preds_list.append(preds_df[[f'pred_{temp}']]) # Select only the prediction column
                        logger.info(f"Loaded {len(preds_df)} predictions for model '{model_name}' at temperature {temp}.")
                    except Exception as e:
                        logger.warning(f"Failed to load or process predictions file {path} for temp {temp}: {e}")

                # Merge predictions based on the unique residue index
                if len(loaded_preds_list) >= 2:
                     aligned_preds_df = pd.concat(loaded_preds_list, axis=1, join='inner') # Use inner join to keep only common residues
                     logger.info(f"Aligned predictions across {len(prediction_files)} temperatures. Found {len(aligned_preds_df)} common residues.")
                     if aligned_preds_df.empty:
                          logger.warning("No common residues found across all loaded prediction files. Cannot calculate transitions.")
                          plot_transitions = False
                else:
                     aligned_preds_df = pd.DataFrame() # Ensure it exists but is empty
                     plot_transitions = False


    # --- Perform Comparisons & Plotting ---

    # 1. Plot Metrics vs Temperature
    if not combined_metrics_df.empty and _plotting_available:
        metrics_to_plot = temp_config.get("comparison", {}).get("metrics", [])
        logger.info(f"Plotting metrics {metrics_to_plot} vs temperature...")
        # Filter combined metrics for the specific model if one was provided
        plot_metrics_df = combined_metrics_df
        if model_name:
             plot_metrics_df = combined_metrics_df[combined_metrics_df['model'] == model_name].copy()
             if plot_metrics_df.empty:
                  logger.warning(f"No metrics found for specified model '{model_name}' in loaded data.")


        if not plot_metrics_df.empty:
             for metric in metrics_to_plot:
                 if metric in plot_metrics_df.columns:
                      plot_path = os.path.join(comparison_output_dir, f"{model_name or 'all'}_metric_{metric}_vs_temp.png")
                      plot_metric_vs_temperature(plot_metrics_df, metric, plot_path)
                 else:
                      logger.warning(f"Metric '{metric}' requested for plotting not found in loaded metrics data.")
        else:
             logger.warning("No metrics data available for plotting after filtering.")


    # 2. Calculate and Plot Transition Matrices
    if plot_transitions and not aligned_preds_df.empty:
        num_classes = get_binning_config(config).get('num_classes', 5)
        class_names = list(get_class_names(config).values()) if get_class_names(config) else None
        logger.info(f"Calculating and plotting transition matrices for model '{model_name}'...")

        # Compare adjacent temperatures in the sorted list
        for i in range(len(sorted_temps) - 1):
            temp1 = sorted_temps[i]
            temp2 = sorted_temps[i+1]
            col1 = f'pred_{temp1}'
            col2 = f'pred_{temp2}'

            if col1 not in aligned_preds_df.columns or col2 not in aligned_preds_df.columns:
                 logger.warning(f"Missing prediction columns for transition between {temp1}K and {temp2}K. Skipping.")
                 continue

            preds1_aligned = aligned_preds_df[col1]
            preds2_aligned = aligned_preds_df[col2]

            logger.info(f"Calculating transition matrix between {temp1}K and {temp2}K ({len(preds1_aligned)} common residues).")

            try:
                 transition_matrix = calculate_transition_matrix(preds1_aligned, preds2_aligned, num_classes, class_names)

                 if transition_matrix is not None:
                      # Save matrix to CSV
                      matrix_path = os.path.join(comparison_output_dir, f"{model_name}_transition_{temp1}_to_{temp2}.csv")
                      transition_matrix.to_csv(matrix_path)
                      logger.info(f"Transition matrix saved to {matrix_path}")

                      # Plot normalized matrix
                      plot_path_norm = os.path.join(comparison_output_dir, f"{model_name}_transition_{temp1}_to_{temp2}_norm.png")
                      plot_transition_matrix(transition_matrix, str(temp1), str(temp2), plot_path_norm, normalize=True)

                      # Plot count matrix
                      plot_path_counts = os.path.join(comparison_output_dir, f"{model_name}_transition_{temp1}_to_{temp2}_counts.png")
                      plot_transition_matrix(transition_matrix, str(temp1), str(temp2), plot_path_counts, normalize=False)

            except Exception as e:
                 logger.error(f"Failed to calculate or plot transition matrix between {temp1}K and {temp2}K: {e}", exc_info=True)

    elif plot_transitions and aligned_preds_df.empty:
         logger.warning("Skipping transition matrix plotting as no aligned predictions were loaded.")

    logger.info(f"--- Temperature Comparison Finished ---")
    logger.info(f"Results saved in: {comparison_output_dir}")

--- End File: drwiggle/temperature/comparison.py ---

--- File: drwiggle/utils/helpers.py ---
---------------------------------------
import os
import logging
import time
from functools import wraps
from typing import Iterable, TypeVar, Optional, Dict, List, Any
import joblib # For saving/loading python objects easily
import glob
import traceback

try:
    from tqdm.auto import tqdm # Use richer progress bars if available (notebooks)
except ImportError:
    try:
        from tqdm import tqdm # Fallback to standard tqdm
    except ImportError:
        # Provide a dummy tqdm if not installed at all
        print("Warning: tqdm not installed. Progress bars will be disabled.")
        print("Install tqdm for progress bars: pip install tqdm")
        def tqdm(iterable, *args, **kwargs):
            return iterable

logger = logging.getLogger(__name__)

T = TypeVar('T') # Generic type variable

def timer(func):
    """Decorator to time function execution and log the duration."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        duration = end_time - start_time
        # Log duration at DEBUG level to avoid cluttering INFO logs
        logger.debug(f"Function '{func.__name__}' executed in {duration:.4f} seconds")
        return result
    return wrapper

def ensure_dir(directory_path: str):
    """
    Creates a directory if it doesn't exist. Logs success or failure.

    Args:
        directory_path: The path of the directory to create.
    """
    if not directory_path:
        logger.warning("ensure_dir called with empty or None path. Skipping.")
        return
    abs_path = os.path.abspath(directory_path)
    if not os.path.exists(abs_path):
        try:
            os.makedirs(abs_path, exist_ok=True)
            logger.debug(f"Created directory: {abs_path}")
        except OSError as e:
            logger.error(f"Failed to create directory {abs_path}: {e}")
            raise # Re-raise the exception after logging
    else:
         logger.debug(f"Directory already exists: {abs_path}")

# Wrapper for tqdm progress bar with optional disabling based on log level
def progress_bar(
    iterable: Iterable[T],
    desc: Optional[str] = None,
    total: Optional[int] = None,
    disable: Optional[bool] = None, # Allow explicit disabling
    leave: bool = True,
    **kwargs
) -> Iterable[T]:
    """
    Provides a tqdm progress bar with flexible disabling options.

    Disables the bar if log level is WARNING or higher, unless explicitly
    enabled via `disable=False`.

    Args:
        iterable: The iterable to wrap with a progress bar.
        desc: Optional description for the progress bar.
        total: Optional total number of items (useful if len(iterable) is slow/unavailable).
        disable: Explicitly disable (True) or enable (False) the bar. If None, uses log level.
        leave: Whether to leave the finished progress bar visible (default True).
        **kwargs: Additional arguments passed directly to tqdm.

    Returns:
        The tqdm-wrapped iterable.
    """
    if disable is None: # Automatic disabling based on log level
        log_level = logging.getLogger().getEffectiveLevel()
        # Disable if logging level is WARNING or above (more severe)
        effective_disable = log_level >= logging.WARNING
    else: # Respect explicit setting
        effective_disable = disable

    # Try to get total length if not provided and not disabled
    if total is None and not effective_disable:
        try:
            total = len(iterable) # type: ignore
        except (TypeError, AttributeError):
            total = None # Cannot determine length

    # Set default tqdm arguments if not provided
    kwargs.setdefault('ncols', 100) # Default width
    kwargs.setdefault('bar_format', '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]')

    return tqdm(
        iterable,
        desc=desc,
        total=total,
        disable=effective_disable,
        leave=leave,
        **kwargs
    )

def save_object(obj: Any, path: str, compress: int = 3):
    """
    Saves a Python object to a file using joblib.

    Args:
        obj: The Python object to save.
        path: The file path (directory will be created).
        compress: Compression level for joblib (0-9). Default is 3.
    """
    try:
        ensure_dir(os.path.dirname(path))
        joblib.dump(obj, path, compress=compress)
        logger.info(f"Object saved successfully to {path} (compression={compress})")
    except Exception as e:
        logger.error(f"Failed to save object to {path}: {e}", exc_info=True)
        raise IOError(f"Could not save object to file: {path}") from e

def load_object(path: str) -> Any:
    """
    Loads a Python object from a file saved using joblib.

    Args:
        path: The file path.

    Returns:
        The loaded Python object.

    Raises:
        FileNotFoundError: If the file does not exist.
        IOError: If loading fails.
    """
    if not os.path.exists(path):
        logger.error(f"File not found for loading object: {path}")
        raise FileNotFoundError(f"Cannot load object, file not found: {path}")
    try:
        obj = joblib.load(path)
        logger.info(f"Object loaded successfully from {path}")
        return obj
    except Exception as e:
        logger.error(f"Failed to load object from {path}: {e}", exc_info=True)
        raise IOError(f"Could not load object from file: {path}") from e

# Add other general utility functions here as needed.

--- End File: drwiggle/utils/helpers.py ---

--- File: drwiggle/utils/metrics.py ---
---------------------------------------
import logging
from typing import Dict, Any, List, Optional, Union
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
    cohen_kappa_score,
    mean_absolute_error # For ordinal error
)
# Weighted kappa requires careful handling due to potential version differences
try:
    # Prefer newer signature if available
    from sklearn.metrics import cohen_kappa_score as calculate_weighted_kappa
    _kappa_supports_weights = True
except ImportError:
    # Fallback for older versions might exist but is less standard
    logger = logging.getLogger(__name__)
    logger.warning("Could not import weighted kappa calculation logic from sklearn.metrics. Weighted Kappa might be unavailable.")
    _kappa_supports_weights = False
    def calculate_weighted_kappa(*args, **kwargs): return np.nan

logger = logging.getLogger(__name__)

def calculate_ordinal_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Calculates the Mean Absolute Error between class indices."""
    if y_true.shape != y_pred.shape:
        logger.error(f"Shape mismatch for ordinal error: y_true={y_true.shape}, y_pred={y_pred.shape}")
        # Return NaN or raise error? Returning NaN for now.
        return np.nan
    if y_true.ndim != 1:
        logger.warning(f"Ordinal error expects 1D arrays, got y_true={y_true.ndim}D, y_pred={y_pred.ndim}D. Flattening.")
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()
    try:
        return mean_absolute_error(y_true, y_pred)
    except Exception as e:
         logger.error(f"Failed to calculate ordinal error (MAE): {e}", exc_info=True)
         return np.nan


def evaluate_classification(
    y_true: Union[np.ndarray, pd.Series],
    y_pred: Union[np.ndarray, pd.Series],
    config: Dict[str, Any],
    y_prob: Optional[Union[np.ndarray, pd.DataFrame]] = None, # Add probabilities if available
    model_name: Optional[str] = "Unknown Model" # For logging context
) -> Dict[str, float]:
    """
    Evaluate classification performance using metrics specified in the config.

    Args:
        y_true: True class labels (n_samples,).
        y_pred: Predicted class labels (n_samples,).
        config: Configuration dictionary containing evaluation settings.
        y_prob: Predicted probabilities (n_samples, n_classes), optional.
        model_name: Name of the model being evaluated (for logging).

    Returns:
        Dictionary containing calculated metric values (floats). Returns NaN for failed metrics.
    """
    metrics_config = config.get("evaluation", {}).get("metrics", {})
    class_names_map = config.get("evaluation", {}).get("class_names", {})
    num_classes = len(class_names_map) if class_names_map else 0

    # Infer num_classes if not explicitly available
    if num_classes == 0 and (y_true is not None and len(y_true) > 0) and (y_pred is not None and len(y_pred) > 0):
        try:
            max_label = int(max(np.max(y_true), np.max(y_pred)))
            num_classes = max_label + 1
            logger.warning(f"Number of classes not found in config, inferred as {num_classes} from data.")
        except Exception:
            logger.error("Could not infer number of classes from y_true/y_pred.")
            num_classes = 0 # Set back to 0 if inference fails


    class_labels = list(range(num_classes)) if num_classes > 0 else []

    results: Dict[str, float] = {} # Store results here

    # Convert inputs to numpy arrays if they are pandas Series/DataFrames
    if isinstance(y_true, pd.Series): y_true = y_true.values
    if isinstance(y_pred, pd.Series): y_pred = y_pred.values
    if isinstance(y_prob, pd.DataFrame): y_prob = y_prob.values

    # Basic input validation
    if y_true is None or y_pred is None:
         logger.error(f"Missing y_true or y_pred for evaluation ({model_name}). Cannot calculate metrics.")
         return {metric: np.nan for metric in metrics_config if metrics_config.get(metric)}
    if y_true.shape != y_pred.shape:
        logger.error(f"Shape mismatch for evaluation ({model_name}): y_true={y_true.shape}, y_pred={y_pred.shape}")
        return {metric: np.nan for metric in metrics_config if metrics_config.get(metric)}
    if len(y_true) == 0:
         logger.warning(f"Evaluation arrays for {model_name} are empty. Returning NaN for all metrics.")
         return {metric: np.nan for metric in metrics_config if metrics_config.get(metric)}
    if num_classes == 0:
         logger.error(f"Cannot determine number of classes for evaluation ({model_name}). Metrics requiring labels will fail.")
         # Allow metrics like accuracy that don't strictly need num_classes?

    # --- Calculate Enabled Metrics ---
    logger.info(f"Calculating evaluation metrics for {model_name}...")
    try:
        if metrics_config.get("accuracy"):
            try:
                results["accuracy"] = accuracy_score(y_true, y_pred)
            except Exception as e:
                 logger.warning(f"Failed to calculate accuracy: {e}")
                 results["accuracy"] = np.nan

        if metrics_config.get("balanced_accuracy"):
             try:
                 # Requires labels if not all classes are present in y_true/y_pred
                 results["balanced_accuracy"] = balanced_accuracy_score(y_true, y_pred)
             except Exception as e:
                 logger.warning(f"Failed to calculate balanced_accuracy: {e}")
                 results["balanced_accuracy"] = np.nan

        # Precision, Recall, F1
        prf1_metrics = {"precision", "recall", "f1"}
        if any(metrics_config.get(m) for m in prf1_metrics):
            try:
                # Calculate macro and weighted averages
                # Pass labels to ensure calculation considers all potential classes
                p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(
                    y_true, y_pred, average='macro', zero_division=0, labels=class_labels if class_labels else None
                )
                p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(
                    y_true, y_pred, average='weighted', zero_division=0, labels=class_labels if class_labels else None
                )

                if metrics_config.get("precision"):
                     results["precision_macro"] = p_macro
                     results["precision_weighted"] = p_weighted
                if metrics_config.get("recall"):
                     results["recall_macro"] = r_macro
                     results["recall_weighted"] = r_weighted
                if metrics_config.get("f1"):
                     results["f1_macro"] = f1_macro
                     results["f1_weighted"] = f1_weighted

            except Exception as e:
                 logger.warning(f"Failed to calculate precision/recall/f1 scores: {e}")
                 if metrics_config.get("precision"): results["precision_macro"] = results["precision_weighted"] = np.nan
                 if metrics_config.get("recall"): results["recall_macro"] = results["recall_weighted"] = np.nan
                 if metrics_config.get("f1"): results["f1_macro"] = results["f1_weighted"] = np.nan

        if metrics_config.get("cohen_kappa"):
            try:
                results["cohen_kappa"] = cohen_kappa_score(y_true, y_pred, labels=class_labels if class_labels else None)
            except Exception as e:
                 logger.warning(f"Failed to calculate cohen_kappa: {e}")
                 results["cohen_kappa"] = np.nan

        if metrics_config.get("weighted_kappa"):
             if _kappa_supports_weights:
                 try:
                     # Use quadratic weights suitable for ordinal classes
                     results["weighted_kappa"] = calculate_weighted_kappa(y_true, y_pred, weights='quadratic', labels=class_labels if class_labels else None)
                 except Exception as e:
                      logger.warning(f"Failed to calculate weighted_kappa (quadratic): {e}")
                      results["weighted_kappa"] = np.nan
             else:
                  logger.warning("Weighted kappa calculation skipped as it's not supported or import failed.")
                  results["weighted_kappa"] = np.nan


        if metrics_config.get("ordinal_error"):
            results["ordinal_error"] = calculate_ordinal_error(y_true, y_pred)

        # Add more metrics here if needed (e.g., AUC, LogLoss if y_prob provided)
        # Example: Log Loss
        # if metrics_config.get("log_loss") and y_prob is not None:
        #     from sklearn.metrics import log_loss
        #     try:
        #         # Ensure y_prob has shape (n_samples, n_classes) and covers all potential classes
        #         if y_prob.shape[1] < num_classes:
        #              logger.warning(f"y_prob has fewer columns ({y_prob.shape[1]}) than classes ({num_classes}). Cannot compute log_loss accurately.")
        #              results["log_loss"] = np.nan
        #         else:
        #              # Need to ensure labels passed to log_loss match the columns of y_prob
        #              results["log_loss"] = log_loss(y_true, y_prob[:, :num_classes], labels=class_labels)
        #     except Exception as e:
        #          logger.warning(f"Failed to calculate log_loss: {e}")
        #          results["log_loss"] = np.nan


    except Exception as e:
        logger.error(f"Unexpected error during metric calculation for {model_name}: {e}", exc_info=True)
        # Ensure all requested metrics have a NaN entry if calculation failed globally
        for metric in metrics_config:
            if metrics_config.get(metric) and metric not in results:
                 results[metric] = np.nan

    # Log computed metrics
    metrics_log = ", ".join([f"{k}: {v:.4f}" for k, v in results.items() if not np.isnan(v)])
    logger.info(f"Evaluation Metrics ({model_name}): {metrics_log}")
    if any(np.isnan(v) for v in results.values()):
        nan_metrics = [k for k, v in results.items() if np.isnan(v)]
        logger.warning(f"Could not compute the following metrics for {model_name}: {nan_metrics}")


    return results

# --- Functions to generate report structures (can be saved/printed later) ---

def generate_classification_report_dict(
     y_true: Union[np.ndarray, pd.Series],
     y_pred: Union[np.ndarray, pd.Series],
     config: Dict[str, Any], # Pass config to get class names
) -> Dict[str, Any]:
     """
     Generates a classification report as a dictionary using sklearn.

     Args:
         y_true: True class labels.
         y_pred: Predicted class labels.
         config: Main configuration dictionary.

     Returns:
         Dictionary representation of the classification report. Returns empty dict on error.
     """
     if isinstance(y_true, pd.Series): y_true = y_true.values
     if isinstance(y_pred, pd.Series): y_pred = y_pred.values

     class_names_map = config.get("evaluation", {}).get("class_names", {})
     num_classes = len(class_names_map) if class_names_map else 0
     if num_classes == 0: # Infer if needed
          try:
               max_label = int(max(np.max(y_true), np.max(y_pred)))
               num_classes = max_label + 1
          except Exception: num_classes = 0

     labels = list(range(num_classes)) if num_classes > 0 else None
     target_names = [class_names_map.get(i, f"Class_{i}") for i in labels] if labels and class_names_map else None

     logger.debug(f"Generating classification report with labels={labels}, target_names={target_names}")

     try:
          # Ensure labels arg matches target_names if provided
          report = classification_report(
               y_true, y_pred,
               labels=labels,
               target_names=target_names,
               output_dict=True,
               zero_division=0
          )
          return report
     except Exception as e:
          logger.error(f"Could not generate classification report: {e}", exc_info=True)
          return {}

def generate_confusion_matrix_df(
     y_true: Union[np.ndarray, pd.Series],
     y_pred: Union[np.ndarray, pd.Series],
     config: Dict[str, Any], # Pass config to get class names
) -> Optional[pd.DataFrame]:
     """
     Calculates the confusion matrix and returns it as a pandas DataFrame.

     Args:
         y_true: True class labels.
         y_pred: Predicted class labels.
         config: Main configuration dictionary.

     Returns:
         pandas DataFrame representing the confusion matrix, or None on error.
         Rows represent True labels, Columns represent Predicted labels.
     """
     if isinstance(y_true, pd.Series): y_true = y_true.values
     if isinstance(y_pred, pd.Series): y_pred = y_pred.values

     class_names_map = config.get("evaluation", {}).get("class_names", {})
     num_classes = len(class_names_map) if class_names_map else 0
     if num_classes == 0: # Infer if needed
          try:
               max_label = int(max(np.max(y_true), np.max(y_pred)))
               num_classes = max_label + 1
          except Exception: num_classes = 0

     labels = list(range(num_classes)) if num_classes > 0 else []
     cm_labels = [class_names_map.get(i, f"Class_{i}") for i in labels] if labels and class_names_map else None
     if labels and not cm_labels: # Generate default names if needed
          cm_labels = [f"Class_{i}" for i in labels]


     if not labels:
          logger.warning("Cannot generate confusion matrix: number of classes is zero or could not be determined.")
          return None

     logger.debug(f"Generating confusion matrix with labels={labels}, class_names={cm_labels}")

     try:
          cm_array = confusion_matrix(y_true, y_pred, labels=labels)
          cm_df = pd.DataFrame(cm_array, index=cm_labels, columns=cm_labels)
          cm_df.index.name = 'True Label'
          cm_df.columns.name = 'Predicted Label'
          return cm_df
     except Exception as e:
          logger.error(f"Could not calculate confusion matrix: {e}", exc_info=True)
          return None

--- End File: drwiggle/utils/metrics.py ---

--- File: drwiggle/utils/pdb_tools.py ---
---------------------------------------
# --- File: drwiggle/utils/pdb_tools.py ---
# Corrected with debug print statements around BioPython import

import logging
import os
import re
import warnings
from typing import Dict, Any, Optional, Tuple, List
import pandas as pd
import numpy as np

# Local imports (keep these first if possible, though sometimes order matters less)
# Defer config import if it causes circular issues, but seems okay here
from drwiggle.config import get_pdb_config, get_pdb_feature_config
from drwiggle.utils.helpers import ensure_dir

logger = logging.getLogger(__name__)

# --- DEBUG ---
print("DEBUG: Attempting to import BioPython in pdb_tools.py...")
# --- END DEBUG ---

# Biopython imports
try:
    from Bio.PDB import PDBParser, PDBIO, Select, Polypeptide
    from Bio.PDB.DSSP import DSSP
    from Bio.PDB.exceptions import PDBException
    from Bio.PDB.PDBList import PDBList
    from Bio.SeqUtils import seq1 # To convert 3-letter AA code to 1-letter
    _biopython_available = True
    # --- DEBUG ---
    print("DEBUG: BioPython imported successfully in pdb_tools.py.")
    # --- END DEBUG ---
except ImportError:
    # --- DEBUG ---
    print("DEBUG: ImportError caught in pdb_tools.py.")
    # --- END DEBUG ---
    # Log the warning using the logger setup by the main application
    # Check if a logger exists before trying to use it during initial import phase
    try:
        logging.getLogger(__name__).warning("BioPython not found. PDB processing features will be unavailable. Install with `pip install biopython`.")
    except Exception: # Catch potential logging setup issues during import
        print("Warning: BioPython not found (logging not fully configured yet). Install with `pip install biopython`.")

    # Define dummy classes/functions to avoid errors if module is imported but BP not installed
    class PDBParser: pass
    class PDBIO: pass
    class Select: pass
    class Polypeptide: pass
    class DSSP: pass
    class PDBException(Exception): pass
    class PDBList: pass
    def seq1(res): return 'X'
    _biopython_available = False


# --- PDB Parsing and Feature Extraction ---

def fetch_pdb(pdb_id: str, cache_dir: str) -> Optional[str]:
    """
    Downloads a PDB file if not already cached.

    Args:
        pdb_id: The 4-character PDB ID.
        cache_dir: The directory to store/retrieve PDB files.

    Returns:
        The path to the cached PDB file (format .pdb), or None if download fails.
    """
    if not _biopython_available:
         # Use logger now as it should be configured when function is called
         logger.error("BioPython PDBList not available for fetching PDB files.")
         return None

    ensure_dir(cache_dir)
    pdb_list = PDBList(pdb=cache_dir, obsolete_pdb=cache_dir, verbose=False)
    # Explicitly request pdb format, adjust filename handling
    try:
        # retrieve_pdb_file returns the path it *would* have if downloaded/cached
        expected_path = pdb_list.retrieve_pdb_file(pdb_id, pdir=cache_dir, file_format='pdb')

        # Check if the file actually exists after retrieve_pdb_file call
        if os.path.exists(expected_path):
            logger.info(f"PDB file for {pdb_id} found/downloaded at: {expected_path}")
            return expected_path
        else:
             # Sometimes PDBList doesn't error but fails to download
             logger.error(f"Failed to retrieve PDB file for {pdb_id} (expected path: {expected_path}). Check ID and network.")
             return None

    except Exception as e:
        logger.error(f"Error retrieving PDB file for {pdb_id}: {e}", exc_info=True)
        return None

def parse_pdb(pdb_path_or_id: str, pdb_config: Dict[str, Any]) -> Optional[Any]:
    """
    Parses a PDB file using BioPython's PDBParser. Handles fetching if ID is given.

    Args:
        pdb_path_or_id: Path to the PDB file or a 4-character PDB ID.
        pdb_config: PDB configuration dictionary (must contain 'pdb_cache_dir').

    Returns:
        Bio.PDB Model object (the first model found), or None if parsing/fetching fails.
    """
    if not _biopython_available:
         logger.error("BioPython PDBParser not available for parsing PDB files.")
         return None

    pdb_id_pattern = re.compile(r"^[a-zA-Z0-9]{4}$")
    pdb_path = None
    structure_id = "structure" # Default ID for the structure object

    if os.path.isfile(pdb_path_or_id):
        pdb_path = os.path.abspath(pdb_path_or_id)
        structure_id = os.path.splitext(os.path.basename(pdb_path))[0] # Use filename stem as ID
        logger.info(f"Parsing local PDB file: {pdb_path}")
    elif pdb_id_pattern.match(pdb_path_or_id):
        pdb_id = pdb_path_or_id.upper()
        structure_id = pdb_id # Use PDB ID as structure ID
        cache_dir = pdb_config.get('pdb_cache_dir')
        if not cache_dir:
             logger.error("pdb_cache_dir not specified in config. Cannot fetch PDB ID.")
             return None
        logger.info(f"Attempting to fetch PDB ID: {pdb_id} using cache: {cache_dir}")
        pdb_path = fetch_pdb(pdb_id, cache_dir)
        if not pdb_path: return None # Fetch failed
    else:
        logger.error(f"Invalid PDB input: '{pdb_path_or_id}'. Must be a valid file path or 4-character PDB ID.")
        return None

    parser = PDBParser(QUIET=True, STRUCTURE_BUILDER=Polypeptide.PolypeptideBuilder()) # Use builder for phi/psi
    try:
        structure = parser.get_structure(structure_id, pdb_path)
        logger.info(f"Successfully parsed PDB structure '{structure.id}'. Models: {len(structure)}")
        if len(structure) > 1:
             logger.warning(f"PDB file contains multiple models ({len(structure)}). Using only the first model (ID: {structure[0].id}).")
        if len(structure) == 0:
            logger.error(f"No models found in PDB structure '{structure.id}'. Cannot proceed.")
            return None
        return structure[0] # Return only the first model
    except PDBException as e:
        logger.error(f"Bio.PDB parsing error for {pdb_path}: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDB file {pdb_path}: {e}", exc_info=True)
        return None


def extract_pdb_features(
    structure_model: Any, # Should be a Bio.PDB Model object
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Extracts features like B-factor, SS, ACC, Dihedrals from a Bio.PDB Model.

    Args:
        structure_model: The Bio.PDB Model object (typically structure[0]).
        config: The main configuration dictionary.

    Returns:
        DataFrame containing extracted features per residue.
    """
    if not _biopython_available:
        logger.error("BioPython not available, cannot extract PDB features.")
        return pd.DataFrame()

    pdb_config = get_pdb_config(config)
    feature_flags = get_pdb_feature_config(config)
    model_id = structure_model.id
    pdb_structure_id = structure_model.get_parent().id # Get the overall structure ID
    data = []

    # --- Run DSSP if needed ---
    dssp_results = None
    dssp_path = pdb_config.get('dssp_path') # Path to executable
    needs_dssp = feature_flags.get('secondary_structure') or feature_flags.get('solvent_accessibility')

    if needs_dssp:
        # DSSP requires a file path. Save the model temporarily.
        # Using a unique temp name based on structure and model ID
        temp_pdb_dir = os.path.join(pdb_config.get("pdb_cache_dir", "."), "temp") # Save in cache/temp
        ensure_dir(temp_pdb_dir)
        temp_pdb_path = os.path.join(temp_pdb_dir, f"_temp_{pdb_structure_id}_model_{model_id}.pdb")

        io = PDBIO()
        io.set_structure(structure_model)
        io.save(temp_pdb_path)
        logger.debug(f"Temporarily saved model {model_id} to {temp_pdb_path} for DSSP.")

        try:
            logger.info(f"Running DSSP (using path: {dssp_path or 'system PATH'})...")
            # Pass model object AND file path to DSSP constructor
            dssp_results = DSSP(structure_model, temp_pdb_path, dssp=dssp_path)
            logger.info(f"DSSP calculation successful for {len(dssp_results)} residues.")
        except FileNotFoundError as e:
             # Check if dssp_path was specified or if it failed from PATH search
             search_location = f"specified path '{dssp_path}'" if dssp_path else "system PATH"
             logger.error(f"DSSP executable not found at {search_location}. Cannot calculate SS/ACC. Error: {e}")
             logger.error("Please install DSSP (e.g., `sudo apt install dssp` or `conda install dssp`) "
                          "and ensure it's in your PATH, or set 'pdb.dssp_path' in config.")
             dssp_results = None # Ensure it's None if failed
        except PDBException as e: # Catch DSSP execution errors (e.g., invalid PDB for DSSP)
             logger.error(f"DSSP calculation failed for {temp_pdb_path}: {e}")
             dssp_results = None
        except Exception as e: # Catch other unexpected errors
             logger.error(f"Unexpected error running DSSP: {e}", exc_info=True)
             dssp_results = None
        finally:
             # Clean up temporary PDB file
             if os.path.exists(temp_pdb_path):
                  try: os.remove(temp_pdb_path)
                  except OSError: logger.warning(f"Could not remove temporary PDB file: {temp_pdb_path}")

    # --- Iterate through residues and extract features ---
    logger.info(f"Extracting features for Model ID: {model_id} of Structure: {pdb_structure_id}")
    residue_counter = 0
    # Note: Pre-calculating phi/psi list was removed for robustness, calculating per residue below

    for chain in structure_model:
        chain_id = chain.id
        for residue in chain.get_residues():
            res_id_tuple = residue.get_id() # tuple: (hetflag, resid, icode)
            resname = residue.get_resname()

            # Skip HETATMs and non-standard residues
            if res_id_tuple[0] != ' ':
                continue
            try:
                is_standard_aa = Polypeptide.is_aa(residue, standard=True)
            except Exception:
                is_standard_aa = False
            if not is_standard_aa:
                continue

            res_seq_id = res_id_tuple[1] # Residue sequence number
            res_icode = res_id_tuple[2].strip() # Insertion code

            residue_features = {
                "domain_id": pdb_structure_id,
                "chain_id": chain_id,
                "resid": res_seq_id,
                **({"icode": res_icode} if res_icode else {}),
                "resname": resname,
            }
            residue_counter += 1

            # Extract B-factor
            if feature_flags.get('b_factor'):
                ca_atom = residue.get("CA")
                if ca_atom:
                    bfactors = [ca_atom.get_bfactor()]
                else:
                     backbone_atoms = ['N', 'CA', 'C', 'O']
                     bfactors = [atom.get_bfactor() for atom_name, atom in residue.items() if atom_name in backbone_atoms]
                     if not bfactors:
                          bfactors = [atom.get_bfactor() for atom in residue if atom.element != 'H']
                residue_features['b_factor'] = np.mean(bfactors) if bfactors else 0.0

            # Extract DSSP features
            ss = '-'
            rsa = np.nan
            if dssp_results:
                 dssp_key = (chain_id, res_id_tuple)
                 if dssp_key in dssp_results:
                      dssp_data = dssp_results[dssp_key]
                      if feature_flags.get('secondary_structure'):
                           ss = dssp_data[2]
                      if feature_flags.get('solvent_accessibility'):
                           rsa = dssp_data[3]
                 else:
                      if not hasattr(extract_pdb_features, "_dssp_missing_logged"):
                           logger.warning(f"Residue {chain_id}:{res_id_tuple} not found in DSSP results. DSSP might skip residues. Subsequent warnings suppressed.")
                           extract_pdb_features._dssp_missing_logged = True

            residue_features['dssp'] = ss
            residue_features['relative_accessibility'] = rsa if not pd.isna(rsa) else None

            # Extract Dihedral angles
            if feature_flags.get('dihedral_angles'):
                 phi = None
                 psi = None
                 try:
                      phi = Polypeptide.calc_phi(residue)
                      psi = Polypeptide.calc_psi(residue)
                 except Exception as e:
                      logger.debug(f"Could not calculate phi/psi for {chain_id}:{res_id_tuple}: {e}")

                 residue_features['phi'] = np.degrees(phi) if phi is not None else None
                 residue_features['psi'] = np.degrees(psi) if psi is not None else None

            data.append(residue_features)

    # Reset DSSP logging flag
    if hasattr(extract_pdb_features, "_dssp_missing_logged"):
        del extract_pdb_features._dssp_missing_logged

    df = pd.DataFrame(data)
    logger.info(f"Extracted features for {residue_counter} standard residues across {len(structure_model)} chains.")

    # --- Post-processing ---
    if feature_flags.get('core_exterior_encoded'):
         logger.warning("Feature 'core_exterior_encoded' requested, but calculation logic is not implemented. Column will be missing or filled with UNK.")
         if 'relative_accessibility' in df.columns and not df['relative_accessibility'].isnull().all():
             threshold = 0.20
             df['core_exterior'] = df['relative_accessibility'].apply(lambda x: 'SURFACE' if pd.notna(x) and x >= threshold else 'CORE')
             logger.info(f"Assigned 'core_exterior' based on RSA threshold ({threshold}).")
         else:
              df['core_exterior'] = 'UNK'

    # Ensure required columns exist
    expected_cols = ['domain_id', 'chain_id', 'resid']
    if feature_flags.get('b_factor'): expected_cols.append('b_factor')
    if feature_flags.get('secondary_structure'): expected_cols.append('dssp')
    if feature_flags.get('solvent_accessibility'): expected_cols.append('relative_accessibility')
    if feature_flags.get('dihedral_angles'): expected_cols.extend(['phi', 'psi'])
    if feature_flags.get('core_exterior_encoded'): expected_cols.append('core_exterior')

    for col in expected_cols:
        if col not in df.columns:
            logger.warning(f"Expected feature column '{col}' not found after extraction. Adding column with NaN/defaults.")
            default_val = 0.0 if col == 'b_factor' else ('-' if col == 'dssp' else ('UNK' if col == 'core_exterior' else np.nan))
            df[col] = default_val

    return df

# --- PDB Visualization/Output ---

class ColorByFlexibilitySelect(Select):
    """Bio.PDB Selector to set B-factor based on predicted flexibility class."""
    def __init__(self, predictions_map: Dict[Tuple[str, int], int], default_b: float = 20.0):
        """
        Args:
            predictions_map: Dictionary mapping (chain_id, resid) to predicted_class.
            default_b: B-factor value for residues not in the predictions map.
        """
        self.predictions = predictions_map
        self.default_b = default_b
        self.class_to_bfactor = {
            0: 10.0,  # Very Rigid
            1: 25.0,  # Rigid
            2: 40.0,  # Moderately Flexible
            3: 60.0,  # Flexible
            4: 80.0,  # Very Flexible
        }
        max_class = max(self.class_to_bfactor.keys())
        max_b = self.class_to_bfactor[max_class]
        for i in range(max_class + 1, 10):
            self.class_to_bfactor[i] = max_b + (i - max_class) * 15.0

        logger.debug(f"B-factor mapping for coloring: {self.class_to_bfactor}")

    def accept_atom(self, atom) -> int:
        """Accepts the atom and sets its B-factor based on prediction."""
        residue = atom.get_parent()
        chain = residue.get_parent()
        res_id_tuple = residue.get_id()

        # Default to keeping original B-factor unless it's a standard AA we have a prediction for
        bfactor_to_set = atom.get_bfactor() # Start with original

        if res_id_tuple[0] == ' ':
            try: # Protect against non-standard residues causing errors in is_aa
                if Polypeptide.is_aa(residue.get_resname(), standard=True):
                    chain_id = chain.id
                    res_seq_id = res_id_tuple[1]
                    pred_key = (chain_id, res_seq_id)
                    predicted_class = self.predictions.get(pred_key)

                    if predicted_class is not None:
                         bfactor_to_set = self.class_to_bfactor.get(predicted_class, self.default_b)
                    else:
                         bfactor_to_set = self.default_b # Apply default if not found
                         if not hasattr(ColorByFlexibilitySelect, "_missing_logged"):
                              logger.warning(f"Residue {chain_id}:{res_seq_id} not in prediction map. Setting B-factor to default {self.default_b}. Subsequent warnings suppressed.")
                              ColorByFlexibilitySelect._missing_logged = True
            except Exception as e:
                logger.debug(f"Skipping B-factor modification for residue {chain.id}:{res_id_tuple} due to error: {e}")

        # Set the B-factor (original or modified)
        atom.set_bfactor(float(bfactor_to_set))
        return 1 # Keep the atom


def color_pdb_by_flexibility(
    structure_model: Any, # Bio.PDB Model object
    predictions_df: pd.DataFrame, # Must contain 'chain_id', 'resid', 'predicted_class'
    output_pdb_path: str
):
    """
    Creates a new PDB file where the B-factor column reflects the predicted flexibility class.

    Args:
        structure_model: The Bio.PDB Model object to modify.
        predictions_df: DataFrame with prediction results.
        output_pdb_path: Path to save the colored PDB file.
    """
    if not _biopython_available:
        logger.error("BioPython not available. Cannot create colored PDB.")
        return

    logger.info(f"Generating colored PDB file (using B-factor column): {output_pdb_path}")

    required_cols = ['chain_id', 'resid', 'predicted_class']
    if not all(col in predictions_df.columns for col in required_cols):
         logger.error(f"Predictions DataFrame must contain columns: {required_cols}. Found: {predictions_df.columns.tolist()}")
         return
    try:
        predictions_df['resid'] = predictions_df['resid'].astype(int)
        pred_map = predictions_df.set_index(['chain_id', 'resid'])['predicted_class'].to_dict()
    except Exception as e:
         logger.error(f"Error creating prediction map: {e}", exc_info=True)
         return

    io = PDBIO()
    io.set_structure(structure_model)
    ensure_dir(os.path.dirname(output_pdb_path))

    if hasattr(ColorByFlexibilitySelect, "_missing_logged"):
        del ColorByFlexibilitySelect._missing_logged

    try:
        io.save(output_pdb_path, select=ColorByFlexibilitySelect(pred_map, default_b=20.0))
        logger.info(f"Colored PDB saved successfully to {output_pdb_path}")
    except Exception as e:
        logger.error(f"Failed to save colored PDB file: {e}", exc_info=True)


def generate_pymol_script(
    predictions_df: pd.DataFrame, # Must contain 'chain_id', 'resid', 'predicted_class'
    config: Dict[str, Any],
    output_pml_path: str,
    pdb_filename: Optional[str] = None # Optional: PDB filename to load in script
):
    """
    Generates a PyMOL (.pml) script to color a structure by flexibility class.

    Args:
        predictions_df: DataFrame with prediction results.
        config: Main configuration dictionary (for colors).
        output_pml_path: Path to save the PyMOL script.
        pdb_filename: Optional name/path of the PDB file to be loaded in the script.
                      If None, assumes the structure is already loaded in PyMOL.
    """
    logger.info(f"Generating PyMOL script: {output_pml_path}")
    colors_map = get_visualization_colors(config)
    class_names_map = get_class_names(config)
    num_classes = config.get('binning', {}).get('num_classes', 5)

    if len(colors_map) < num_classes:
        logger.warning(f"Visualization colors defined ({len(colors_map)}) are fewer than number of classes ({num_classes}). Coloring may be incomplete.")

    script_lines = [
        f"# PyMOL Script generated by drWiggle to color by flexibility",
        f"# Timestamp: {pd.Timestamp.now()}",
        "bg_color white",
        "set cartoon_fancy_helices, 1",
        "set cartoon_smooth_loops, 1",
        "show cartoon",
        "color grey80, all"
    ]

    if pdb_filename:
        safe_pdb_filename = pdb_filename.replace("\\", "/")
        script_lines.insert(1, f"load {safe_pdb_filename}")
        obj_name = os.path.splitext(os.path.basename(safe_pdb_filename))[0]
        script_lines.append(f"disable all")
        script_lines.append(f"enable {obj_name}")
        script_lines.append(f"show cartoon, {obj_name}")
        script_lines.append(f"color grey80, {obj_name}")

    pymol_color_names = {}
    for class_idx in range(num_classes):
        color_hex = colors_map.get(class_idx)
        class_name_safe = class_names_map.get(class_idx, f"class_{class_idx}").replace(" ", "_").replace("-","_").replace("/","_")
        color_name_pymol = f"flex_{class_name_safe}"

        if color_hex:
            try:
                color_hex = color_hex.lstrip('#')
                r = int(color_hex[0:2], 16) / 255.0
                g = int(color_hex[2:4], 16) / 255.0
                b = int(color_hex[4:6], 16) / 255.0
                script_lines.append(f"set_color {color_name_pymol}, [{r:.3f}, {g:.3f}, {b:.3f}]")
                pymol_color_names[class_idx] = color_name_pymol
            except Exception:
                logger.warning(f"Invalid hex color format '{color_hex}' for class {class_idx}. Using grey80.")
                pymol_color_names[class_idx] = "grey80"
        else:
             logger.warning(f"Color not defined for class {class_idx}. Using grey80.")
             pymol_color_names[class_idx] = "grey80"

    required_cols = ['chain_id', 'resid', 'predicted_class']
    if not all(col in predictions_df.columns for col in required_cols):
         logger.error(f"Predictions DataFrame for PyMOL script must contain columns: {required_cols}. Found: {predictions_df.columns.tolist()}")
         return

    try: # Add try-except around this block
        predictions_df['resid'] = predictions_df['resid'].astype(int)

        for class_idx in range(num_classes):
            class_residues = predictions_df[predictions_df['predicted_class'] == class_idx]
            color_name = pymol_color_names.get(class_idx, "grey80")

            if not class_residues.empty:
                selection_parts = []
                for chain, group in class_residues.groupby('chain_id'):
                     res_ids_str = "+".join(map(str, sorted(group['resid'].unique())))
                     selection_parts.append(f"(chain {chain} and resi {res_ids_str})")

                if selection_parts:
                    full_selection = " or ".join(selection_parts)
                    script_lines.append(f"color {color_name}, ({full_selection})")
                else:
                     logger.debug(f"No residues found for class {class_idx} to color.")

        script_lines.append("zoom vis")
        script_lines.append(f"print('drWiggle coloring applied using colors: {pymol_color_names}')")

    except Exception as e:
        logger.error(f"Error occurred while generating PyMOL color commands: {e}", exc_info=True)
        # Optionally add a line indicating failure in the script itself
        script_lines.append("print('ERROR: Failed to generate complete coloring commands.')")


    ensure_dir(os.path.dirname(output_pml_path))
    try:
        with open(output_pml_path, 'w') as f:
            f.write("\n".join(script_lines))
        logger.info(f"PyMOL script saved successfully to {output_pml_path}")
    except Exception as e:
        logger.error(f"Failed to write PyMOL script: {e}", exc_info=True)

# --- End File ---

--- End File: drwiggle/utils/pdb_tools.py ---

--- File: drwiggle/utils/visualization.py ---
---------------------------------------
import logging
import os
from typing import Dict, Any, List, Optional, Union
import numpy as np
import pandas as pd

# Configure Matplotlib backend for compatibility
import matplotlib
try:
    # Try using 'Agg' first for non-interactive environments
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import seaborn as sns
    _plotting_available = True
except ImportError:
    # Fallback or warning if libraries not installed
    logging.getLogger(__name__).warning("Matplotlib or Seaborn not found. Plotting functions will be disabled. Install them (`pip install matplotlib seaborn`)")
    # Define dummy plt, sns if not available to avoid runtime errors later
    class DummyPlt:
        def subplots(self, *args, **kwargs): return None, DummyAx()
        def close(self, *args, **kwargs): pass
        def __getattr__(self, name): return lambda *args, **kwargs: None # Dummy any other plt calls
    class DummyAx:
        def __getattr__(self, name): return lambda *args, **kwargs: None
    plt = DummyPlt()
    sns = None # Assign None to sns
    _plotting_available = False
except Exception as e:
    # Handle other potential backend errors
     logging.getLogger(__name__).warning(f"Error setting up Matplotlib backend 'Agg': {e}. Plotting might fail. Trying default backend.")
     try:
         import matplotlib.pyplot as plt
         import seaborn as sns
         _plotting_available = True
     except ImportError:
        logging.getLogger(__name__).warning("Matplotlib or Seaborn not found after backend error. Plotting disabled.")
        plt = DummyPlt()
        sns = None
        _plotting_available = False


from drwiggle.utils.helpers import ensure_dir
from drwiggle.config import get_visualization_colors, get_class_names

logger = logging.getLogger(__name__)

# --- Plot Saving Helper ---

def _save_plot(figure: Optional[matplotlib.figure.Figure], output_path: str, dpi: int = 150):
    """Helper function to save a Matplotlib plot and close the figure."""
    if not _plotting_available or figure is None:
        logger.warning(f"Plotting libraries unavailable or figure invalid. Cannot save plot to {output_path}")
        if figure: plt.close(figure) # Close even if not saved
        return

    try:
        ensure_dir(os.path.dirname(output_path))
        # Use tight_layout with padding to prevent labels overlapping edges
        figure.tight_layout(pad=1.1)
        figure.savefig(output_path, dpi=dpi, bbox_inches='tight')
        logger.info(f"Plot saved to {output_path}")
    except Exception as e:
        logger.error(f"Failed to save plot to {output_path}: {e}", exc_info=True)
    finally:
        # Ensure the figure is closed to free memory, regardless of saving success
        plt.close(figure)

# --- Specific Plotting Functions ---

def plot_bin_distribution(
    rmsf_values: Union[np.ndarray, pd.Series],
    boundaries: List[float],
    config: Dict[str, Any],
    output_path: str,
    num_bins_hist: int = 50,
    title: Optional[str] = None
):
    """Plots the RMSF distribution histogram with bin boundaries marked."""
    if not _plotting_available: return
    if boundaries is None or len(boundaries) < 2:
         logger.warning("Cannot plot bin distribution: Invalid boundaries provided.")
         return
    if isinstance(rmsf_values, pd.Series): rmsf_values = rmsf_values.values

    # Filter out NaNs or Infs from RMSF values before plotting
    finite_rmsf = rmsf_values[np.isfinite(rmsf_values)]
    if len(finite_rmsf) == 0:
         logger.warning("No finite RMSF values to plot for distribution.")
         return
    if len(finite_rmsf) < len(rmsf_values):
         logger.warning(f"Plotting RMSF distribution using {len(finite_rmsf)} finite values (excluded {len(rmsf_values) - len(finite_rmsf)} non-finite).")


    fig, ax = plt.subplots(figsize=(10, 6))
    # Use seaborn's histplot for potentially nicer aesthetics and KDE integration
    sns.histplot(finite_rmsf, bins=num_bins_hist, kde=True, stat="density", alpha=0.7, label='RMSF Distribution', ax=ax)

    # Add vertical lines for boundaries
    colors_map = get_visualization_colors(config)
    num_classes = len(boundaries) - 1
    # Use a default colormap if config colors don't match num_classes or are missing
    default_cmap = plt.cm.viridis
    class_colors = [colors_map.get(i, default_cmap(i / num_classes)) for i in range(num_classes)]


    for i, bound in enumerate(boundaries):
        if np.isfinite(bound): # Skip infinite boundaries
             # Assign color based on the *bin to the right* of the boundary line
             color_idx = min(i, num_classes - 1) # Use color of the bin starting at this boundary
             line_color = class_colors[color_idx]
             ax.axvline(bound, color=line_color, linestyle='--', lw=1.5,
                        label=f'Boundary {i}' if (i==0 and np.isfinite(boundaries[0])) or (i==len(boundaries)-1 and np.isfinite(boundaries[-1])) else None) # Label first/last finite maybe?

    ax.set_xlabel("RMSF Value (Å)") # Assuming Angstroms
    ax.set_ylabel("Density")
    plot_title = title or "RMSF Distribution and Class Boundaries"
    ax.set_title(plot_title)
    # Improve legend handling - maybe only add one entry for boundaries
    # handles, labels = ax.get_legend_handles_labels()
    # if handles: ax.legend(handles=handles, labels=labels, fontsize='small', loc='best')
    ax.grid(axis='y', linestyle=':', alpha=0.5)

    # Set sensible x-limits based on data range
    data_min, data_max = np.min(finite_rmsf), np.max(finite_rmsf)
    data_std = np.std(finite_rmsf)
    ax.set_xlim(left=max(0, data_min - data_std*0.2),
                right=data_max + data_std*0.2)


    _save_plot(fig, output_path)

def plot_confusion_matrix(
    cm_df: pd.DataFrame, # Confusion matrix as DataFrame from utils.metrics
    output_path: str,
    normalize: bool = True,
    title: Optional[str] = None
):
    """Plots the confusion matrix from a DataFrame."""
    if not _plotting_available or sns is None: return
    if cm_df is None or cm_df.empty:
         logger.warning("Cannot plot confusion matrix: Input DataFrame is None or empty.")
         return

    cm_array = cm_df.values
    class_names = cm_df.columns.tolist() # Get names from columns

    if normalize:
        # Normalize by row (true label) to get recall per class on diagonal
        cm_sum = cm_array.sum(axis=1)[:, np.newaxis]
        # Avoid division by zero for classes with no samples
        with np.errstate(divide='ignore', invalid='ignore'):
             cm_norm = cm_array.astype('float') / cm_sum
        cm_norm[np.isnan(cm_norm)] = 0 # Set NaNs (from 0/0) to 0
        data_to_plot = cm_norm
        fmt = '.2f'
        plot_title = title or "Normalized Confusion Matrix (Recall)"
    else:
        data_to_plot = cm_array
        fmt = 'd'
        plot_title = title or 'Confusion Matrix (Counts)'

    fig, ax = plt.subplots(figsize=(max(6, len(class_names)*0.8), max(5, len(class_names)*0.7)))
    sns.heatmap(data_to_plot, annot=True, fmt=fmt, cmap="Blues", ax=ax,
                xticklabels=class_names, yticklabels=class_names,
                linewidths=.5, linecolor='lightgray', annot_kws={"size": 10}) # Adjust annotation size

    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")
    ax.set_title(plot_title, fontsize=14)
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    plt.setp(ax.get_yticklabels(), rotation=0)

    _save_plot(fig, output_path)


def plot_feature_importance(
    importances: Dict[str, float], # Dict {feature_name: score}
    output_path: str,
    top_n: int = 25,
    title: Optional[str] = None,
    model_name: Optional[str] = None
):
    """Plots the top N feature importances from a dictionary."""
    if not _plotting_available or sns is None: return
    if not importances:
        logger.warning("No feature importances provided to plot.")
        return

    # Create DataFrame and sort
    importance_df = pd.DataFrame(list(importances.items()), columns=['feature', 'importance'])
    # Filter out near-zero-importance features before selecting top N
    importance_df = importance_df[importance_df['importance'] > 1e-6] # Threshold near zero
    importance_df = importance_df.sort_values(by='importance', ascending=False).head(top_n)

    if importance_df.empty:
         logger.warning("No features with importance > 0 found to plot.")
         return

    # Plotting
    fig, ax = plt.subplots(figsize=(10, max(6, len(importance_df) * 0.35))) # Adjust height
    sns.barplot(x='importance', y='feature', data=importance_df, ax=ax, palette='viridis_r') # Reverse viridis

    ax.set_xlabel("Importance Score", fontsize=12)
    ax.set_ylabel("Feature Name", fontsize=12)
    plot_title = title or f"Top {len(importance_df)} Feature Importances"
    if model_name: plot_title += f" ({model_name})"
    ax.set_title(plot_title, fontsize=14)
    ax.tick_params(axis='both', which='major', labelsize=10)
    ax.grid(axis='x', linestyle=':', alpha=0.6)

    _save_plot(fig, output_path)

def plot_class_distribution(
    class_labels: Union[np.ndarray, pd.Series],
    config: Dict[str, Any],
    output_path: str,
    title: Optional[str] = None
):
    """Plots the distribution of predicted or actual classes."""
    if not _plotting_available or sns is None: return
    if isinstance(class_labels, pd.Series): class_labels = class_labels.values
    if class_labels is None or len(class_labels) == 0:
        logger.warning("No class labels provided for plotting distribution.")
        return


    class_names_map = get_class_names(config)
    num_classes = len(class_names_map) if class_names_map else (np.max(class_labels) + 1 if len(class_labels) > 0 else 0)
    if num_classes == 0:
        logger.warning("Cannot plot class distribution: Number of classes is zero.")
        return
    class_names = [class_names_map.get(i, f"Class_{i}") for i in range(num_classes)]

    unique_classes, counts = np.unique(class_labels, return_counts=True)
    class_counts = dict(zip(unique_classes, counts))

    # Ensure all classes are represented, even if count is 0
    plot_data = pd.DataFrame({
        'class_index': range(num_classes),
        'class_name': class_names,
        'count': [class_counts.get(i, 0) for i in range(num_classes)]
    })
    total_count = plot_data['count'].sum()
    plot_data['percentage'] = (plot_data['count'] / total_count) * 100 if total_count > 0 else 0

    fig, ax = plt.subplots(figsize=(max(8, num_classes * 1.2), 6)) # Adjusted size
    colors_map = get_visualization_colors(config)
    colors_list = [colors_map.get(i, None) for i in range(num_classes)] if colors_map else None
    # Use default palette if colors not fully defined
    palette = colors_list if colors_list and all(colors_list) else sns.color_palette()

    bar_plot = sns.barplot(x='class_name', y='count', data=plot_data, ax=ax, palette=palette)

    # Add percentage labels on top of bars
    for index, row in plot_data.iterrows():
        if row['count'] > 0: # Only label bars with counts
             ax.text(index, row['count'], f"{row['percentage']:.1f}%",
                     color='black', ha="center", va='bottom', fontsize=9)

    ax.set_xlabel("Flexibility Class", fontsize=12)
    ax.set_ylabel("Number of Residues", fontsize=12)
    plot_title = title or "Class Distribution"
    ax.set_title(plot_title, fontsize=14)
    plt.setp(ax.get_xticklabels(), rotation=30, ha="right", fontsize=10)
    ax.tick_params(axis='y', labelsize=10)
    ax.grid(axis='y', linestyle=':', alpha=0.6)
    # Adjust y-limit to make space for text labels
    ax.set_ylim(top=ax.get_ylim()[1] * 1.1)

    _save_plot(fig, output_path)

# --- Temperature Comparison Plots ---

def plot_metric_vs_temperature(
     metrics_df: pd.DataFrame, # Should contain 'temperature', 'model', and metric columns
     metric: str,
     output_path: str,
     title: Optional[str] = None
):
    """Plots a specific metric against temperature for different models."""
    if not _plotting_available or sns is None: return
    if metrics_df is None or metrics_df.empty or metric not in metrics_df.columns:
        logger.warning(f"Cannot plot metric '{metric}' vs temperature: DataFrame empty or metric column missing.")
        return

    # Ensure temperature is numeric, handle non-numeric gracefully (like 'average')
    metrics_df['temperature_num'] = pd.to_numeric(metrics_df['temperature'], errors='coerce')
    plot_df = metrics_df.dropna(subset=['temperature_num', metric]).copy()
    plot_df = plot_df.sort_values(by=['model', 'temperature_num'])

    if plot_df.empty:
        logger.warning(f"No valid data points found for metric '{metric}' vs temperature plot.")
        return

    num_models = plot_df['model'].nunique()
    fig, ax = plt.subplots(figsize=(10, 6))

    # Use Seaborn for potentially nicer plotting with hue for models
    sns.lineplot(data=plot_df, x='temperature_num', y=metric, hue='model',
                 marker='o', ax=ax, markersize=7, legend='full')

    ax.set_xlabel("Temperature (K)", fontsize=12)
    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=12) # Nicer label
    plot_title = title or f"{ax.get_ylabel()} vs Temperature"
    ax.set_title(plot_title, fontsize=14)
    ax.tick_params(axis='both', which='major', labelsize=10)

    # Adjust legend position if many models
    if num_models > 5:
        ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    else:
        ax.legend(title='Model', fontsize=10)

    ax.grid(True, linestyle=':', alpha=0.7)

    _save_plot(fig, output_path)


def plot_transition_matrix(
     transition_matrix: pd.DataFrame, # Rows=T1, Cols=T2, Values=Count/Prob
     t1_name: str,
     t2_name: str,
     output_path: str,
     normalize: bool = True,
     title: Optional[str] = None
):
     """Plots a class transition matrix between two temperatures."""
     if not _plotting_available or sns is None: return
     if transition_matrix is None or transition_matrix.empty:
        logger.warning(f"Cannot plot transition matrix from {t1_name} to {t2_name}: Matrix is None or empty.")
        return

     matrix_data = transition_matrix.values
     class_names = transition_matrix.columns.tolist() # Assume row/col names match

     if normalize:
          # Normalize by row (T1 class) -> probability of transitioning to T2 class
          row_sums = matrix_data.sum(axis=1)[:, np.newaxis]
          with np.errstate(divide='ignore', invalid='ignore'):
               matrix_norm = matrix_data.astype('float') / row_sums
          matrix_norm[np.isnan(matrix_norm)] = 0 # Handle division by zero for rows with no samples
          data_to_plot = matrix_norm
          fmt = '.2f'
          plot_title = title or f"Normalized Class Transition Probability ({t1_name} K -> {t2_name} K)"
          cmap = "viridis" # Use a sequential colormap for probabilities
     else:
          data_to_plot = matrix_data
          fmt = 'd'
          plot_title = title or f"Class Transition Counts ({t1_name} K -> {t2_name} K)"
          cmap = "Blues" # Use Blues for counts

     fig, ax = plt.subplots(figsize=(max(6, len(class_names)*0.9), max(5, len(class_names)*0.8)))
     sns.heatmap(data_to_plot, annot=True, fmt=fmt, cmap=cmap, ax=ax,
                 xticklabels=class_names, yticklabels=class_names,
                 linewidths=.5, linecolor='lightgray', annot_kws={"size": 10})

     ax.set_xlabel(f"Predicted Class at {t2_name} K", fontsize=12)
     ax.set_ylabel(f"Predicted Class at {t1_name} K", fontsize=12)
     ax.set_title(plot_title, fontsize=14)
     plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor", fontsize=10)
     plt.setp(ax.get_yticklabels(), rotation=0, fontsize=10)

     _save_plot(fig, output_path)

--- End File: drwiggle/utils/visualization.py ---

=========================================
         End of Context File             
=========================================
